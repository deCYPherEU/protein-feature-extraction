{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protein feature extraction pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will contain the pipeline for extracting features from protein sequences. It will be used as a way to show the output without needing to run the `pipeline.py` file locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "from fondant.pipeline import Pipeline\n",
    "import os\n",
    "from config import MOCK_DATA_PATH_FONDANT\n",
    "\n",
    "# check if the manifest file is removed.\n",
    "REMOVED_MANIFEST = False\n",
    "\n",
    "# check if the output folder exists\n",
    "OUTPUT_FOLDER = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Mock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python utils/generate_mock_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MNQRGMPIQSLVTNVKINRLEENDCIHTRHRVRPGRTDGKNLHAMM...</td>\n",
       "      <td>Seq1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MAGLKPEVPLHDGINKFGKSDFAGQEGPKIVTTTDKALLVANGALK...</td>\n",
       "      <td>Seq2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MVDLKKELKNFVDSDFPGSPKQEAQGIDVRILLSFNNAAFREALII...</td>\n",
       "      <td>Seq3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MELILAKARLEFECDWGLLMLEPCVPPTKIFADRNYAVGVMFESDK...</td>\n",
       "      <td>Seq4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MRVLCDGSTGYACAKNTRIRFREKVASVLAKIQGYEQTFPHHMPNM...</td>\n",
       "      <td>Seq5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sequence  name\n",
       "0  MNQRGMPIQSLVTNVKINRLEENDCIHTRHRVRPGRTDGKNLHAMM...  Seq1\n",
       "1  MAGLKPEVPLHDGINKFGKSDFAGQEGPKIVTTTDKALLVANGALK...  Seq2\n",
       "2  MVDLKKELKNFVDSDFPGSPKQEAQGIDVRILLSFNNAAFREALII...  Seq3\n",
       "3  MELILAKARLEFECDWGLLMLEPCVPPTKIFADRNYAVGVMFESDK...  Seq4\n",
       "4  MRVLCDGSTGYACAKNTRIRFREKVASVLAKIQGYEQTFPHHMPNM...  Seq5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show content of the mock data\n",
    "import pandas as pd\n",
    "mock_df = pd.read_parquet(\".\" + MOCK_DATA_PATH_FONDANT)  # dot added to make it relative to the current directory\n",
    "mock_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new pipeline\n",
    "\n",
    "BASE_PATH = \".fondant\"\n",
    "PIPELINE_NAME = \"feature_extraction_pipeline\"\n",
    "\n",
    "pipeline = Pipeline(\n",
    "\tname=PIPELINE_NAME,\n",
    "\tbase_path=BASE_PATH,\n",
    "\tdescription=\"A pipeline to extract features from protein sequences.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-12 15:27:11,305 | fondant.pipeline.pipeline | INFO] The consumes section of the component spec is not defined. Can not infer consumes of the OperationSpec. Please define a consumes section in the dataset interface. \n"
     ]
    }
   ],
   "source": [
    "# Read the dataset\n",
    "\n",
    "dataset = pipeline.read(\n",
    "\t\"load_from_parquet\",\n",
    "\targuments={\n",
    "\t\t\"dataset_uri\": MOCK_DATA_PATH_FONDANT,\n",
    "\t},\n",
    "\tproduces={\n",
    "\t\t\"sequence\": pa.string()\n",
    "\t}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components\n",
    "\n",
    "---\n",
    "\n",
    "### generate_protein_sequence_checksum_component\n",
    "\n",
    "This component generates a checksum for the protein sequence.\n",
    "\n",
    "---\n",
    "\n",
    "### biopython_component\n",
    "\n",
    "Extracts features from the protein sequence using Biopython.\n",
    "\n",
    "---\n",
    "\n",
    "### iFeatureOmega_component\n",
    "\n",
    "Extracts features from the protein sequence using the [iFeatureOmega-CLI GitHub repo](https://github.com/Superzchen/iFeatureOmega-CLI). Arguments are used to specify the type of features to extract.\n",
    "\n",
    "---\n",
    "\n",
    "### filter_pdb_component\n",
    "\n",
    "Filters PDB files that are already predicted to avoid redundant predictions. Arguments need to be specified before running the pipeline:\n",
    "```json\n",
    "\"storage_type\": \"local\",\n",
    "\"pdb_path\": \"/data/<your-pdb-folder-path>\",\n",
    "\"bucket_name\": \"your-bucket-name\",\n",
    "\"project_id\": \"your-project-id\",\n",
    "\"google_cloud_credentials_path\": \"/data/<your-credentials>.json\"\n",
    "```\n",
    "\n",
    "If only using local, keep bucket_name, project_id, and google_cloud_credentials_path as empty strings. Using remote requires a Google Cloud Storage bucket with credentials and a project ID.\n",
    "\n",
    "---\n",
    "\n",
    "### predict_protein_3D_structure_component\n",
    "\n",
    "Predicts the 3D structure of the protein using ESMFold. This component requires a `.env` file with the following variables:\n",
    "```env\n",
    "HF_API_KEY=\"\"\n",
    "HF_ENDPOINT_URL=\"\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### store_pdb_component\n",
    "\n",
    "Stores the PDB files in the provided storage_type. Arguments need to be specified before running the pipeline:\n",
    "```json\n",
    "\"storage_type\": \"local\",\n",
    "\"pdb_path\": \"/data/<your-pdb-folder-path>\",\n",
    "\"bucket_name\": \"your-bucket-name\",\n",
    "\"project_id\": \"your-project-id\",\n",
    "\"google_cloud_credentials_path\": \"/data/<your-credentials>.json\"\n",
    "```\n",
    "\n",
    "If only using local, keep bucket_name, project_id, and google_cloud_credentials_path as empty strings. Using remote requires a Google Cloud Storage bucket with credentials and a project ID.\n",
    "\n",
    "---\n",
    "\n",
    "### msa_component\n",
    "\n",
    "Generates the multiple sequence alignment for the protein sequence using [Clustal Omega](http://www.clustal.org/omega/). It's recommended to use a smaller number of sequences or none at all due to potential time consumption.\n",
    "\n",
    "---\n",
    "\n",
    "### unikp_component\n",
    "\n",
    "Uses the UniKP endpoint on HuggingFace to predict the kinetic parameters of a protein sequence and substrate (SMILES) combination. See README for the description of the contents of this file.\n",
    "\n",
    "```yaml\n",
    "\"protein_smiles_path\": \"/data/<path_protein_smiles>\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### peptide_component\n",
    "\n",
    "Calculates the features from the protein sequence using the `peptides` package.\n",
    "\n",
    "---\n",
    "\n",
    "### deepTMpred_component\n",
    "\n",
    "Predicts the transmembrane regions of the protein sequence using the [DeepTMpred GitHub repository](https://github.com/ISYSLAB-HUST/DeepTMpred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-12 15:27:38,667 | fondant.pipeline.pipeline | WARNING] Component `Biopython component` has an image tag set to latest. Caching for the component will be disabled to prevent unpredictable behavior due to images updates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-12 15:27:38,675 | fondant.pipeline.pipeline | WARNING] Component `Generate Protein Sequence Checksum Component` has an image tag set to latest. Caching for the component will be disabled to prevent unpredictable behavior due to images updates\n",
      "[2024-06-12 15:27:38,679 | fondant.pipeline.pipeline | WARNING] Component `MSA component` has an image tag set to latest. Caching for the component will be disabled to prevent unpredictable behavior due to images updates\n",
      "[2024-06-12 15:27:38,685 | fondant.pipeline.pipeline | WARNING] Component `PDB Features component` has an image tag set to latest. Caching for the component will be disabled to prevent unpredictable behavior due to images updates\n",
      "[2024-06-12 15:27:38,691 | fondant.pipeline.pipeline | WARNING] Component `UniKP Component` has an image tag set to latest. Caching for the component will be disabled to prevent unpredictable behavior due to images updates\n",
      "[2024-06-12 15:27:38,698 | fondant.pipeline.pipeline | WARNING] Component `Peptide Features component` has an image tag set to latest. Caching for the component will be disabled to prevent unpredictable behavior due to images updates\n",
      "[2024-06-12 15:27:38,703 | fondant.pipeline.pipeline | WARNING] Component `DeepTM prediction Component` has an image tag set to latest. Caching for the component will be disabled to prevent unpredictable behavior due to images updates\n"
     ]
    }
   ],
   "source": [
    "_ = dataset.apply(\n",
    "\t\"./components/biopython_component\"\n",
    ").apply(\n",
    "\t\"./components/generate_protein_sequence_checksum_component\"\n",
    "# ).apply(\n",
    "# \t\"./components/iFeatureOmega_component\",\n",
    "# \t# currently forcing the number of rows to 5, but there needs to be a better way to do this, see readme for more info\n",
    "# \tinput_partition_rows=5,\n",
    "# \targuments={\n",
    "# \t\t\"descriptors\": [\"AAC\", \"CTDC\", \"CTDT\"]\n",
    "# \t}\n",
    "# ).apply(\n",
    "# \t\"./components/filter_pdb_component\",\n",
    "# \targuments={\n",
    "# \t\t\"method\": \"local\",\n",
    "# \t\t\"local_pdb_path\": \"/data/pdb_files\",\n",
    "# \t\t\"bucket_name\": \"\",\n",
    "# \t\t\"project_id\": \"\",\n",
    "# \t\t\"google_cloud_credentials_path\": \"\"\n",
    "# \t}\n",
    "# ).apply(\n",
    "# \t\"./components/predict_protein_3D_structure_component\",\n",
    "# ).apply(\n",
    "# \t\"./components/store_pdb_component\",\n",
    "# \targuments={\n",
    "# \t\t\"method\": \"local\",\n",
    "# \t\t\"local_pdb_path\": \"/data/pdb_files/\",\n",
    "# \t\t\"bucket_name\": \"elated-chassis-400207_dbtl_pipeline_outputs\",\n",
    "# \t\t\"project_id\": \"elated-chassis-400207\",\n",
    "# \t\t\"google_cloud_credentials_path\": \"/data/google_cloud_credentials.json\"\n",
    "# \t}\n",
    ").apply(\n",
    "\t\"./components/msa_component\",\n",
    "# ).apply(\n",
    "# \t\"./components/pdb_features_component\"\n",
    "# ).apply(\n",
    "# \t\"./components/unikp_component\",\n",
    "# \targuments={\n",
    "# \t\t\"protein_smiles_path\": \"/data/protein_smiles.json\",\n",
    "# \t},\n",
    "# ).apply(\n",
    "# \t\"./components/peptide_features_component\"\n",
    "# ).apply(\n",
    "# \t\"./components/DeepTMpred_component\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pipeline.py` file needs to be run using the command line. The following command will run the pipeline:\n",
    "\n",
    "```bash\n",
    "fondant < full_path_to_pipeline.py >\\data:/data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-12 15:27:42,680 | root | INFO] Found reference to un-compiled pipeline... compiling\n",
      "[2024-06-12 15:27:42,680 | fondant.pipeline.compiler | INFO] Compiling feature_extraction_pipeline to .fondant/compose.yaml\n",
      "[2024-06-12 15:27:42,681 | fondant.pipeline.compiler | INFO] Base path found on local system, setting up .fondant as mount volume\n",
      "[2024-06-12 15:27:42,681 | fondant.pipeline.pipeline | INFO] Sorting pipeline component graph topologically.\n"
     ]
    },
    {
     "ename": "InvalidPipelineDefinition",
     "evalue": "Component 'pdb_features_component' is trying to invoke thefield 'pdb_string', which has not been defined or createdin the previous components. \nAvailable field names: ['sequence', 'sequence_length', 'molecular_weight', 'aromaticity', 'isoelectric_point', 'instability_index', 'gravy', 'helix', 'turn', 'sheet', 'charge_at_ph3', 'charge_at_ph5', 'charge_at_ph7', 'charge_at_ph9', 'molar_extinction_coefficient_oxidized', 'molar_extinction_coefficient_reduced', 'flexibility_max', 'flexibility_min', 'flexibility_mean', 'sequence_checksum', 'msa_sequence']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidPipelineDefinition\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# get current full path to the project\u001b[39;00m\n\u001b[1;32m     12\u001b[0m mounted_data \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:/data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m \u001b[43mDockerRunner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_volumes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmounted_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Software/Sandbox/protein-feature-extraction/.venv/lib/python3.10/site-packages/fondant/pipeline/runner.py:82\u001b[0m, in \u001b[0;36mDockerRunner.run\u001b[0;34m(self, input, extra_volumes, build_args, auth_provider)\u001b[0m\n\u001b[1;32m     78\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound reference to un-compiled pipeline... compiling\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     80\u001b[0m     )\n\u001b[1;32m     81\u001b[0m     compiler \u001b[38;5;241m=\u001b[39m DockerCompiler()\n\u001b[0;32m---> 82\u001b[0m     \u001b[43mcompiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_volumes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_volumes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbuild_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuild_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43mauth_provider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(output_path)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Software/Sandbox/protein-feature-extraction/.venv/lib/python3.10/site-packages/fondant/pipeline/compiler.py:121\u001b[0m, in \u001b[0;36mDockerCompiler.compile\u001b[0;34m(self, pipeline, output_path, extra_volumes, build_args, auth_provider)\u001b[0m\n\u001b[1;32m    117\u001b[0m     extra_volumes\u001b[38;5;241m.\u001b[39mappend(auth_provider\u001b[38;5;241m.\u001b[39mget_path())\n\u001b[1;32m    119\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompiling \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpipeline\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 121\u001b[0m spec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_spec\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextra_volumes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_volumes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbuild_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuild_args\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mNoAliasDumper\u001b[39;00m(yaml\u001b[38;5;241m.\u001b[39mSafeDumper):\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mignore_aliases\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n",
      "File \u001b[0;32m~/Software/Sandbox/protein-feature-extraction/.venv/lib/python3.10/site-packages/fondant/pipeline/compiler.py:200\u001b[0m, in \u001b[0;36mDockerCompiler._generate_spec\u001b[0;34m(self, pipeline, extra_volumes, build_args)\u001b[0m\n\u001b[1;32m    196\u001b[0m run_id \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mget_run_id()\n\u001b[1;32m    198\u001b[0m services \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 200\u001b[0m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m component_cache_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m component_id, component \u001b[38;5;129;01min\u001b[39;00m pipeline\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/Software/Sandbox/protein-feature-extraction/.venv/lib/python3.10/site-packages/fondant/pipeline/pipeline.py:584\u001b[0m, in \u001b[0;36mPipeline.validate\u001b[0;34m(self, run_id)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Sort and run validation on the pipeline definition.\u001b[39;00m\n\u001b[1;32m    578\u001b[0m \n\u001b[1;32m    579\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;124;03m    run_id: run identifier\u001b[39;00m\n\u001b[1;32m    581\u001b[0m \n\u001b[1;32m    582\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort_graph()\n\u001b[0;32m--> 584\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_pipeline_definition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Software/Sandbox/protein-feature-extraction/.venv/lib/python3.10/site-packages/fondant/pipeline/pipeline.py:631\u001b[0m, in \u001b[0;36mPipeline._validate_pipeline_definition\u001b[0;34m(self, run_id)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m component_field_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m manifest\u001b[38;5;241m.\u001b[39mfields:\n\u001b[1;32m    625\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    626\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComponent \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcomponent_op\u001b[38;5;241m.\u001b[39mcomponent_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is trying to invoke the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    627\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfield \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcomponent_field_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, which has not been defined or created\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    628\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min the previous components. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    629\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAvailable field names: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(manifest\u001b[38;5;241m.\u001b[39mfields\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    630\u001b[0m     )\n\u001b[0;32m--> 631\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidPipelineDefinition(\n\u001b[1;32m    632\u001b[0m         msg,\n\u001b[1;32m    633\u001b[0m     )\n\u001b[1;32m    635\u001b[0m \u001b[38;5;66;03m# Get the corresponding manifest fields\u001b[39;00m\n\u001b[1;32m    636\u001b[0m manifest_field \u001b[38;5;241m=\u001b[39m manifest\u001b[38;5;241m.\u001b[39mfields[component_field_name]\n",
      "\u001b[0;31mInvalidPipelineDefinition\u001b[0m: Component 'pdb_features_component' is trying to invoke thefield 'pdb_string', which has not been defined or createdin the previous components. \nAvailable field names: ['sequence', 'sequence_length', 'molecular_weight', 'aromaticity', 'isoelectric_point', 'instability_index', 'gravy', 'helix', 'turn', 'sheet', 'charge_at_ph3', 'charge_at_ph5', 'charge_at_ph7', 'charge_at_ph9', 'molar_extinction_coefficient_oxidized', 'molar_extinction_coefficient_reduced', 'flexibility_max', 'flexibility_min', 'flexibility_mean', 'sequence_checksum', 'msa_sequence']"
     ]
    }
   ],
   "source": [
    "from fondant.pipeline.runner import DockerRunner\n",
    "import shutil\n",
    "\n",
    "# remove the most recent output folder if the manifest file is removed\n",
    "# without a manifest file in the most recent output folder, the pipeline cannot be run\n",
    "if OUTPUT_FOLDER and REMOVED_MANIFEST:\n",
    "\tshutil.rmtree(OUTPUT_FOLDER)\n",
    "\t# remove cache\n",
    "\tshutil.rmtree(os.path.join(BASE_PATH, PIPELINE_NAME, \"cache\"))\n",
    "\n",
    "# get current full path to the project\n",
    "mounted_data = os.path.join(os.path.abspath(\"data\"), \":/data\")\n",
    "\n",
    "DockerRunner().run(input=pipeline, extra_volumes=mounted_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "The following results have been taken from the output of the pipeline, which is stored in the `.fondant` directory. This directory contains the output of each component, together with the cache of the previous run. Currently, the pipeline doesn't implement the `write_to_file` component, so the results will be taken individually from the output of each component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# get the most recent folder in the folder named: BASE_PATH + PIPELINE_NAME + PIPELINE_NAME-<timestamp>\n",
    "matching_folders = glob.glob(f\"{BASE_PATH}/{PIPELINE_NAME}/{PIPELINE_NAME}-*\")\n",
    "\n",
    "if matching_folders:\n",
    "    OUTPUT_FOLDER = max(matching_folders, key=os.path.getctime)\n",
    "else:\n",
    "    print(\"No matching folders found\")\n",
    "    exit()\n",
    "\n",
    "if os.path.exists(OUTPUT_FOLDER):\n",
    "\t# remove the manifest file from each folder in the output folder\n",
    "\tfor root, dirs, files in os.walk(OUTPUT_FOLDER):\n",
    "\t\tfor file in files:\n",
    "\t\t\tif file == \"manifest.json\":\n",
    "\t\t\t\tos.remove(os.path.join(root, file))\n",
    "\t\t\t\tREMOVED_MANIFEST = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def merge_parquet_folders(folder_path):\n",
    "\tmerge_df = pd.DataFrame()\n",
    "\t\n",
    "\tfor folder in os.listdir(folder_path):\n",
    "\t\tparquet_partitions = os.path.join(folder_path, folder)\n",
    "\t\tdf = pd.read_parquet(parquet_partitions)\n",
    "\t\t\n",
    "\t\tif merge_df.empty:\n",
    "\t\t\tmerge_df = df\n",
    "\t\telse:\n",
    "\t\t\tmerge_df = merge_df.merge(df, on=\"sequence\")\n",
    "\t\n",
    "\treturn merge_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if REMOVED_MANIFEST and os.path.exists(OUTPUT_FOLDER):\n",
    "\tmerged_df = merge_parquet_folders(OUTPUT_FOLDER)\n",
    "\tmerged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if REMOVED_MANIFEST and os.path.exists(OUTPUT_FOLDER):\n",
    "\tif not os.path.exists(os.path.join(os.path.abspath(\"data\"), \"export\")):\n",
    "\t\tos.makedirs(os.path.join(os.path.abspath(\"data\"), \"export\"))\n",
    "\n",
    "\toutput_path = os.path.join(os.path.abspath(\"data\"), \"export\")\n",
    "\n",
    "\tmerged_df.to_parquet(os.path.join(output_path, \"results.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the output file\n",
    "\n",
    "output_df = pd.read_parquet(\"./data/export/results.parquet\")\n",
    "output_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "protein-feature-extraction-NoVdeDG9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
