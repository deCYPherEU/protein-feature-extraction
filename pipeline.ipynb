{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protein feature extraction pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will contain the pipeline for extracting features from protein sequences. It will be used as a way to show the output without needing to run the `pipeline.py` file locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import logging\n",
    "from fondant.pipeline import Pipeline\n",
    "from fondant.pipeline.runner import DockerRunner\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "from config import MOCK_DATA_PATH_FONDANT\n",
    "\n",
    "# check if the manifest file is removed.\n",
    "REMOVED_MANIFEST = False\n",
    "\n",
    "# check if the output folder exists\n",
    "OUTPUT_FOLDER = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Mock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python utils/generate_mock_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MNQRGMPIQSLVTNVKINRLEENDCIHTRHRVRPGRTDGKNLHAMM...</td>\n",
       "      <td>Seq1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MAGLKPEVPLHDGINKFGKSDFAGQEGPKIVTTTDKALLVANGALK...</td>\n",
       "      <td>Seq2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MVDLKKELKNFVDSDFPGSPKQEAQGIDVRILLSFNNAAFREALII...</td>\n",
       "      <td>Seq3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MELILAKARLEFECDWGLLMLEPCVPPTKIFADRNYAVGVMFESDK...</td>\n",
       "      <td>Seq4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MRVLCDGSTGYACAKNTRIRFREKVASVLAKIQGYEQTFPHHMPNM...</td>\n",
       "      <td>Seq5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sequence  name\n",
       "0  MNQRGMPIQSLVTNVKINRLEENDCIHTRHRVRPGRTDGKNLHAMM...  Seq1\n",
       "1  MAGLKPEVPLHDGINKFGKSDFAGQEGPKIVTTTDKALLVANGALK...  Seq2\n",
       "2  MVDLKKELKNFVDSDFPGSPKQEAQGIDVRILLSFNNAAFREALII...  Seq3\n",
       "3  MELILAKARLEFECDWGLLMLEPCVPPTKIFADRNYAVGVMFESDK...  Seq4\n",
       "4  MRVLCDGSTGYACAKNTRIRFREKVASVLAKIQGYEQTFPHHMPNM...  Seq5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show content of the mock data\n",
    "import pandas as pd\n",
    "mock_df = pd.read_parquet(\".\" + MOCK_DATA_PATH_FONDANT)  # dot added to make it relative to the current directory\n",
    "mock_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new pipeline\n",
    "\n",
    "BASE_PATH = \".fondant\"\n",
    "PIPELINE_NAME = \"feature_extraction_pipeline\"\n",
    "\n",
    "pipeline = Pipeline(\n",
    "\tname=PIPELINE_NAME,\n",
    "\tbase_path=BASE_PATH,\n",
    "\tdescription=\"A pipeline to extract features from protein sequences.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-07-10 12:15:32,952 | fondant.pipeline.pipeline | INFO] The consumes section of the component spec is not defined. Can not infer consumes of the OperationSpec. Please define a consumes section in the dataset interface. \n"
     ]
    }
   ],
   "source": [
    "# Read the dataset\n",
    "\n",
    "dataset = pipeline.read(\n",
    "\t\"load_from_parquet\",\n",
    "\targuments={\n",
    "\t\t\"dataset_uri\": MOCK_DATA_PATH_FONDANT,\n",
    "\t},\n",
    "\tproduces={\n",
    "\t\t\"sequence\": pa.string()\n",
    "\t}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components\n",
    "\n",
    "---\n",
    "\n",
    "### generate_protein_sequence_checksum_component\n",
    "\n",
    "This component generates a checksum for the protein sequence.\n",
    "\n",
    "---\n",
    "\n",
    "### biopython_component\n",
    "\n",
    "Extracts features from the protein sequence using Biopython.\n",
    "\n",
    "---\n",
    "\n",
    "### iFeatureOmega_component\n",
    "\n",
    "Extracts features from the protein sequence using the [iFeatureOmega-CLI GitHub repo](https://github.com/Superzchen/iFeatureOmega-CLI). Arguments are used to specify the type of features to extract.\n",
    "\n",
    "---\n",
    "\n",
    "### filter_pdb_component\n",
    "\n",
    "Filters PDB files that are already predicted to avoid redundant predictions. Arguments need to be specified before running the pipeline:\n",
    "```json\n",
    "\"storage_type\": \"local\",\n",
    "\"pdb_path\": \"/data/<your-pdb-folder-path>\",\n",
    "\"bucket_name\": \"your-bucket-name\",\n",
    "\"project_id\": \"your-project-id\",\n",
    "\"google_cloud_credentials_path\": \"/data/<your-credentials>.json\"\n",
    "```\n",
    "\n",
    "If only using local, keep bucket_name, project_id, and google_cloud_credentials_path as empty strings. Using remote requires a Google Cloud Storage bucket with credentials and a project ID.\n",
    "\n",
    "---\n",
    "\n",
    "### predict_protein_3D_structure_component\n",
    "\n",
    "Predicts the 3D structure of the protein using ESMFold. This component requires a `.env` file with the following variables:\n",
    "```env\n",
    "HF_API_KEY=\"\"\n",
    "HF_ENDPOINT_URL=\"\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### store_pdb_component\n",
    "\n",
    "Stores the PDB files in the provided storage_type. Arguments need to be specified before running the pipeline:\n",
    "```json\n",
    "\"storage_type\": \"local\",\n",
    "\"pdb_path\": \"/data/<your-pdb-folder-path>\",\n",
    "\"bucket_name\": \"your-bucket-name\",\n",
    "\"project_id\": \"your-project-id\",\n",
    "\"google_cloud_credentials_path\": \"/data/<your-credentials>.json\"\n",
    "```\n",
    "\n",
    "If only using local, keep bucket_name, project_id, and google_cloud_credentials_path as empty strings. Using remote requires a Google Cloud Storage bucket with credentials and a project ID.\n",
    "\n",
    "---\n",
    "\n",
    "### msa_component\n",
    "\n",
    "Generates the multiple sequence alignment for the protein sequence using [Clustal Omega](http://www.clustal.org/omega/). It's recommended to use a smaller number of sequences or none at all due to potential time consumption.\n",
    "\n",
    "---\n",
    "\n",
    "### unikp_component\n",
    "\n",
    "Uses the UniKP endpoint on HuggingFace to predict the kinetic parameters of a protein sequence and substrate (SMILES) combination. See README for the description of the contents of this file.\n",
    "\n",
    "```yaml\n",
    "\"protein_smiles_path\": \"/data/<path_protein_smiles>\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### peptide_component\n",
    "\n",
    "Calculates the features from the protein sequence using the `peptides` package.\n",
    "\n",
    "---\n",
    "\n",
    "### deepTMpred_component\n",
    "\n",
    "Predicts the transmembrane regions of the protein sequence using the [DeepTMpred GitHub repository](https://github.com/ISYSLAB-HUST/DeepTMpred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-07-10 12:29:23,356 | fondant.pipeline.pipeline | WARNING] Component `Biopython component` has an image tag set to latest. Caching for the component will be disabled to prevent unpredictable behavior due to images updates\n",
      "[2024-07-10 12:29:23,363 | fondant.pipeline.pipeline | WARNING] Component `Generate Protein Sequence Checksum Component` has an image tag set to latest. Caching for the component will be disabled to prevent unpredictable behavior due to images updates\n"
     ]
    },
    {
     "ename": "InvalidComponentSpec",
     "evalue": "{'type': 'map_'} is not valid under any of the given schemas\n\nFailed validating 'anyOf' in schema['properties']['produces']['additionalProperties']:\n    {'anyOf': [{'$ref': '#/definitions/field'},\n               {'$ref': '#/definitions/additionalProperties'}]}\n\nOn instance['produces']['unikp_kinetic_prediction']:\n    {'type': 'map_'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidComponentSpec\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./components/biopython_component\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./components/generate_protein_sequence_checksum_component\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;43;03m# ).apply(\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;43;03m#     \"./components/iFeatureOmega_component\",\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;43;03m#     # currently forcing the number of rows to 5, but there needs to be a better way to do this, see readme for more info\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;43;03m#     input_partition_rows=5,\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;43;03m#     arguments={\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;43;03m#         \"descriptors\": [\"AAC\", \"CTDC\", \"CTDT\"]\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;43;03m#     }\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;43;03m# ).apply(\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;43;03m#     \"./components/filter_pdb_component\",\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;43;03m#     arguments={\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;43;03m#         \"method\": \"local\",\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;43;03m#         \"local_pdb_path\": \"/data/pdb_files\",\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;43;03m#         \"bucket_name\": \"\",\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;43;03m#         \"project_id\": \"\",\u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;43;03m#         \"google_cloud_credentials_path\": \"\"\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;43;03m#     }\u001b[39;49;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;43;03m# ).apply(\u001b[39;49;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;43;03m#     \"./components/predict_protein_3D_structure_component\",\u001b[39;49;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;43;03m# ).apply(\u001b[39;49;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;43;03m#     \"./components/store_pdb_component\",\u001b[39;49;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;43;03m#     arguments={\u001b[39;49;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;43;03m#         \"method\": \"local\",\u001b[39;49;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;43;03m#         \"local_pdb_path\": \"/data/pdb_files/\",\u001b[39;49;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;43;03m#         \"bucket_name\": \"elated-chassis-400207_dbtl_pipeline_outputs\",\u001b[39;49;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;43;03m#         \"project_id\": \"elated-chassis-400207\",\u001b[39;49;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;43;03m#         \"google_cloud_credentials_path\": \"/data/google_cloud_credentials.json\"\u001b[39;49;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;43;03m#     }\u001b[39;49;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;43;03m# ).apply(\u001b[39;49;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;43;03m#     \"./components/msa_component\",\u001b[39;49;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;43;03m#     input_partition_rows='10000'\u001b[39;49;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;43;03m# ).apply(\u001b[39;49;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;43;03m#     \"./components/pdb_features_component\"\u001b[39;49;00m\n\u001b[1;32m     37\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./components/unikp_component\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43marguments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtarget_molecule_smiles\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/data/target_molecule_smiles.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./components/peptide_features_component\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# ).apply(\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m#     \"./components/DeepTMpred_component\"\u001b[39;00m\n\u001b[1;32m     46\u001b[0m )\n",
      "File \u001b[0;32m~/Software/deCYPher/protein-feature-extraction/.venv/lib/python3.10/site-packages/fondant/pipeline/pipeline.py:798\u001b[0m, in \u001b[0;36mDataset.apply\u001b[0;34m(self, ref, consumes, produces, arguments, input_partition_rows, resources, cache)\u001b[0m\n\u001b[1;32m    700\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m    701\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    702\u001b[0m     ref: t\u001b[38;5;241m.\u001b[39mAny,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    709\u001b[0m     cache: t\u001b[38;5;241m.\u001b[39mOptional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    710\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    711\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    712\u001b[0m \u001b[38;5;124;03m    Apply the provided component on the dataset.\u001b[39;00m\n\u001b[1;32m    713\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    796\u001b[0m \u001b[38;5;124;03m        An intermediate dataset.\u001b[39;00m\n\u001b[1;32m    797\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 798\u001b[0m     operation \u001b[38;5;241m=\u001b[39m \u001b[43mComponentOp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_ref\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m        \u001b[49m\u001b[43mref\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproduces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproduces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconsumes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconsumes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m        \u001b[49m\u001b[43marguments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marguments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_partition_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_partition_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresources\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(operation)\n",
      "File \u001b[0;32m~/Software/deCYPher/protein-feature-extraction/.venv/lib/python3.10/site-packages/fondant/pipeline/pipeline.py:324\u001b[0m, in \u001b[0;36mComponentOp.from_ref\u001b[0;34m(cls, ref, fields, **kwargs)\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidLightweightComponent(msg)\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ref, (\u001b[38;5;28mstr\u001b[39m, Path)):\n\u001b[0;32m--> 324\u001b[0m     operation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_component_yaml\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m        \u001b[49m\u001b[43mref\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mInvalid reference type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(ref)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;124m        Expected a string, Path, or a lightweight component class.\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n",
      "File \u001b[0;32m~/Software/deCYPher/protein-feature-extraction/.venv/lib/python3.10/site-packages/fondant/pipeline/pipeline.py:191\u001b[0m, in \u001b[0;36mComponentOp.from_component_yaml\u001b[0;34m(cls, path, fields, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    189\u001b[0m     component_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_registry_path(\u001b[38;5;28mstr\u001b[39m(path))\n\u001b[0;32m--> 191\u001b[0m component_spec \u001b[38;5;241m=\u001b[39m \u001b[43mComponentSpec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcomponent_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOMPONENT_SPEC_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m image \u001b[38;5;241m=\u001b[39m Image(\n\u001b[1;32m    196\u001b[0m     base_image\u001b[38;5;241m=\u001b[39mcomponent_spec\u001b[38;5;241m.\u001b[39mimage,\n\u001b[1;32m    197\u001b[0m )\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\n\u001b[1;32m    200\u001b[0m     image\u001b[38;5;241m=\u001b[39mimage,\n\u001b[1;32m    201\u001b[0m     component_spec\u001b[38;5;241m=\u001b[39mcomponent_spec,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    205\u001b[0m )\n",
      "File \u001b[0;32m~/Software/deCYPher/protein-feature-extraction/.venv/lib/python3.10/site-packages/fondant/core/component_spec.py:161\u001b[0m, in \u001b[0;36mComponentSpec.from_file\u001b[0;34m(cls, path)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file_:\n\u001b[1;32m    160\u001b[0m     specification \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39msafe_load(file_)\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspecification\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Software/deCYPher/protein-feature-extraction/.venv/lib/python3.10/site-packages/fondant/core/component_spec.py:172\u001b[0m, in \u001b[0;36mComponentSpec.from_dict\u001b[0;34m(cls, component_spec_dict)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load the component spec from a dictionary.\"\"\"\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcomponent_spec_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    174\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid component spec: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Software/deCYPher/protein-feature-extraction/.venv/lib/python3.10/site-packages/fondant/core/component_spec.py:125\u001b[0m, in \u001b[0;36mComponentSpec.__init__\u001b[0;34m(self, name, image, description, consumes, produces, previous_index, args, tags)\u001b[0m\n\u001b[1;32m    122\u001b[0m     spec_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m args\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_specification \u001b[38;5;241m=\u001b[39m spec_dict\n\u001b[0;32m--> 125\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_spec\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Software/deCYPher/protein-feature-extraction/.venv/lib/python3.10/site-packages/fondant/core/component_spec.py:154\u001b[0m, in \u001b[0;36mComponentSpec._validate_spec\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    152\u001b[0m     validator\u001b[38;5;241m.\u001b[39mvalidate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_specification)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m jsonschema\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidComponentSpec\u001b[38;5;241m.\u001b[39mcreate_from(e)\n",
      "\u001b[0;31mInvalidComponentSpec\u001b[0m: {'type': 'map_'} is not valid under any of the given schemas\n\nFailed validating 'anyOf' in schema['properties']['produces']['additionalProperties']:\n    {'anyOf': [{'$ref': '#/definitions/field'},\n               {'$ref': '#/definitions/additionalProperties'}]}\n\nOn instance['produces']['unikp_kinetic_prediction']:\n    {'type': 'map_'}"
     ]
    }
   ],
   "source": [
    "_ = dataset.apply(\n",
    "    \"./components/biopython_component\"\n",
    ").apply(\n",
    "    \"./components/generate_protein_sequence_checksum_component\"\n",
    "# ).apply(\n",
    "#     \"./components/iFeatureOmega_component\",\n",
    "#     # currently forcing the number of rows to 5, but there needs to be a better way to do this, see readme for more info\n",
    "#     input_partition_rows=5,\n",
    "#     arguments={\n",
    "#         \"descriptors\": [\"AAC\", \"CTDC\", \"CTDT\"]\n",
    "#     }\n",
    "# ).apply(\n",
    "#     \"./components/filter_pdb_component\",\n",
    "#     arguments={\n",
    "#         \"method\": \"local\",\n",
    "#         \"local_pdb_path\": \"/data/pdb_files\",\n",
    "#         \"bucket_name\": \"\",\n",
    "#         \"project_id\": \"\",\n",
    "#         \"google_cloud_credentials_path\": \"\"\n",
    "#     }\n",
    "# ).apply(\n",
    "#     \"./components/predict_protein_3D_structure_component\",\n",
    "# ).apply(\n",
    "#     \"./components/store_pdb_component\",\n",
    "#     arguments={\n",
    "#         \"method\": \"local\",\n",
    "#         \"local_pdb_path\": \"/data/pdb_files/\",\n",
    "#         \"bucket_name\": \"elated-chassis-400207_dbtl_pipeline_outputs\",\n",
    "#         \"project_id\": \"elated-chassis-400207\",\n",
    "#         \"google_cloud_credentials_path\": \"/data/google_cloud_credentials.json\"\n",
    "#     }\n",
    "# ).apply(\n",
    "#     \"./components/msa_component\",\n",
    "#     input_partition_rows='10000'\n",
    "# ).apply(\n",
    "#     \"./components/pdb_features_component\"\n",
    ").apply(\n",
    "    \"./components/unikp_component\",\n",
    "    arguments={\n",
    "        \"target_molecule_smiles\": \"/data/target_molecule_smiles.json\",\n",
    "    },\n",
    ").apply(\n",
    "    \"./components/peptide_features_component\"\n",
    "# ).apply(\n",
    "#     \"./components/DeepTMpred_component\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pipeline.py` file needs to be run using the command line. The following command will run the pipeline:\n",
    "\n",
    "```bash\n",
    "fondant < full_path_to_pipeline.py >\\data:/data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-07-10 12:28:38,501 | root | INFO] Found reference to un-compiled pipeline... compiling\n",
      "[2024-07-10 12:28:38,502 | fondant.pipeline.compiler | INFO] Compiling feature_extraction_pipeline to .fondant/compose.yaml\n",
      "[2024-07-10 12:28:38,503 | fondant.pipeline.compiler | INFO] Base path found on local system, setting up .fondant as mount volume\n",
      "[2024-07-10 12:28:38,504 | fondant.pipeline.pipeline | INFO] Sorting pipeline component graph topologically.\n",
      "[2024-07-10 12:28:38,514 | fondant.pipeline.pipeline | INFO] All pipeline component specifications match.\n",
      "[2024-07-10 12:28:38,515 | fondant.pipeline.compiler | INFO] Compiling service for load_from_parquet\n",
      "[2024-07-10 12:28:38,517 | fondant.pipeline.compiler | INFO] Compiling service for biopython_component\n",
      "[2024-07-10 12:28:38,517 | fondant.pipeline.compiler | INFO] Found Dockerfile for biopython_component, adding build step.\n",
      "[2024-07-10 12:28:38,518 | fondant.pipeline.compiler | INFO] Compiling service for generate_protein_sequence_checksum_component\n",
      "[2024-07-10 12:28:38,518 | fondant.pipeline.compiler | INFO] Found Dockerfile for generate_protein_sequence_checksum_component, adding build step.\n",
      "[2024-07-10 12:28:38,519 | fondant.pipeline.compiler | INFO] Compiling service for unikp_component\n",
      "[2024-07-10 12:28:38,519 | fondant.pipeline.compiler | INFO] Found Dockerfile for unikp_component, adding build step.\n",
      "[2024-07-10 12:28:38,520 | fondant.pipeline.compiler | INFO] Compiling service for peptide_features_component\n",
      "[2024-07-10 12:28:38,520 | fondant.pipeline.compiler | INFO] Found Dockerfile for peptide_features_component, adding build step.\n",
      "[2024-07-10 12:28:38,539 | fondant.pipeline.compiler | INFO] Successfully compiled to .fondant/compose.yaml\n",
      "time=\"2024-07-10T12:28:38+02:00\" level=warning msg=\"/home/pietercoussement/Software/deCYPher/protein-feature-extraction/.fondant/compose.yaml: `version` is obsolete\"\n",
      " load_from_parquet Pulling \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pipeline run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " load_from_parquet Pulled \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0 building with \"desktop-linux\" instance using docker driver\n",
      "\n",
      "#1 [biopython_component internal] load build definition from Dockerfile\n",
      "#1 transferring dockerfile: 480B done\n",
      "#1 DONE 0.0s\n",
      "\n",
      "#2 [biopython_component internal] load metadata for docker.io/fndnt/fondant:0.11.dev5-py3.10\n",
      "#2 DONE 0.0s\n",
      "\n",
      "#3 [biopython_component internal] load .dockerignore\n",
      "#3 transferring context: 2B done\n",
      "#3 DONE 0.0s\n",
      "\n",
      "#4 [biopython_component 1/6] FROM docker.io/fndnt/fondant:0.11.dev5-py3.10\n",
      "#4 DONE 0.0s\n",
      "\n",
      "#5 [biopython_component internal] load build context\n",
      "#5 transferring context: 3.25kB done\n",
      "#5 DONE 0.0s\n",
      "\n",
      "#6 [biopython_component 4/6] RUN pip3 install --no-cache-dir -r requirements.txt\n",
      "#6 CACHED\n",
      "\n",
      "#7 [biopython_component 2/6] RUN apt-get update &&     apt-get upgrade -y &&     apt-get install git -y\n",
      "#7 CACHED\n",
      "\n",
      "#8 [biopython_component 3/6] COPY requirements.txt ./\n",
      "#8 CACHED\n",
      "\n",
      "#9 [biopython_component 5/6] WORKDIR /component/src\n",
      "#9 CACHED\n",
      "\n",
      "#10 [biopython_component 6/6] COPY src/ .\n",
      "#10 CACHED\n",
      "\n",
      "#11 [biopython_component] exporting to image\n",
      "#11 exporting layers done\n",
      "#11 writing image sha256:2c9c5b8de3bcb0e6b229292528a316cbcdecab60c38145e45ffba2465d53822a done\n",
      "#11 naming to docker.io/library/feature_extraction_pipeline-biopython_component done\n",
      "#11 DONE 0.0s\n",
      "\n",
      "#12 [generate_protein_sequence_checksum_component internal] load build definition from Dockerfile\n",
      "#12 transferring dockerfile: 480B done\n",
      "#12 DONE 0.0s\n",
      "\n",
      "#2 [generate_protein_sequence_checksum_component internal] load metadata for docker.io/fndnt/fondant:0.11.dev5-py3.10\n",
      "#2 DONE 0.0s\n",
      "\n",
      "#13 [generate_protein_sequence_checksum_component internal] load .dockerignore\n",
      "#13 transferring context: 2B done\n",
      "#13 DONE 0.0s\n",
      "\n",
      "#4 [generate_protein_sequence_checksum_component 1/6] FROM docker.io/fndnt/fondant:0.11.dev5-py3.10\n",
      "#4 DONE 0.0s\n",
      "\n",
      "#14 [generate_protein_sequence_checksum_component internal] load build context\n",
      "#14 transferring context: 1.14kB done\n",
      "#14 DONE 0.0s\n",
      "\n",
      "#15 [generate_protein_sequence_checksum_component 3/6] COPY requirements.txt ./\n",
      "#15 CACHED\n",
      "\n",
      "#16 [generate_protein_sequence_checksum_component 4/6] RUN pip3 install --no-cache-dir -r requirements.txt\n",
      "#16 CACHED\n",
      "\n",
      "#17 [generate_protein_sequence_checksum_component 5/6] WORKDIR /component/src\n",
      "#17 CACHED\n",
      "\n",
      "#7 [generate_protein_sequence_checksum_component 2/6] RUN apt-get update &&     apt-get upgrade -y &&     apt-get install git -y\n",
      "#7 CACHED\n",
      "\n",
      "#18 [generate_protein_sequence_checksum_component 6/6] COPY src/ .\n",
      "#18 CACHED\n",
      "\n",
      "#19 [generate_protein_sequence_checksum_component] exporting to image\n",
      "#19 exporting layers done\n",
      "#19 writing image sha256:56049561c81c2bee7af20822240fce2d0f54b84851a116476e54bfc4447cf872 done\n",
      "#19 naming to docker.io/library/feature_extraction_pipeline-generate_protein_sequence_checksum_component done\n",
      "#19 DONE 0.0s\n",
      "\n",
      "#20 [unikp_component internal] load build definition from Dockerfile\n",
      "#20 transferring dockerfile: 538B done\n",
      "#20 DONE 0.0s\n",
      "\n",
      "#2 [unikp_component internal] load metadata for docker.io/fndnt/fondant:0.11.dev5-py3.10\n",
      "#2 DONE 0.0s\n",
      "\n",
      "#21 [unikp_component internal] load .dockerignore\n",
      "#21 transferring context: 2B done\n",
      "#21 DONE 0.0s\n",
      "\n",
      "#4 [unikp_component 1/7] FROM docker.io/fndnt/fondant:0.11.dev5-py3.10\n",
      "#4 DONE 0.0s\n",
      "\n",
      "#22 [unikp_component internal] load build context\n",
      "#22 transferring context: 3.89kB done\n",
      "#22 DONE 0.0s\n",
      "\n",
      "#7 [unikp_component 2/7] RUN apt-get update &&     apt-get upgrade -y &&     apt-get install git -y\n",
      "#7 CACHED\n",
      "\n",
      "#23 [unikp_component 6/7] COPY .env .\n",
      "#23 CACHED\n",
      "\n",
      "#24 [unikp_component 3/7] COPY requirements.txt ./\n",
      "#24 CACHED\n",
      "\n",
      "#25 [unikp_component 4/7] RUN pip3 install --no-cache-dir -r requirements.txt\n",
      "#25 CACHED\n",
      "\n",
      "#26 [unikp_component 5/7] WORKDIR /component/src\n",
      "#26 CACHED\n",
      "\n",
      "#27 [unikp_component 7/7] COPY src/ .\n",
      "#27 CACHED\n",
      "\n",
      "#28 [unikp_component] exporting to image\n",
      "#28 exporting layers done\n",
      "#28 writing image sha256:89924681c5f65dfaa2b1d7ac0cfd9bb09f2db3734826f77e81287f15ce16fb06 done\n",
      "#28 naming to docker.io/library/feature_extraction_pipeline-unikp_component done\n",
      "#28 DONE 0.0s\n",
      "\n",
      "#29 [peptide_features_component internal] load build definition from Dockerfile\n",
      "#29 transferring dockerfile: 480B done\n",
      "#29 DONE 0.0s\n",
      "\n",
      "#2 [peptide_features_component internal] load metadata for docker.io/fndnt/fondant:0.11.dev5-py3.10\n",
      "#2 DONE 0.0s\n",
      "\n",
      "#30 [peptide_features_component internal] load .dockerignore\n",
      "#30 transferring context: 2B done\n",
      "#30 DONE 0.0s\n",
      "\n",
      "#4 [peptide_features_component 1/6] FROM docker.io/fndnt/fondant:0.11.dev5-py3.10\n",
      "#4 DONE 0.0s\n",
      "\n",
      "#31 [peptide_features_component internal] load build context\n",
      "#31 transferring context: 2.35kB done\n",
      "#31 DONE 0.0s\n",
      "\n",
      "#32 [peptide_features_component 5/6] WORKDIR /component/src\n",
      "#32 CACHED\n",
      "\n",
      "#7 [peptide_features_component 2/6] RUN apt-get update &&     apt-get upgrade -y &&     apt-get install git -y\n",
      "#7 CACHED\n",
      "\n",
      "#33 [peptide_features_component 3/6] COPY requirements.txt ./\n",
      "#33 CACHED\n",
      "\n",
      "#34 [peptide_features_component 4/6] RUN pip3 install --no-cache-dir -r requirements.txt\n",
      "#34 CACHED\n",
      "\n",
      "#35 [peptide_features_component 6/6] COPY src/ .\n",
      "#35 CACHED\n",
      "\n",
      "#36 [peptide_features_component] exporting to image\n",
      "#36 exporting layers done\n",
      "#36 writing image sha256:50afff57e09aa3d6457da0f12c13d81dc033b29f3ad3449c74110e0b5cf6a46b done\n",
      "#36 naming to docker.io/library/feature_extraction_pipeline-peptide_features_component done\n",
      "#36 DONE 0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Container feature_extraction_pipeline-load_from_parquet-1  Recreate\n",
      " Container feature_extraction_pipeline-load_from_parquet-1  Recreated\n",
      " Container feature_extraction_pipeline-biopython_component-1  Recreate\n",
      " Container feature_extraction_pipeline-biopython_component-1  Recreated\n",
      " Container feature_extraction_pipeline-generate_protein_sequence_checksum_component-1  Recreate\n",
      " Container feature_extraction_pipeline-generate_protein_sequence_checksum_component-1  Recreated\n",
      " Container feature_extraction_pipeline-unikp_component-1  Recreate\n",
      " Container feature_extraction_pipeline-unikp_component-1  Recreated\n",
      " Container feature_extraction_pipeline-peptide_features_component-1  Recreate\n",
      " Container feature_extraction_pipeline-peptide_features_component-1  Recreated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attaching to biopython_component-1, generate_protein_sequence_checksum_component-1, load_from_parquet-1, peptide_features_component-1, unikp_component-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load_from_parquet-1                             | [2024-07-10 10:28:43,603 | fondant.cli | INFO] Component `LoadFromParquet` found in module main\n",
      "load_from_parquet-1                             | [2024-07-10 10:28:43,611 | fondant.component.executor | INFO] Skipping component execution\n",
      "load_from_parquet-1                             | [2024-07-10 10:28:43,612 | fondant.component.executor | INFO] Matching execution detected for component. The last execution of the component originated from `feature_extraction_pipeline-20240701161943`.\n",
      "load_from_parquet-1                             | [2024-07-10 10:28:43,614 | fondant.component.executor | INFO] Saving output manifest to /.fondant/feature_extraction_pipeline/feature_extraction_pipeline-20240710122838/load_from_parquet/manifest.json\n",
      "load_from_parquet-1                             | [2024-07-10 10:28:43,614 | fondant.component.executor | INFO] Writing cache key with manifest reference to /.fondant/feature_extraction_pipeline/cache/d41a53a1be34f7fd0a29002364c9f666.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[Kload_from_parquet-1 exited with code 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gracefully stopping... (press Ctrl+C again to force)\n",
      " Container feature_extraction_pipeline-peptide_features_component-1  Stopping\n",
      " Container feature_extraction_pipeline-peptide_features_component-1  Stopped\n",
      " Container feature_extraction_pipeline-unikp_component-1  Stopping\n",
      " Container feature_extraction_pipeline-unikp_component-1  Stopped\n",
      " Container feature_extraction_pipeline-generate_protein_sequence_checksum_component-1  Stopping\n",
      " Container feature_extraction_pipeline-generate_protein_sequence_checksum_component-1  Stopped\n",
      " Container feature_extraction_pipeline-biopython_component-1  Stopping\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 13\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# import shutil\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# remove the most recent output folder if the manifest file is removed\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# get current full path to the project\u001b[39;00m\n\u001b[1;32m     11\u001b[0m mounted_data \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:/data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m \u001b[43mDockerRunner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_volumes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmounted_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Software/deCYPher/protein-feature-extraction/.venv/lib/python3.10/site-packages/fondant/pipeline/runner.py:89\u001b[0m, in \u001b[0;36mDockerRunner.run\u001b[0;34m(self, input, extra_volumes, build_args, auth_provider)\u001b[0m\n\u001b[1;32m     81\u001b[0m     compiler \u001b[38;5;241m=\u001b[39m DockerCompiler()\n\u001b[1;32m     82\u001b[0m     compiler\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m     84\u001b[0m         output_path\u001b[38;5;241m=\u001b[39moutput_path,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m         auth_provider\u001b[38;5;241m=\u001b[39mauth_provider,\n\u001b[1;32m     88\u001b[0m     )\n\u001b[0;32m---> 89\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(\u001b[38;5;28minput\u001b[39m)\n",
      "File \u001b[0;32m~/Software/deCYPher/protein-feature-extraction/.venv/lib/python3.10/site-packages/fondant/pipeline/runner.py:48\u001b[0m, in \u001b[0;36mDockerRunner._run\u001b[0;34m(self, input_spec, *args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting pipeline run...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# copy the current environment with the DOCKER_DEFAULT_PLATFORM argument\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# nosec\u001b[39;49;00m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDOCKER_DEFAULT_PLATFORM\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlinux/amd64\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished pipeline run.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:347\u001b[0m, in \u001b[0;36mcall\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m p:\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 347\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m:  \u001b[38;5;66;03m# Including KeyboardInterrupt, wait handled that.\u001b[39;00m\n\u001b[1;32m    349\u001b[0m         p\u001b[38;5;241m.\u001b[39mkill()\n",
      "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:1209\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1207\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m _time() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1211\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m     \u001b[38;5;66;03m# The first keyboard interrupt waits briefly for the child to\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m     \u001b[38;5;66;03m# exit under the common assumption that it also received the ^C\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m     \u001b[38;5;66;03m# generated SIGINT and will exit rapidly.\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:1959\u001b[0m, in \u001b[0;36mPopen._wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1958\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# Another thread waited.\u001b[39;00m\n\u001b[0;32m-> 1959\u001b[0m (pid, sts) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1960\u001b[0m \u001b[38;5;66;03m# Check the pid and loop as waitpid has been known to\u001b[39;00m\n\u001b[1;32m   1961\u001b[0m \u001b[38;5;66;03m# return 0 even without WNOHANG in odd situations.\u001b[39;00m\n\u001b[1;32m   1962\u001b[0m \u001b[38;5;66;03m# http://bugs.python.org/issue14396.\u001b[39;00m\n\u001b[1;32m   1963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pid \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid:\n",
      "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:1917\u001b[0m, in \u001b[0;36mPopen._try_wait\u001b[0;34m(self, wait_flags)\u001b[0m\n\u001b[1;32m   1915\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[39;00m\n\u001b[1;32m   1916\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1917\u001b[0m     (pid, sts) \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitpid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_flags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1918\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mChildProcessError\u001b[39;00m:\n\u001b[1;32m   1919\u001b[0m     \u001b[38;5;66;03m# This happens if SIGCLD is set to be ignored or waiting\u001b[39;00m\n\u001b[1;32m   1920\u001b[0m     \u001b[38;5;66;03m# for child processes has otherwise been disabled for our\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m     \u001b[38;5;66;03m# process.  This child is dead, we can't get the status.\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m     pid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import shutil\n",
    "\n",
    "# remove the most recent output folder if the manifest file is removed\n",
    "# without a manifest file in the most recent output folder, the pipeline cannot be run\n",
    "# if OUTPUT_FOLDER and REMOVED_MANIFEST:\n",
    "# \tshutil.rmtree(OUTPUT_FOLDER)\n",
    "# \t# remove cache\n",
    "# \tshutil.rmtree(os.path.join(BASE_PATH, PIPELINE_NAME, \"cache\"))\n",
    "\n",
    "# get current full path to the project\n",
    "mounted_data = os.path.join(os.path.abspath(\"data\"), \":/data\")\n",
    "\n",
    "DockerRunner().run(input=pipeline, extra_volumes=mounted_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "The following results have been taken from the output of the pipeline, which is stored in the `.fondant` directory. This directory contains the output of each component, together with the cache of the previous run. Currently, the pipeline doesn't implement the `write_to_file` component, so the results will be taken individually from the output of each component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-07-10 12:16:03,594 | root | INFO] Last folder: .fondant/feature_extraction_pipeline/feature_extraction_pipeline-20240710121533\n"
     ]
    }
   ],
   "source": [
    "# find the most recent output folder\n",
    "# get the most recent folder in the folder named: BASE_PATH + PIPELINE_NAME + PIPELINE_NAME-<timestamp>\n",
    "matching_folders = glob.glob(f\"{BASE_PATH}/{PIPELINE_NAME}/{PIPELINE_NAME}-*\")\n",
    "\n",
    "if matching_folders:\n",
    "    last_folder = max(matching_folders, key=os.path.getctime)\n",
    "\n",
    "logging.info(f\"Last folder: {last_folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def merge_parquet_folders(folder_path):\n",
    "    df_list = []\n",
    "\n",
    "    for folder in Path(folder_path).iterdir():\n",
    "        if folder.is_dir():\n",
    "            logging.info(f\"Reading parquet partitions from: {folder}\")\n",
    "            parquet_files = list(folder.glob(\"*.parquet\"))\n",
    "            logging.info(f\"Found {len(parquet_files)} parquet files\")\n",
    "            dfs = [pd.read_parquet(file) for file in parquet_files]\n",
    "            dfs = [x for x in dfs if not x.empty]\n",
    "            if len(dfs) == 0:\n",
    "                continue\n",
    "            df = pd.concat(dfs)\n",
    "            df_list.append(df)\n",
    "\n",
    "    return df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-07-10 12:16:03,691 | root | INFO] Reading parquet partitions from: .fondant/feature_extraction_pipeline/feature_extraction_pipeline-20240710121533/biopython_component\n",
      "[2024-07-10 12:16:03,702 | root | INFO] Found 8 parquet files\n",
      "[2024-07-10 12:16:03,778 | root | INFO] Reading parquet partitions from: .fondant/feature_extraction_pipeline/feature_extraction_pipeline-20240710121533/load_from_parquet\n",
      "[2024-07-10 12:16:03,785 | root | INFO] Found 0 parquet files\n",
      "[2024-07-10 12:16:03,787 | root | INFO] Reading parquet partitions from: .fondant/feature_extraction_pipeline/feature_extraction_pipeline-20240710121533/generate_protein_sequence_checksum_component\n",
      "[2024-07-10 12:16:03,791 | root | INFO] Found 8 parquet files\n"
     ]
    }
   ],
   "source": [
    "dataframe_list = merge_parquet_folders(last_folder)\n",
    "\n",
    "\n",
    "df_final = pd.concat(dataframe_list, axis=1)\n",
    "df_final = df_final.loc[:,~df_final.columns.duplicated()]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['pdb_string', 'msa_sequence'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf_final\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msequence\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpdb_string\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmsa_sequence\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mto_json(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_json.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecords\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Software/deCYPher/protein-feature-extraction/.venv/lib/python3.10/site-packages/pandas/core/frame.py:4096\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4094\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4095\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4096\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4098\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/Software/deCYPher/protein-feature-extraction/.venv/lib/python3.10/site-packages/pandas/core/indexes/base.py:6199\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6196\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6197\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6199\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6201\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6203\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/Software/deCYPher/protein-feature-extraction/.venv/lib/python3.10/site-packages/pandas/core/indexes/base.py:6251\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6250\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6251\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['pdb_string', 'msa_sequence'] not in index\""
     ]
    }
   ],
   "source": [
    "df_final[[\"sequence\", \"pdb_string\", \"msa_sequence\"]].to_json(\"test_json.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering out columns that are not properly stored in a csv\n",
    "columns_to_remove = ['pdb_string']\n",
    "df_final = df_final.drop(columns=columns_to_remove)\n",
    "\n",
    "# write to file\n",
    "df_final.to_csv(f\"{last_folder}/final_output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "protein-feature-extraction-NoVdeDG9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
