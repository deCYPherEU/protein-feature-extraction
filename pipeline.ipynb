{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protein feature extraction pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will contain the pipeline for extracting features from protein sequences. It will be used as a way to show the output without needing to run the `pipeline.py` file locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "from fondant.pipeline import Pipeline\n",
    "import os\n",
    "from config import MOCK_DATA_PATH_FONDANT\n",
    "\n",
    "# check if the manifest file is removed.\n",
    "REMOVED_MANIFEST = False\n",
    "\n",
    "# check if the output folder exists\n",
    "OUTPUT_FOLDER = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Mock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python utils/generate_mock_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MNQRGMPIQSLVTNVKINRLEENDCIHTRHRVRPGRTDGKNLHAMM...</td>\n",
       "      <td>Seq1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MAGLKPEVPLHDGINKFGKSDFAGQEGPKIVTTTDKALLVANGALK...</td>\n",
       "      <td>Seq2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MVDLKKELKNFVDSDFPGSPKQEAQGIDVRILLSFNNAAFREALII...</td>\n",
       "      <td>Seq3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MELILAKARLEFECDWGLLMLEPCVPPTKIFADRNYAVGVMFESDK...</td>\n",
       "      <td>Seq4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MRVLCDGSTGYACAKNTRIRFREKVASVLAKIQGYEQTFPHHMPNM...</td>\n",
       "      <td>Seq5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sequence  name\n",
       "0  MNQRGMPIQSLVTNVKINRLEENDCIHTRHRVRPGRTDGKNLHAMM...  Seq1\n",
       "1  MAGLKPEVPLHDGINKFGKSDFAGQEGPKIVTTTDKALLVANGALK...  Seq2\n",
       "2  MVDLKKELKNFVDSDFPGSPKQEAQGIDVRILLSFNNAAFREALII...  Seq3\n",
       "3  MELILAKARLEFECDWGLLMLEPCVPPTKIFADRNYAVGVMFESDK...  Seq4\n",
       "4  MRVLCDGSTGYACAKNTRIRFREKVASVLAKIQGYEQTFPHHMPNM...  Seq5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show content of the mock data\n",
    "import pandas as pd\n",
    "mock_df = pd.read_parquet(\".\" + MOCK_DATA_PATH_FONDANT)  # dot added to make it relative to the current directory\n",
    "mock_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new pipeline\n",
    "\n",
    "BASE_PATH = \".fondant\"\n",
    "PIPELINE_NAME = \"feature_extraction_pipeline\"\n",
    "\n",
    "pipeline = Pipeline(\n",
    "\tname=PIPELINE_NAME,\n",
    "\tbase_path=BASE_PATH,\n",
    "\tdescription=\"A pipeline to extract features from protein sequences.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "\n",
    "dataset = pipeline.read(\n",
    "\t\"load_from_parquet\",\n",
    "\targuments={\n",
    "\t\t\"dataset_uri\": MOCK_DATA_PATH_FONDANT,\n",
    "\t},\n",
    "\tproduces={\n",
    "\t\t\"sequence\": pa.string()\n",
    "\t}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components\n",
    "\n",
    "This section will contain the components that will be used in the pipeline.\n",
    "\n",
    "These are the components that will be used in the pipeline:\n",
    "\n",
    "- `generate_protein_sequence_checksum_component`: This component will generate a checksum for the protein sequence.\n",
    "\n",
    "- `biopython_component`: This component will extract features from the protein sequence using Biopython.\n",
    "\n",
    "- `iFeatureOmega_component`: This component will extract features from the protein sequence using iFeature Omega. This component uses arguments to specify the type of features to extract.\n",
    "\n",
    "- `filter_pdb_component`: This component will filter the PDB files that are already predicted, so the pipeline doesn't need to predict them again. You'll need to specify the following arguments before running the pipeline:\n",
    "```json\n",
    "\"storage_type\": \"local\",\n",
    "\"pdb_path\": \"/data/pdb_files/\",\n",
    "\"bucket_name\": \"your-bucket-name\",\n",
    "\"project_id\": \"your-project-id\",\n",
    "\"google_cloud_credentials_path\": \"/data/google_cloud_credentials.json\"\n",
    "```\n",
    "\n",
    "If you're only using `local`, then you can keep the `bucket_name`, `project_id` and `google_cloud_credentials_path` as empty strings. Using `remote` will require you to have a Google Cloud Storage bucket with credentials and a project ID.\n",
    "\n",
    "- `predict_protein_3D_structure_component`: This component will predict the 3D structure of the protein using ESMFold.\n",
    "\n",
    "- `store_pdb_component`: This component will store the PDB files in the provided `storage_type`. You'll need to specify the following arguments before running the pipeline:\n",
    "```json\n",
    "\"storage_type\": \"local\",\n",
    "\"pdb_path\": \"/data/pdb_files/\",\n",
    "\"bucket_name\": \"your-bucket-name\",\n",
    "\"project_id\": \"your-project-id\",\n",
    "\"google_cloud_credentials_path\": \"/data/google_cloud_credentials.json\"\n",
    "```\n",
    "\n",
    "If you're only using `local`, then you can keep the `bucket_name`, `project_id` and `google_cloud_credentials_path` as empty strings. Using `remote` will require you to have a Google Cloud Storage bucket with credentials and a project ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-04-18 19:13:51,557 | fondant.pipeline.pipeline | WARNING] Component `Biopython component` has an image tag set to latest. Caching for the component will be disabled to prevent unpredictable behavior due to images updates\n",
      "[2024-04-18 19:13:51,576 | fondant.pipeline.pipeline | WARNING] Component `Generate Protein Sequence Checksum Component` has an image tag set to latest. Caching for the component will be disabled to prevent unpredictable behavior due to images updates\n",
      "[2024-04-18 19:13:51,621 | fondant.pipeline.pipeline | WARNING] Component `iFeatureOmega component` has an image tag set to latest. Caching for the component will be disabled to prevent unpredictable behavior due to images updates\n"
     ]
    }
   ],
   "source": [
    "# Apply the components to the dataset\n",
    "\n",
    "_ = dataset.apply(\n",
    "\t\"./components/biopython_component\"\n",
    ").apply(\n",
    "\t\"./components/generate_protein_sequence_checksum_component\"\n",
    ").apply(\n",
    "\t\"./components/iFeatureOmega_component\",\n",
    "\t# currently forcing the number of rows to 5, see readme for more info\n",
    "\tinput_partition_rows=5,\n",
    "\targuments={\n",
    "\t\t\"descriptors\": [\"AAC\", \"CTDC\", \"CTDT\"]\n",
    "\t}\n",
    ").apply(\n",
    "\t\"./components/filter_pdb_component\",\n",
    "\targuments={\n",
    "\t\t\"method\": \"local\",\n",
    "\t\t\"local_pdb_path\": \"/data/pdb_files\",\n",
    "\t\t\"bucket_name\": \"elated-chassis-400207_dbtl_pipeline_outputs\",\n",
    "\t\t\"project_id\": \"elated-chassis-400207\",\n",
    "\t\t\"google_cloud_credentials_path\": \"/data/google_cloud_credentials.json\"\n",
    "\t}\n",
    ").apply(\n",
    "\t\"./components/predict_protein_3D_structure_component\",\n",
    ").apply(\n",
    "\t\"./components/store_pdb_component\",\n",
    "\targuments={\n",
    "\t\t\"method\": \"local\",\n",
    "\t\t\"local_pdb_path\": \"/data/pdb_files/\",\n",
    "\t\t\"bucket_name\": \"elated-chassis-400207_dbtl_pipeline_outputs\",\n",
    "\t\t\"project_id\": \"elated-chassis-400207\",\n",
    "\t\t\"google_cloud_credentials_path\": \"/data/google_cloud_credentials.json\"\n",
    "\t}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pipeline.py` file needs to be run using the command line. The following command will run the pipeline:\n",
    "\n",
    "```bash\n",
    "fondant < full_path_to_pipeline.py >\\data:/data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-04-18 19:18:04,150 | root | INFO] Found reference to un-compiled pipeline... compiling\n",
      "[2024-04-18 19:18:04,163 | fondant.pipeline.compiler | INFO] Compiling feature_extraction_pipeline to .fondant/compose.yaml\n",
      "[2024-04-18 19:18:04,163 | fondant.pipeline.compiler | INFO] Base path found on local system, setting up .fondant as mount volume\n",
      "[2024-04-18 19:18:04,171 | fondant.pipeline.pipeline | INFO] Sorting pipeline component graph topologically.\n",
      "[2024-04-18 19:18:04,211 | fondant.pipeline.pipeline | INFO] All pipeline component specifications match.\n",
      "[2024-04-18 19:18:04,212 | fondant.pipeline.compiler | INFO] Compiling service for load_from_parquet\n",
      "[2024-04-18 19:18:04,219 | fondant.pipeline.compiler | INFO] Compiling service for biopython_component\n",
      "[2024-04-18 19:18:04,219 | fondant.pipeline.compiler | INFO] Found Dockerfile for biopython_component, adding build step.\n",
      "[2024-04-18 19:18:04,219 | fondant.pipeline.compiler | INFO] Compiling service for generate_protein_sequence_checksum_component\n",
      "[2024-04-18 19:18:04,219 | fondant.pipeline.compiler | INFO] Found Dockerfile for generate_protein_sequence_checksum_component, adding build step.\n",
      "[2024-04-18 19:18:04,224 | fondant.pipeline.compiler | INFO] Compiling service for ifeatureomega_component\n",
      "[2024-04-18 19:18:04,224 | fondant.pipeline.compiler | INFO] Found Dockerfile for ifeatureomega_component, adding build step.\n",
      "[2024-04-18 19:18:04,246 | fondant.pipeline.compiler | INFO] Successfully compiled to .fondant/compose.yaml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pipeline run...\n",
      "Finished pipeline run.\n"
     ]
    }
   ],
   "source": [
    "from fondant.pipeline.runner import DockerRunner\n",
    "import shutil\n",
    "\n",
    "# remove the most recent output folder if the manifest file is removed\n",
    "# without a manifest file in the most recent output folder, the pipeline cannot be run\n",
    "if OUTPUT_FOLDER and REMOVED_MANIFEST:\n",
    "\tshutil.rmtree(OUTPUT_FOLDER)\n",
    "\t# remove cache\n",
    "\tshutil.rmtree(os.path.join(BASE_PATH, PIPELINE_NAME, \"cache\"))\n",
    "\n",
    "# get current full path to the project\n",
    "mounted_data = os.path.join(os.path.abspath(\"data\"), \":/data\")\n",
    "\n",
    "DockerRunner().run(input=pipeline, extra_volumes=mounted_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "The following results have been taken from the output of the pipeline, which is stored in the `.fondant` directory. This directory contains the output of each component, together with the cache of the previous run. Currently, the pipeline doesn't implement the `write_to_file` component, so the results will be taken individually from the output of each component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# get the most recent folder in the folder named: BASE_PATH + PIPELINE_NAME + PIPELINE_NAME-<timestamp>\n",
    "matching_folders = glob.glob(f\"{BASE_PATH}/{PIPELINE_NAME}/{PIPELINE_NAME}-*\")\n",
    "\n",
    "if matching_folders:\n",
    "    OUTPUT_FOLDER = max(matching_folders, key=os.path.getctime)\n",
    "else:\n",
    "    print(\"No matching folders found\")\n",
    "    exit()\n",
    "\n",
    "if os.path.exists(OUTPUT_FOLDER):\n",
    "\t# remove the manifest file from each folder in the output folder\n",
    "\tfor root, dirs, files in os.walk(OUTPUT_FOLDER):\n",
    "\t\tfor file in files:\n",
    "\t\t\tif file == \"manifest.json\":\n",
    "\t\t\t\tos.remove(os.path.join(root, file))\n",
    "\t\t\t\tREMOVED_MANIFEST = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "def merge_parquet_folders(folder_path):\n",
    "\tmerged_df = pd.DataFrame()\n",
    "\t\n",
    "\t# Loop through each folder in the output_folder\n",
    "\tfor folder in os.listdir(folder_path):\n",
    "\t\tparquet_partitions = os.path.join(folder_path, folder)\n",
    "\t\t\n",
    "\t\tfolder_df = pd.DataFrame()\n",
    "\t\t\n",
    "\t\tfor file in os.listdir(parquet_partitions):\n",
    "\t\t\tif file.endswith(\".parquet\"):\n",
    "\t\t\t\tfile_path = os.path.join(parquet_partitions, file)\n",
    "\t\t\t\tdf = pq.read_table(file_path).to_pandas()\t\t\n",
    "\t\t\t\tfolder_df = pd.concat([folder_df, df])\n",
    "\t\t\n",
    "\t\tif not merged_df.empty:\n",
    "\t\t\tmerged_df = pd.merge(merged_df, folder_df, how='outer', on='sequence')\n",
    "\t\telse:\n",
    "\t\t\tmerged_df = folder_df.copy()\n",
    "\t\n",
    "\treturn merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\denis\\AppData\\Local\\Temp\\ipykernel_2200\\1060381786.py:18: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  folder_df = pd.concat([folder_df, df])\n",
      "C:\\Users\\denis\\AppData\\Local\\Temp\\ipykernel_2200\\1060381786.py:18: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  folder_df = pd.concat([folder_df, df])\n",
      "C:\\Users\\denis\\AppData\\Local\\Temp\\ipykernel_2200\\1060381786.py:18: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  folder_df = pd.concat([folder_df, df])\n",
      "C:\\Users\\denis\\AppData\\Local\\Temp\\ipykernel_2200\\1060381786.py:18: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  folder_df = pd.concat([folder_df, df])\n",
      "C:\\Users\\denis\\AppData\\Local\\Temp\\ipykernel_2200\\1060381786.py:18: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  folder_df = pd.concat([folder_df, df])\n"
     ]
    }
   ],
   "source": [
    "if REMOVED_MANIFEST and os.path.exists(OUTPUT_FOLDER):\n",
    "\tmerged_df = merge_parquet_folders(OUTPUT_FOLDER)\n",
    "\tmerged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if REMOVED_MANIFEST and os.path.exists(OUTPUT_FOLDER):\n",
    "\tif not os.path.exists(os.path.join(os.path.abspath(\"data\"), \"export\")):\n",
    "\t\tos.makedirs(os.path.join(os.path.abspath(\"data\"), \"export\"))\n",
    "\n",
    "\toutput_path = os.path.join(os.path.abspath(\"data\"), \"export\")\n",
    "\n",
    "\tmerged_df.to_parquet(os.path.join(output_path, \"results.parquet\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "protein-feature-extraction-NoVdeDG9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
