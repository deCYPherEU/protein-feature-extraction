{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protein feature extraction pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will contain the pipeline for extracting features from protein sequences. It will be used as a way to show the output without needing to run the `pipeline.py` file locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import logging\n",
    "from fondant.pipeline import Pipeline\n",
    "from fondant.pipeline.runner import DockerRunner\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "from config import MOCK_DATA_PATH_FONDANT\n",
    "\n",
    "# check if the manifest file is removed.\n",
    "REMOVED_MANIFEST = False\n",
    "\n",
    "# check if the output folder exists\n",
    "OUTPUT_FOLDER = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Mock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python utils/generate_mock_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MNQRGMPIQSLVTNVKINRLEENDCIHTRHRVRPGRTDGKNLHAMM...</td>\n",
       "      <td>Seq1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MAGLKPEVPLHDGINKFGKSDFAGQEGPKIVTTTDKALLVANGALK...</td>\n",
       "      <td>Seq2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MVDLKKELKNFVDSDFPGSPKQEAQGIDVRILLSFNNAAFREALII...</td>\n",
       "      <td>Seq3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MELILAKARLEFECDWGLLMLEPCVPPTKIFADRNYAVGVMFESDK...</td>\n",
       "      <td>Seq4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MRVLCDGSTGYACAKNTRIRFREKVASVLAKIQGYEQTFPHHMPNM...</td>\n",
       "      <td>Seq5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sequence  name\n",
       "0  MNQRGMPIQSLVTNVKINRLEENDCIHTRHRVRPGRTDGKNLHAMM...  Seq1\n",
       "1  MAGLKPEVPLHDGINKFGKSDFAGQEGPKIVTTTDKALLVANGALK...  Seq2\n",
       "2  MVDLKKELKNFVDSDFPGSPKQEAQGIDVRILLSFNNAAFREALII...  Seq3\n",
       "3  MELILAKARLEFECDWGLLMLEPCVPPTKIFADRNYAVGVMFESDK...  Seq4\n",
       "4  MRVLCDGSTGYACAKNTRIRFREKVASVLAKIQGYEQTFPHHMPNM...  Seq5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show content of the mock data\n",
    "import pandas as pd\n",
    "mock_df = pd.read_parquet(\".\" + MOCK_DATA_PATH_FONDANT)  # dot added to make it relative to the current directory\n",
    "mock_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new pipeline\n",
    "\n",
    "BASE_PATH = \".fondant\"\n",
    "PIPELINE_NAME = \"feature_extraction_pipeline\"\n",
    "\n",
    "pipeline = Pipeline(\n",
    "\tname=PIPELINE_NAME,\n",
    "\tbase_path=BASE_PATH,\n",
    "\tdescription=\"A pipeline to extract features from protein sequences.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-13 15:05:20,177 | fondant.pipeline.pipeline | INFO] The consumes section of the component spec is not defined. Can not infer consumes of the OperationSpec. Please define a consumes section in the dataset interface. \n"
     ]
    }
   ],
   "source": [
    "# Read the dataset\n",
    "\n",
    "dataset = pipeline.read(\n",
    "\t\"load_from_parquet\",\n",
    "\targuments={\n",
    "\t\t\"dataset_uri\": MOCK_DATA_PATH_FONDANT,\n",
    "\t},\n",
    "\tproduces={\n",
    "\t\t\"sequence\": pa.string()\n",
    "\t}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components\n",
    "\n",
    "---\n",
    "\n",
    "### generate_protein_sequence_checksum_component\n",
    "\n",
    "This component generates a checksum for the protein sequence.\n",
    "\n",
    "---\n",
    "\n",
    "### biopython_component\n",
    "\n",
    "Extracts features from the protein sequence using Biopython.\n",
    "\n",
    "---\n",
    "\n",
    "### iFeatureOmega_component\n",
    "\n",
    "Extracts features from the protein sequence using the [iFeatureOmega-CLI GitHub repo](https://github.com/Superzchen/iFeatureOmega-CLI). Arguments are used to specify the type of features to extract.\n",
    "\n",
    "---\n",
    "\n",
    "### filter_pdb_component\n",
    "\n",
    "Filters PDB files that are already predicted to avoid redundant predictions. Arguments need to be specified before running the pipeline:\n",
    "```json\n",
    "\"storage_type\": \"local\",\n",
    "\"pdb_path\": \"/data/<your-pdb-folder-path>\",\n",
    "\"bucket_name\": \"your-bucket-name\",\n",
    "\"project_id\": \"your-project-id\",\n",
    "\"google_cloud_credentials_path\": \"/data/<your-credentials>.json\"\n",
    "```\n",
    "\n",
    "If only using local, keep bucket_name, project_id, and google_cloud_credentials_path as empty strings. Using remote requires a Google Cloud Storage bucket with credentials and a project ID.\n",
    "\n",
    "---\n",
    "\n",
    "### predict_protein_3D_structure_component\n",
    "\n",
    "Predicts the 3D structure of the protein using ESMFold. This component requires a `.env` file with the following variables:\n",
    "```env\n",
    "HF_API_KEY=\"\"\n",
    "HF_ENDPOINT_URL=\"\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### store_pdb_component\n",
    "\n",
    "Stores the PDB files in the provided storage_type. Arguments need to be specified before running the pipeline:\n",
    "```json\n",
    "\"storage_type\": \"local\",\n",
    "\"pdb_path\": \"/data/<your-pdb-folder-path>\",\n",
    "\"bucket_name\": \"your-bucket-name\",\n",
    "\"project_id\": \"your-project-id\",\n",
    "\"google_cloud_credentials_path\": \"/data/<your-credentials>.json\"\n",
    "```\n",
    "\n",
    "If only using local, keep bucket_name, project_id, and google_cloud_credentials_path as empty strings. Using remote requires a Google Cloud Storage bucket with credentials and a project ID.\n",
    "\n",
    "---\n",
    "\n",
    "### msa_component\n",
    "\n",
    "Generates the multiple sequence alignment for the protein sequence using [Clustal Omega](http://www.clustal.org/omega/). It's recommended to use a smaller number of sequences or none at all due to potential time consumption.\n",
    "\n",
    "---\n",
    "\n",
    "### unikp_component\n",
    "\n",
    "Uses the UniKP endpoint on HuggingFace to predict the kinetic parameters of a protein sequence and substrate (SMILES) combination. See README for the description of the contents of this file.\n",
    "\n",
    "```yaml\n",
    "\"protein_smiles_path\": \"/data/<path_protein_smiles>\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### peptide_component\n",
    "\n",
    "Calculates the features from the protein sequence using the `peptides` package.\n",
    "\n",
    "---\n",
    "\n",
    "### deepTMpred_component\n",
    "\n",
    "Predicts the transmembrane regions of the protein sequence using the [DeepTMpred GitHub repository](https://github.com/ISYSLAB-HUST/DeepTMpred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-13 15:05:20,198 | fondant.pipeline.pipeline | WARNING] Component `Biopython component` has an image tag set to latest. Caching for the component will be disabled to prevent unpredictable behavior due to images updates\n",
      "[2024-06-13 15:05:20,202 | fondant.pipeline.pipeline | WARNING] Component `Generate Protein Sequence Checksum Component` has an image tag set to latest. Caching for the component will be disabled to prevent unpredictable behavior due to images updates\n",
      "[2024-06-13 15:05:20,223 | fondant.pipeline.pipeline | WARNING] Component `iFeatureOmega component` has an image tag set to latest. Caching for the component will be disabled to prevent unpredictable behavior due to images updates\n",
      "[2024-06-13 15:05:20,229 | fondant.pipeline.pipeline | WARNING] Component `Filter PDB Component` has an image tag set to latest. Caching for the component will be disabled to prevent unpredictable behavior due to images updates\n",
      "[2024-06-13 15:05:20,238 | fondant.pipeline.pipeline | WARNING] Component `Predict Protein 3D Structure Component` has an image tag set to latest. Caching for the component will be disabled to prevent unpredictable behavior due to images updates\n",
      "[2024-06-13 15:05:20,246 | fondant.pipeline.pipeline | WARNING] Component `Store PDB Component` has an image tag set to latest. Caching for the component will be disabled to prevent unpredictable behavior due to images updates\n",
      "[2024-06-13 15:05:20,254 | fondant.pipeline.pipeline | WARNING] Component `MSA component` has an image tag set to latest. Caching for the component will be disabled to prevent unpredictable behavior due to images updates\n"
     ]
    }
   ],
   "source": [
    "_ = dataset.apply(\n",
    "\t\"./components/biopython_component\"\n",
    ").apply(\n",
    "\t\"./components/generate_protein_sequence_checksum_component\"\n",
    ").apply(\n",
    "\t\"./components/iFeatureOmega_component\",\n",
    "\t# currently forcing the number of rows to 5, but there needs to be a better way to do this, see readme for more info\n",
    "\tinput_partition_rows=5,\n",
    "\targuments={\n",
    "\t\t\"descriptors\": [\"AAC\", \"CTDC\", \"CTDT\"]\n",
    "\t}\n",
    ").apply(\n",
    "\t\"./components/filter_pdb_component\",\n",
    "\targuments={\n",
    "\t\t\"method\": \"local\",\n",
    "\t\t\"local_pdb_path\": \"/data/pdb_files\",\n",
    "\t\t\"bucket_name\": \"\",\n",
    "\t\t\"project_id\": \"\",\n",
    "\t\t\"google_cloud_credentials_path\": \"\"\n",
    "\t}\n",
    ").apply(\n",
    "\t\"./components/predict_protein_3D_structure_component\",\n",
    ").apply(\n",
    "\t\"./components/store_pdb_component\",\n",
    "\targuments={\n",
    "\t\t\"method\": \"local\",\n",
    "\t\t\"local_pdb_path\": \"/data/pdb_files/\",\n",
    "\t\t\"bucket_name\": \"elated-chassis-400207_dbtl_pipeline_outputs\",\n",
    "\t\t\"project_id\": \"elated-chassis-400207\",\n",
    "\t\t\"google_cloud_credentials_path\": \"/data/google_cloud_credentials.json\"\n",
    "\t}\n",
    ").apply(\n",
    "\t\"./components/msa_component\",\n",
    "    input_partition_rows='10000'\n",
    "# ).apply(\n",
    "# \t\"./components/pdb_features_component\"\n",
    "# ).apply(\n",
    "# \t\"./components/unikp_component\",\n",
    "# \targuments={\n",
    "# \t\t\"protein_smiles_path\": \"/data/protein_smiles.json\",\n",
    "# \t},\n",
    "# ).apply(\n",
    "# \t\"./components/peptide_features_component\"\n",
    "# ).apply(\n",
    "# \t\"./components/DeepTMpred_component\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pipeline.py` file needs to be run using the command line. The following command will run the pipeline:\n",
    "\n",
    "```bash\n",
    "fondant < full_path_to_pipeline.py >\\data:/data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-13 15:05:20,326 | root | INFO] Found reference to un-compiled pipeline... compiling\n",
      "[2024-06-13 15:05:20,326 | fondant.pipeline.compiler | INFO] Compiling feature_extraction_pipeline to .fondant/compose.yaml\n",
      "[2024-06-13 15:05:20,327 | fondant.pipeline.compiler | INFO] Base path found on local system, setting up .fondant as mount volume\n",
      "[2024-06-13 15:05:20,327 | fondant.pipeline.pipeline | INFO] Sorting pipeline component graph topologically.\n",
      "[2024-06-13 15:05:20,348 | fondant.pipeline.pipeline | INFO] All pipeline component specifications match.\n",
      "[2024-06-13 15:05:20,349 | fondant.pipeline.compiler | INFO] Compiling service for load_from_parquet\n",
      "[2024-06-13 15:05:20,349 | fondant.pipeline.compiler | INFO] Compiling service for biopython_component\n",
      "[2024-06-13 15:05:20,350 | fondant.pipeline.compiler | INFO] Found Dockerfile for biopython_component, adding build step.\n",
      "[2024-06-13 15:05:20,350 | fondant.pipeline.compiler | INFO] Compiling service for generate_protein_sequence_checksum_component\n",
      "[2024-06-13 15:05:20,351 | fondant.pipeline.compiler | INFO] Found Dockerfile for generate_protein_sequence_checksum_component, adding build step.\n",
      "[2024-06-13 15:05:20,351 | fondant.pipeline.compiler | INFO] Compiling service for ifeatureomega_component\n",
      "[2024-06-13 15:05:20,352 | fondant.pipeline.compiler | INFO] Found Dockerfile for ifeatureomega_component, adding build step.\n",
      "[2024-06-13 15:05:20,352 | fondant.pipeline.compiler | INFO] Compiling service for filter_pdb_component\n",
      "[2024-06-13 15:05:20,353 | fondant.pipeline.compiler | INFO] Found Dockerfile for filter_pdb_component, adding build step.\n",
      "[2024-06-13 15:05:20,353 | fondant.pipeline.compiler | INFO] Compiling service for predict_protein_3d_structure_component\n",
      "[2024-06-13 15:05:20,353 | fondant.pipeline.compiler | INFO] Found Dockerfile for predict_protein_3d_structure_component, adding build step.\n",
      "[2024-06-13 15:05:20,354 | fondant.pipeline.compiler | INFO] Compiling service for store_pdb_component\n",
      "[2024-06-13 15:05:20,354 | fondant.pipeline.compiler | INFO] Found Dockerfile for store_pdb_component, adding build step.\n",
      "[2024-06-13 15:05:20,354 | fondant.pipeline.compiler | INFO] Compiling service for msa_component\n",
      "[2024-06-13 15:05:20,355 | fondant.pipeline.compiler | INFO] Found Dockerfile for msa_component, adding build step.\n",
      "[2024-06-13 15:05:20,380 | fondant.pipeline.compiler | INFO] Successfully compiled to .fondant/compose.yaml\n",
      "time=\"2024-06-13T15:05:20+02:00\" level=warning msg=\"/home/pietercoussement/Software/Sandbox/protein-feature-extraction/.fondant/compose.yaml: `version` is obsolete\"\n",
      " load_from_parquet Pulling \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pipeline run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " load_from_parquet Pulled \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0 building with \"desktop-linux\" instance using docker driver\n",
      "\n",
      "#1 [biopython_component internal] load build definition from Dockerfile\n",
      "#1 transferring dockerfile: 480B done\n",
      "#1 DONE 0.0s\n",
      "\n",
      "#2 [biopython_component internal] load metadata for docker.io/fndnt/fondant:0.11.dev5-py3.10\n",
      "#2 DONE 0.4s\n",
      "\n",
      "#3 [biopython_component internal] load .dockerignore\n",
      "#3 transferring context: 2B done\n",
      "#3 DONE 0.0s\n",
      "\n",
      "#4 [biopython_component 1/6] FROM docker.io/fndnt/fondant:0.11.dev5-py3.10@sha256:94df1e8e58107a4b46d3110368bf881d4d3fc7732906acbb38b594fd00d20374\n",
      "#4 DONE 0.0s\n",
      "\n",
      "#5 [biopython_component internal] load build context\n",
      "#5 transferring context: 3.25kB done\n",
      "#5 DONE 0.0s\n",
      "\n",
      "#6 [biopython_component 4/6] RUN pip3 install --no-cache-dir -r requirements.txt\n",
      "#6 CACHED\n",
      "\n",
      "#7 [biopython_component 2/6] RUN apt-get update &&     apt-get upgrade -y &&     apt-get install git -y\n",
      "#7 CACHED\n",
      "\n",
      "#8 [biopython_component 3/6] COPY requirements.txt ./\n",
      "#8 CACHED\n",
      "\n",
      "#9 [biopython_component 5/6] WORKDIR /component/src\n",
      "#9 CACHED\n",
      "\n",
      "#10 [biopython_component 6/6] COPY src/ .\n",
      "#10 CACHED\n",
      "\n",
      "#11 [biopython_component] exporting to image\n",
      "#11 exporting layers done\n",
      "#11 writing image sha256:8784fb8818fa278cc79587b81ae25c9e9a84285f6a6d9791ade2ec7b7c1447a0 done\n",
      "#11 naming to docker.io/library/feature_extraction_pipeline-biopython_component done\n",
      "#11 DONE 0.0s\n",
      "\n",
      "#12 [generate_protein_sequence_checksum_component internal] load build definition from Dockerfile\n",
      "#12 transferring dockerfile: 480B done\n",
      "#12 DONE 0.0s\n",
      "\n",
      "#2 [generate_protein_sequence_checksum_component internal] load metadata for docker.io/fndnt/fondant:0.11.dev5-py3.10\n",
      "#2 DONE 0.6s\n",
      "\n",
      "#13 [generate_protein_sequence_checksum_component internal] load .dockerignore\n",
      "#13 transferring context: 2B done\n",
      "#13 DONE 0.0s\n",
      "\n",
      "#4 [generate_protein_sequence_checksum_component 1/6] FROM docker.io/fndnt/fondant:0.11.dev5-py3.10@sha256:94df1e8e58107a4b46d3110368bf881d4d3fc7732906acbb38b594fd00d20374\n",
      "#4 DONE 0.0s\n",
      "\n",
      "#14 [generate_protein_sequence_checksum_component internal] load build context\n",
      "#14 transferring context: 1.17kB done\n",
      "#14 DONE 0.0s\n",
      "\n",
      "#15 [generate_protein_sequence_checksum_component 3/6] COPY requirements.txt ./\n",
      "#15 CACHED\n",
      "\n",
      "#16 [generate_protein_sequence_checksum_component 4/6] RUN pip3 install --no-cache-dir -r requirements.txt\n",
      "#16 CACHED\n",
      "\n",
      "#17 [generate_protein_sequence_checksum_component 5/6] WORKDIR /component/src\n",
      "#17 CACHED\n",
      "\n",
      "#7 [generate_protein_sequence_checksum_component 2/6] RUN apt-get update &&     apt-get upgrade -y &&     apt-get install git -y\n",
      "#7 CACHED\n",
      "\n",
      "#18 [generate_protein_sequence_checksum_component 6/6] COPY src/ .\n",
      "#18 CACHED\n",
      "\n",
      "#19 [generate_protein_sequence_checksum_component] exporting to image\n",
      "#19 exporting layers done\n",
      "#19 writing image sha256:2cc5964bd18b7a36bb1e604b49827e7fd52eef14317d7e5f6faaa3438df320cd done\n",
      "#19 naming to docker.io/library/feature_extraction_pipeline-generate_protein_sequence_checksum_component done\n",
      "#19 DONE 0.0s\n",
      "\n",
      "#20 [ifeatureomega_component internal] load build definition from Dockerfile\n",
      "#20 transferring dockerfile: 668B done\n",
      "#20 DONE 0.0s\n",
      "\n",
      "#2 [ifeatureomega_component internal] load metadata for docker.io/fndnt/fondant:0.11.dev5-py3.10\n",
      "#2 DONE 0.8s\n",
      "\n",
      "#21 [ifeatureomega_component internal] load .dockerignore\n",
      "#21 transferring context: 2B done\n",
      "#21 DONE 0.0s\n",
      "\n",
      "#4 [ifeatureomega_component 1/8] FROM docker.io/fndnt/fondant:0.11.dev5-py3.10@sha256:94df1e8e58107a4b46d3110368bf881d4d3fc7732906acbb38b594fd00d20374\n",
      "#4 DONE 0.0s\n",
      "\n",
      "#22 [ifeatureomega_component internal] load build context\n",
      "#22 transferring context: 3.04kB done\n",
      "#22 DONE 0.0s\n",
      "\n",
      "#23 [ifeatureomega_component 3/8] COPY requirements.txt ./\n",
      "#23 CACHED\n",
      "\n",
      "#24 [ifeatureomega_component 4/8] RUN pip3 install --no-cache-dir -r requirements.txt\n",
      "#24 CACHED\n",
      "\n",
      "#25 [ifeatureomega_component 5/8] WORKDIR /component/src\n",
      "#25 CACHED\n",
      "\n",
      "#7 [ifeatureomega_component 2/8] RUN apt-get update &&     apt-get upgrade -y &&     apt-get install git -y\n",
      "#7 CACHED\n",
      "\n",
      "#26 [ifeatureomega_component 6/8] COPY src/ .\n",
      "#26 CACHED\n",
      "\n",
      "#27 [ifeatureomega_component 7/8] RUN git clone https://github.com/Superzchen/iFeatureOmega-CLI\n",
      "#27 CACHED\n",
      "\n",
      "#28 [ifeatureomega_component 8/8] RUN mv iFeatureOmega-CLI iFeatureOmega_CLI\n",
      "#28 CACHED\n",
      "\n",
      "#29 [ifeatureomega_component] exporting to image\n",
      "#29 exporting layers done\n",
      "#29 writing image sha256:b40c03f1141a19af2fd55fdc185832806d1e565d374d9eac426a0e2482685822 done\n",
      "#29 naming to docker.io/library/feature_extraction_pipeline-ifeatureomega_component done\n",
      "#29 DONE 0.0s\n",
      "\n",
      "#30 [filter_pdb_component internal] load build definition from Dockerfile\n",
      "#30 transferring dockerfile: 480B done\n",
      "#30 DONE 0.1s\n",
      "\n",
      "#2 [filter_pdb_component internal] load metadata for docker.io/fndnt/fondant:0.11.dev5-py3.10\n",
      "#2 DONE 1.0s\n",
      "\n",
      "#31 [filter_pdb_component internal] load .dockerignore\n",
      "#31 transferring context: 2B done\n",
      "#31 DONE 0.0s\n",
      "\n",
      "#4 [filter_pdb_component 1/6] FROM docker.io/fndnt/fondant:0.11.dev5-py3.10@sha256:94df1e8e58107a4b46d3110368bf881d4d3fc7732906acbb38b594fd00d20374\n",
      "#4 DONE 0.0s\n",
      "\n",
      "#32 [filter_pdb_component internal] load build context\n",
      "#32 transferring context: 3.86kB done\n",
      "#32 DONE 0.0s\n",
      "\n",
      "#33 [filter_pdb_component 3/6] COPY requirements.txt ./\n",
      "#33 CACHED\n",
      "\n",
      "#7 [filter_pdb_component 2/6] RUN apt-get update &&     apt-get upgrade -y &&     apt-get install git -y\n",
      "#7 CACHED\n",
      "\n",
      "#34 [filter_pdb_component 4/6] RUN pip3 install --no-cache-dir -r requirements.txt\n",
      "#34 CACHED\n",
      "\n",
      "#35 [filter_pdb_component 5/6] WORKDIR /component/src\n",
      "#35 CACHED\n",
      "\n",
      "#36 [filter_pdb_component 6/6] COPY src/ .\n",
      "#36 CACHED\n",
      "\n",
      "#37 [filter_pdb_component] exporting to image\n",
      "#37 exporting layers done\n",
      "#37 writing image sha256:cc166d3b65338fb2811a3da03aa166e7bad9b7344c2bf670035e6cbf374e7d8f done\n",
      "#37 naming to docker.io/library/feature_extraction_pipeline-filter_pdb_component done\n",
      "#37 DONE 0.0s\n",
      "\n",
      "#38 [predict_protein_3d_structure_component internal] load build definition from Dockerfile\n",
      "#38 transferring dockerfile: 539B done\n",
      "#38 DONE 0.0s\n",
      "\n",
      "#2 [predict_protein_3d_structure_component internal] load metadata for docker.io/fndnt/fondant:0.11.dev5-py3.10\n",
      "#2 DONE 1.2s\n",
      "\n",
      "#39 [predict_protein_3d_structure_component internal] load .dockerignore\n",
      "#39 transferring context: 2B done\n",
      "#39 DONE 0.0s\n",
      "\n",
      "#4 [predict_protein_3d_structure_component 1/7] FROM docker.io/fndnt/fondant:0.11.dev5-py3.10@sha256:94df1e8e58107a4b46d3110368bf881d4d3fc7732906acbb38b594fd00d20374\n",
      "#4 DONE 0.0s\n",
      "\n",
      "#40 [predict_protein_3d_structure_component internal] load build context\n",
      "#40 transferring context: 2.62kB done\n",
      "#40 DONE 0.0s\n",
      "\n",
      "#41 [predict_protein_3d_structure_component 6/7] COPY .env .\n",
      "#41 CACHED\n",
      "\n",
      "#42 [predict_protein_3d_structure_component 3/7] COPY requirements.txt ./\n",
      "#42 CACHED\n",
      "\n",
      "#7 [predict_protein_3d_structure_component 2/7] RUN apt-get update &&     apt-get upgrade -y &&     apt-get install git -y\n",
      "#7 CACHED\n",
      "\n",
      "#43 [predict_protein_3d_structure_component 4/7] RUN pip3 install --no-cache-dir -r requirements.txt\n",
      "#43 CACHED\n",
      "\n",
      "#44 [predict_protein_3d_structure_component 5/7] WORKDIR /component/src\n",
      "#44 CACHED\n",
      "\n",
      "#45 [predict_protein_3d_structure_component 7/7] COPY src/ .\n",
      "#45 CACHED\n",
      "\n",
      "#46 [predict_protein_3d_structure_component] exporting to image\n",
      "#46 exporting layers done\n",
      "#46 writing image sha256:a5abbe8ae3949117f0eab7c8c3bbaed6bc659004a312c664c083e4fd9a899618 done\n",
      "#46 naming to docker.io/library/feature_extraction_pipeline-predict_protein_3d_structure_component done\n",
      "#46 DONE 0.0s\n",
      "\n",
      "#47 [store_pdb_component internal] load build definition from Dockerfile\n",
      "#47 transferring dockerfile: 480B done\n",
      "#47 DONE 0.0s\n",
      "\n",
      "#2 [store_pdb_component internal] load metadata for docker.io/fndnt/fondant:0.11.dev5-py3.10\n",
      "#2 DONE 1.4s\n",
      "\n",
      "#48 [store_pdb_component internal] load .dockerignore\n",
      "#48 transferring context: 2B done\n",
      "#48 DONE 0.0s\n",
      "\n",
      "#4 [store_pdb_component 1/6] FROM docker.io/fndnt/fondant:0.11.dev5-py3.10@sha256:94df1e8e58107a4b46d3110368bf881d4d3fc7732906acbb38b594fd00d20374\n",
      "#4 DONE 0.0s\n",
      "\n",
      "#49 [store_pdb_component internal] load build context\n",
      "#49 transferring context: 3.16kB done\n",
      "#49 DONE 0.0s\n",
      "\n",
      "#50 [store_pdb_component 5/6] WORKDIR /component/src\n",
      "#50 CACHED\n",
      "\n",
      "#51 [store_pdb_component 3/6] COPY requirements.txt ./\n",
      "#51 CACHED\n",
      "\n",
      "#7 [store_pdb_component 2/6] RUN apt-get update &&     apt-get upgrade -y &&     apt-get install git -y\n",
      "#7 CACHED\n",
      "\n",
      "#52 [store_pdb_component 4/6] RUN pip3 install --no-cache-dir -r requirements.txt\n",
      "#52 CACHED\n",
      "\n",
      "#53 [store_pdb_component 6/6] COPY src/ .\n",
      "#53 CACHED\n",
      "\n",
      "#54 [store_pdb_component] exporting to image\n",
      "#54 exporting layers done\n",
      "#54 writing image sha256:2229bb95d650c3ebc614b65a6378c0f6bf7758e2f859b80aebb147435bc2b295 done\n",
      "#54 naming to docker.io/library/feature_extraction_pipeline-store_pdb_component done\n",
      "#54 DONE 0.0s\n",
      "\n",
      "#55 [msa_component internal] load build definition from Dockerfile\n",
      "#55 transferring dockerfile: 700B done\n",
      "#55 DONE 0.0s\n",
      "\n",
      "#2 [msa_component internal] load metadata for docker.io/fndnt/fondant:0.11.dev5-py3.10\n",
      "#2 DONE 1.6s\n",
      "\n",
      "#56 [msa_component internal] load .dockerignore\n",
      "#56 transferring context: 2B done\n",
      "#56 DONE 0.0s\n",
      "\n",
      "#4 [msa_component 1/7] FROM docker.io/fndnt/fondant:0.11.dev5-py3.10@sha256:94df1e8e58107a4b46d3110368bf881d4d3fc7732906acbb38b594fd00d20374\n",
      "#4 DONE 0.0s\n",
      "\n",
      "#57 [msa_component internal] load build context\n",
      "#57 transferring context: 4.33kB done\n",
      "#57 DONE 0.0s\n",
      "\n",
      "#58 [msa_component 5/7] WORKDIR /component/src\n",
      "#58 CACHED\n",
      "\n",
      "#59 [msa_component 6/7] RUN wget http://www.clustal.org/omega/clustalo-1.2.4-Ubuntu-x86_64 &&     mv clustalo-1.2.4-Ubuntu-x86_64 clustalo && \tchmod +x clustalo && \tmv clustalo /usr/local/bin/\n",
      "#59 CACHED\n",
      "\n",
      "#60 [msa_component 2/7] RUN apt-get update &&     apt-get upgrade -y &&     apt-get install git wget -y\n",
      "#60 CACHED\n",
      "\n",
      "#61 [msa_component 3/7] COPY requirements.txt ./\n",
      "#61 CACHED\n",
      "\n",
      "#62 [msa_component 4/7] RUN pip3 install --no-cache-dir -r requirements.txt\n",
      "#62 CACHED\n",
      "\n",
      "#63 [msa_component 7/7] COPY src/ .\n",
      "#63 CACHED\n",
      "\n",
      "#64 [msa_component] exporting to image\n",
      "#64 exporting layers done\n",
      "#64 writing image sha256:e69412ff7a22c61a43d29765bfedc34a9d19285db627566023feaefa01fb2e0c done\n",
      "#64 naming to docker.io/library/feature_extraction_pipeline-msa_component done\n",
      "#64 DONE 0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Container feature_extraction_pipeline-unikp_component-1  Stopping\n",
      " Container feature_extraction_pipeline-deeptm_prediction_component-1  Stopping\n",
      " Container feature_extraction_pipeline-peptide_features_component-1  Stopping\n",
      " Container feature_extraction_pipeline-peptide_features_component-1  Stopped\n",
      " Container feature_extraction_pipeline-peptide_features_component-1  Removing\n",
      " Container feature_extraction_pipeline-deeptm_prediction_component-1  Stopped\n",
      " Container feature_extraction_pipeline-deeptm_prediction_component-1  Removing\n",
      " Container feature_extraction_pipeline-unikp_component-1  Stopped\n",
      " Container feature_extraction_pipeline-unikp_component-1  Removing\n",
      " Container feature_extraction_pipeline-peptide_features_component-1  Removed\n",
      " Container feature_extraction_pipeline-deeptm_prediction_component-1  Removed\n",
      " Container feature_extraction_pipeline-unikp_component-1  Removed\n",
      " Container feature_extraction_pipeline-load_from_parquet-1  Recreate\n",
      " Container feature_extraction_pipeline-load_from_parquet-1  Recreated\n",
      " Container feature_extraction_pipeline-biopython_component-1  Recreate\n",
      " Container feature_extraction_pipeline-biopython_component-1  Recreated\n",
      " Container feature_extraction_pipeline-generate_protein_sequence_checksum_component-1  Recreate\n",
      " Container feature_extraction_pipeline-generate_protein_sequence_checksum_component-1  Recreated\n",
      " Container feature_extraction_pipeline-ifeatureomega_component-1  Recreate\n",
      " Container feature_extraction_pipeline-ifeatureomega_component-1  Recreated\n",
      " Container feature_extraction_pipeline-filter_pdb_component-1  Recreate\n",
      " Container feature_extraction_pipeline-filter_pdb_component-1  Recreated\n",
      " Container feature_extraction_pipeline-predict_protein_3d_structure_component-1  Recreate\n",
      " Container feature_extraction_pipeline-predict_protein_3d_structure_component-1  Recreated\n",
      " Container feature_extraction_pipeline-store_pdb_component-1  Recreate\n",
      " Container feature_extraction_pipeline-store_pdb_component-1  Recreated\n",
      " Container feature_extraction_pipeline-msa_component-1  Recreate\n",
      " Container feature_extraction_pipeline-msa_component-1  Recreated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attaching to biopython_component-1, filter_pdb_component-1, generate_protein_sequence_checksum_component-1, ifeatureomega_component-1, load_from_parquet-1, msa_component-1, predict_protein_3d_structure_component-1, store_pdb_component-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load_from_parquet-1                             | [2024-06-13 13:05:27,606 | fondant.cli | INFO] Component `LoadFromParquet` found in module main\n",
      "load_from_parquet-1                             | [2024-06-13 13:05:27,610 | fondant.component.executor | INFO] Skipping component execution\n",
      "load_from_parquet-1                             | [2024-06-13 13:05:27,611 | fondant.component.executor | INFO] Matching execution detected for component. The last execution of the component originated from `feature_extraction_pipeline-20240613115444`.\n",
      "load_from_parquet-1                             | [2024-06-13 13:05:27,612 | fondant.component.executor | INFO] Saving output manifest to /.fondant/feature_extraction_pipeline/feature_extraction_pipeline-20240613150520/load_from_parquet/manifest.json\n",
      "load_from_parquet-1                             | [2024-06-13 13:05:27,612 | fondant.component.executor | INFO] Writing cache key with manifest reference to /.fondant/feature_extraction_pipeline/cache/d41a53a1be34f7fd0a29002364c9f666.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[Kload_from_parquet-1 exited with code 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "biopython_component-1                           | [2024-06-13 13:05:29,593 | fondant.cli | INFO] Component `BiopythonComponent` found in module main\n",
      "biopython_component-1                           | [2024-06-13 13:05:29,596 | fondant.component.executor | INFO] Caching disabled for the component\n",
      "biopython_component-1                           | [2024-06-13 13:05:29,596 | root | INFO] Executing component\n",
      "biopython_component-1                           | [2024-06-13 13:05:29,890 | distributed.http.proxy | INFO] To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy\n",
      "biopython_component-1                           | [2024-06-13 13:05:29,912 | distributed.scheduler | INFO] State start\n",
      "biopython_component-1                           | [2024-06-13 13:05:29,916 | distributed.scheduler | INFO]   Scheduler at:     tcp://127.0.0.1:43755\n",
      "biopython_component-1                           | [2024-06-13 13:05:29,916 | distributed.scheduler | INFO]   dashboard at:  http://127.0.0.1:8787/status\n",
      "biopython_component-1                           | [2024-06-13 13:05:29,917 | distributed.scheduler | INFO] Registering Worker plugin shuffle\n",
      "biopython_component-1                           | [2024-06-13 13:05:29,938 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:36163'\n",
      "biopython_component-1                           | [2024-06-13 13:05:29,943 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:43405'\n",
      "biopython_component-1                           | [2024-06-13 13:05:29,945 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:39499'\n",
      "biopython_component-1                           | [2024-06-13 13:05:29,947 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:39873'\n",
      "biopython_component-1                           | [2024-06-13 13:05:29,960 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:40353'\n",
      "biopython_component-1                           | [2024-06-13 13:05:29,962 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:33627'\n",
      "biopython_component-1                           | [2024-06-13 13:05:29,968 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:41223'\n",
      "biopython_component-1                           | [2024-06-13 13:05:29,971 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:38603'\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,298 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:46739\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,298 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:46739\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,299 | distributed.worker | INFO]           Worker name:                          4\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,299 | distributed.worker | INFO]          dashboard at:            127.0.0.1:38293\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,299 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:43755\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,299 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,299 | distributed.worker | INFO]               Threads:                          1\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,299 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,299 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-ann486uc\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,299 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,319 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:46739', name: 4, status: init, memory: 0, processing: 0>\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,324 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:46739\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,325 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:54828\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,326 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,328 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:43755\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,331 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,337 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:43755\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,354 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:37097\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,355 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:37097\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,355 | distributed.worker | INFO]           Worker name:                          1\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,355 | distributed.worker | INFO]          dashboard at:            127.0.0.1:36609\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,355 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:43755\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,355 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,355 | distributed.worker | INFO]               Threads:                          1\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,355 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,355 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-j6tr7syn\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,355 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,364 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:37097', name: 1, status: init, memory: 0, processing: 0>\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,364 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:37097\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,364 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:54844\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,366 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,366 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:43755\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,367 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,370 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:43755\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,371 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:44777\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,371 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:44777\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,371 | distributed.worker | INFO]           Worker name:                          5\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,371 | distributed.worker | INFO]          dashboard at:            127.0.0.1:42031\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,371 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:43755\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,371 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,371 | distributed.worker | INFO]               Threads:                          1\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,371 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,371 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-2y8khowl\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,371 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,382 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:44777', name: 5, status: init, memory: 0, processing: 0>\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,386 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:44777\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,387 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:54858\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,389 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,390 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:43755\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,390 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,391 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:43755\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,457 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:38969\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,457 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:38969\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,458 | distributed.worker | INFO]           Worker name:                          7\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,458 | distributed.worker | INFO]          dashboard at:            127.0.0.1:43031\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,458 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:43755\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,458 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,458 | distributed.worker | INFO]               Threads:                          1\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,458 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,458 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-sjnpg3ab\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,458 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,467 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:38969', name: 7, status: init, memory: 0, processing: 0>\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,468 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:38969\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,468 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:54870\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,469 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,470 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:43755\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,470 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,471 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:43755\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,537 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:41367\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,537 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:41367\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,537 | distributed.worker | INFO]           Worker name:                          2\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,537 | distributed.worker | INFO]          dashboard at:            127.0.0.1:45737\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,537 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:43755\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,537 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,537 | distributed.worker | INFO]               Threads:                          1\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,537 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,537 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-pwee9jdb\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,537 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,542 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:41367', name: 2, status: init, memory: 0, processing: 0>\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,543 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:41367\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,543 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:54872\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,544 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,545 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:43755\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,545 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,546 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:43755\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,547 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:39697\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,547 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:39697\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,547 | distributed.worker | INFO]           Worker name:                          6\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,547 | distributed.worker | INFO]          dashboard at:            127.0.0.1:43223\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,548 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:43755\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,548 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,548 | distributed.worker | INFO]               Threads:                          1\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,548 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,548 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-gutxsd8g\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,548 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,551 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:34607\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,551 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:34607\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,551 | distributed.worker | INFO]           Worker name:                          0\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,551 | distributed.worker | INFO]          dashboard at:            127.0.0.1:44599\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,551 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:43755\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,551 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,551 | distributed.worker | INFO]               Threads:                          1\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,552 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,552 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-7t0b_g5n\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,552 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,554 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:39697', name: 6, status: init, memory: 0, processing: 0>\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,554 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:39697\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,554 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:54874\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,555 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,556 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:43755\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,556 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,556 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:43755\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,557 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:34607', name: 0, status: init, memory: 0, processing: 0>\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,558 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:34607\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,558 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:54880\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,558 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,559 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:43755\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,559 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,559 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:43755\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,581 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:45489\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,581 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:45489\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,581 | distributed.worker | INFO]           Worker name:                          3\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,581 | distributed.worker | INFO]          dashboard at:            127.0.0.1:42569\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,581 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:43755\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,582 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,582 | distributed.worker | INFO]               Threads:                          1\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,582 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,582 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-7_gezyd4\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,582 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,587 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:45489', name: 3, status: init, memory: 0, processing: 0>\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,588 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:45489\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,588 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:54882\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,588 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,589 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:43755\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,589 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,590 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:43755\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,641 | distributed.scheduler | INFO] Receive client connection: Client-9d309dfa-2985-11ef-8001-0242ac120002\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,641 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:54898\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,673 | fondant.component.data_io | INFO] The number of partitions of the input dataframe is 1. The available number of workers is 8.\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,674 | fondant.component.data_io | INFO] Repartitioning the data to 8 partitions before processing to maximize worker usage\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,674 | root | INFO] Columns of dataframe: ['sequence']\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,714 | root | INFO] Creating write task for: /.fondant/feature_extraction_pipeline/feature_extraction_pipeline-20240613150520/biopython_component\n",
      "biopython_component-1                           | [2024-06-13 13:05:31,714 | root | INFO] Writing data...\n",
      "biopython_component-1                           | [2024-06-13 13:05:32,095 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:36163'. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-06-13 13:05:32,095 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-06-13 13:05:32,095 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:43405'. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-06-13 13:05:32,095 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-06-13 13:05:32,095 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:39499'. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-06-13 13:05:32,096 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-06-13 13:05:32,096 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:34607. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-06-13 13:05:32,096 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:39873'. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-06-13 13:05:32,096 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-06-13 13:05:32,096 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:40353'. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-06-13 13:05:32,097 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-06-13 13:05:32,097 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:33627'. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-06-13 13:05:32,097 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:41367. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-06-13 13:05:32,097 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-06-13 13:05:32,097 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:41223'. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-06-13 13:05:32,098 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:37097. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-06-13 13:05:32,098 | distributed.core | INFO] Connection to tcp://127.0.0.1:43755 has been closed.\n",
      "biopython_component-1                           | [2024-06-13 13:05:32,098 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:46739. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-06-13 13:05:32,098 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:45489. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-06-13 13:05:32,098 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-06-13 13:05:32,098 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:38603'. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-06-13 13:05:32,099 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:44777. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-06-13 13:05:32,099 | distributed.core | INFO] Connection to tcp://127.0.0.1:43755 has been closed.\n",
      "biopython_component-1                           | [2024-06-13 13:05:32,098 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-06-13 13:05:32,099 | distributed.core | INFO] Connection to tcp://127.0.0.1:43755 has been closed.\n",
      "biopython_component-1                           | [2024-06-13 13:05:32,099 | distributed.core | INFO] Connection to tcp://127.0.0.1:43755 has been closed.\n",
      "biopython_component-1                           | [2024-06-13 13:05:32,099 | distributed.core | INFO] Connection to tcp://127.0.0.1:43755 has been closed.\n",
      "biopython_component-1                           | [2024-06-13 13:05:32,100 | distributed.core | INFO] Connection to tcp://127.0.0.1:43755 has been closed.\n",
      "biopython_component-1                           | [2024-06-13 13:05:32,102 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:38969. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-06-13 13:05:32,102 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:54880; closing.\n",
      "biopython_component-1                           | [2024-06-13 13:05:32,102 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:54872; closing.\n",
      "biopython_component-1                           | [2024-06-13 13:05:32,102 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:54882; closing.\n",
      "biopython_component-1                           | [2024-06-13 13:05:32,103 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:54828; closing.\n",
      "biopython_component-1                           | [2024-06-13 13:05:32,103 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:54844; closing.\n",
      "biopython_component-1                           | [2024-06-13 13:05:32,103 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:34607', name: 0, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718283932.103489')\n",
      "biopython_component-1                           | [2024-06-13 13:05:32,104 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:41367', name: 2, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718283932.1041057')\n",
      "biopython_component-1                           | [2024-06-13 13:05:32,104 | distributed.core | INFO] Connection to tcp://127.0.0.1:43755 has been closed.\n",
      "biopython_component-1                           | [2024-06-13 13:05:32,104 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:45489', name: 3, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718283932.104529')\n",
      "biopython_component-1                           | [2024-06-13 13:05:32,104 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:46739', name: 4, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718283932.1048489')\n",
      "biopython_component-1                           | [2024-06-13 13:05:32,105 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:39697. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-06-13 13:05:32,105 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:37097', name: 1, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718283932.1052954')\n",
      "biopython_component-1                           | [2024-06-13 13:05:32,105 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:54858; closing.\n",
      "biopython_component-1                           | [2024-06-13 13:05:32,106 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:44777', name: 5, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718283932.1067133')\n",
      "biopython_component-1                           | [2024-06-13 13:05:32,107 | distributed.core | INFO] Connection to tcp://127.0.0.1:43755 has been closed.\n",
      "biopython_component-1                           | [2024-06-13 13:05:32,109 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:54870; closing.\n",
      "biopython_component-1                           | [2024-06-13 13:05:32,111 | distributed.batched | INFO] Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:43755 remote=tcp://127.0.0.1:54858>\n",
      "biopython_component-1                           | Traceback (most recent call last):\n",
      "biopython_component-1                           |   File \"/usr/local/lib/python3.11/site-packages/distributed/batched.py\", line 115, in _background_send\n",
      "biopython_component-1                           |     nbytes = yield coro\n",
      "biopython_component-1                           |              ^^^^^^^^^^\n",
      "biopython_component-1                           |   File \"/usr/local/lib/python3.11/site-packages/tornado/gen.py\", line 767, in run\n",
      "biopython_component-1                           |     value = future.result()\n",
      "biopython_component-1                           |             ^^^^^^^^^^^^^^^\n",
      "biopython_component-1                           |   File \"/usr/local/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 262, in write\n",
      "biopython_component-1                           |     raise CommClosedError()\n",
      "biopython_component-1                           | distributed.comm.core.CommClosedError\n",
      "biopython_component-1                           | [2024-06-13 13:05:32,121 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:38969', name: 7, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718283932.1207047')\n",
      "biopython_component-1                           | [2024-06-13 13:05:32,123 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:54874; closing.\n",
      "biopython_component-1                           | [2024-06-13 13:05:32,126 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:39697', name: 6, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718283932.1257792')\n",
      "biopython_component-1                           | [2024-06-13 13:05:32,126 | distributed.scheduler | INFO] Lost all workers\n",
      "biopython_component-1                           | [2024-06-13 13:05:32,535 | distributed.scheduler | INFO] Scheduler closing due to unknown reason...\n",
      "biopython_component-1                           | [2024-06-13 13:05:32,535 | distributed.scheduler | INFO] Scheduler closing all comms\n",
      "biopython_component-1                           | [2024-06-13 13:05:32,539 | fondant.component.executor | INFO] Saving output manifest to /.fondant/feature_extraction_pipeline/feature_extraction_pipeline-20240613150520/biopython_component/manifest.json\n",
      "biopython_component-1                           | [2024-06-13 13:05:32,539 | fondant.component.executor | INFO] Writing cache key with manifest reference to /.fondant/feature_extraction_pipeline/cache/6299637a94f3236310f16b42153e9731.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[Kbiopython_component-1 exited with code 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:34,842 | fondant.cli | INFO] Component `GenerateProteinSequenceChecksumComponent` found in module main\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:34,847 | fondant.component.executor | INFO] Caching disabled for the component\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:34,847 | root | INFO] Executing component\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:35,217 | distributed.http.proxy | INFO] To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:35,241 | distributed.scheduler | INFO] State start\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:35,246 | distributed.scheduler | INFO]   Scheduler at:     tcp://127.0.0.1:43469\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:35,246 | distributed.scheduler | INFO]   dashboard at:  http://127.0.0.1:8787/status\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:35,246 | distributed.scheduler | INFO] Registering Worker plugin shuffle\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:35,270 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:33791'\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:35,276 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:38945'\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:35,278 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:37379'\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:35,294 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:36829'\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:35,297 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:35075'\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:35,299 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:46707'\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:35,301 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:37751'\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:35,308 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:34909'\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:36,958 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:41451\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:36,959 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:41451\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:36,963 | distributed.worker | INFO]           Worker name:                          4\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:36,964 | distributed.worker | INFO]          dashboard at:            127.0.0.1:35609\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:36,964 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:43469\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:36,964 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:36,964 | distributed.worker | INFO]               Threads:                          1\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:36,964 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:36,965 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-9fzz65o9\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:36,965 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:36,973 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:41451', name: 4, status: init, memory: 0, processing: 0>\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:36,978 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:41451\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:36,982 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:36,983 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:43469\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:36,983 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:36,982 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:46130\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:36,987 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:43469\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,004 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:43397\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,004 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:43397\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,004 | distributed.worker | INFO]           Worker name:                          3\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,004 | distributed.worker | INFO]          dashboard at:            127.0.0.1:43671\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,004 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:43469\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,004 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,004 | distributed.worker | INFO]               Threads:                          1\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,004 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,004 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-yhx44bte\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,005 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,012 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:43397', name: 3, status: init, memory: 0, processing: 0>\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,014 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,014 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:43397\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,014 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:42752\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,015 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:43469\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,015 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,016 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:43469\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,068 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:40229\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,068 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:40229\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,068 | distributed.worker | INFO]           Worker name:                          0\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,068 | distributed.worker | INFO]          dashboard at:            127.0.0.1:33073\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,068 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:43469\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,068 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,068 | distributed.worker | INFO]               Threads:                          1\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,069 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,071 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-tqn3t329\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,071 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,077 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:40229', name: 0, status: init, memory: 0, processing: 0>\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,077 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:40229\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,077 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:42760\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,079 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,079 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:43469\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,080 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,081 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:43469\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,171 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:46457\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,171 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:46457\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,171 | distributed.worker | INFO]           Worker name:                          7\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,171 | distributed.worker | INFO]          dashboard at:            127.0.0.1:36851\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,171 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:43469\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,171 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,171 | distributed.worker | INFO]               Threads:                          1\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,171 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,172 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-idzliizm\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,172 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,178 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:46457', name: 7, status: init, memory: 0, processing: 0>\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,179 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:46457\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,179 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:42776\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,180 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,181 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:43469\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,181 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,182 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:43469\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,205 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:46683\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,205 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:46683\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,206 | distributed.worker | INFO]           Worker name:                          5\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,206 | distributed.worker | INFO]          dashboard at:            127.0.0.1:33247\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,206 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:43469\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,206 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,206 | distributed.worker | INFO]               Threads:                          1\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,206 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,206 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-s8tvlcs1\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,206 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,211 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:46683', name: 5, status: init, memory: 0, processing: 0>\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,211 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:46683\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,212 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:42790\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,212 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,213 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:43469\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,213 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,214 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:43469\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,235 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:46181\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,235 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:46181\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,235 | distributed.worker | INFO]           Worker name:                          1\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,235 | distributed.worker | INFO]          dashboard at:            127.0.0.1:38485\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,235 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:43469\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,235 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,235 | distributed.worker | INFO]               Threads:                          1\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,235 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,235 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-2w9u1ude\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,235 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,240 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:34751\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,240 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:34751\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,240 | distributed.worker | INFO]           Worker name:                          2\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,240 | distributed.worker | INFO]          dashboard at:            127.0.0.1:41401\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,240 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:43469\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,240 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,240 | distributed.worker | INFO]               Threads:                          1\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,240 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,241 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-lg3v6x_x\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,241 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,241 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:46181', name: 1, status: init, memory: 0, processing: 0>\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,242 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:46181\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,242 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:42802\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,243 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,244 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:43469\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,244 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,245 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:43469\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,247 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:34751', name: 2, status: init, memory: 0, processing: 0>\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,248 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:34751\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,248 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:42806\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,249 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,249 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:43469\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,249 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,250 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:43469\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,262 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:43237\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,262 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:43237\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,262 | distributed.worker | INFO]           Worker name:                          6\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,262 | distributed.worker | INFO]          dashboard at:            127.0.0.1:33119\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,262 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:43469\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,262 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,262 | distributed.worker | INFO]               Threads:                          1\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,262 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,263 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-utit7tzj\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,263 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,267 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:43237', name: 6, status: init, memory: 0, processing: 0>\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,268 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:43237\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,268 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:42820\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,268 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,269 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:43469\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,269 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,269 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:43469\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,299 | distributed.scheduler | INFO] Receive client connection: Client-a09014a5-2985-11ef-8001-0242ac120002\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,299 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:42830\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,340 | fondant.component.data_io | INFO] The number of partitions of the input dataframe is 5. The available number of workers is 8.\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,340 | fondant.component.data_io | INFO] Repartitioning the data to 8 partitions before processing to maximize worker usage\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,340 | root | INFO] Columns of dataframe: ['sequence']\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,357 | root | INFO] Creating write task for: /.fondant/feature_extraction_pipeline/feature_extraction_pipeline-20240613150520/generate_protein_sequence_checksum_component\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,357 | root | INFO] Writing data...\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,675 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:33791'. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,675 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,675 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:38945'. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,676 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,676 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:37379'. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,676 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:40229. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,676 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,676 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:36829'. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,676 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,676 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:35075'. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,677 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:46181. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,677 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,677 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:46707'. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,677 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:43397. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,677 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:34751. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,677 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,678 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:37751'. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,678 | distributed.core | INFO] Connection to tcp://127.0.0.1:43469 has been closed.\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,678 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,678 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:41451. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,678 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:34909'. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,678 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,678 | distributed.core | INFO] Connection to tcp://127.0.0.1:43469 has been closed.\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,679 | distributed.core | INFO] Connection to tcp://127.0.0.1:43469 has been closed.\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,679 | distributed.core | INFO] Connection to tcp://127.0.0.1:43469 has been closed.\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,679 | distributed.core | INFO] Connection to tcp://127.0.0.1:43469 has been closed.\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,680 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:46683. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,680 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:46457. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,680 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:43237. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,681 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:42760; closing.\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,682 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:42802; closing.\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,682 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:42752; closing.\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,682 | distributed.core | INFO] Connection to tcp://127.0.0.1:43469 has been closed.\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,682 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:42806; closing.\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,683 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:46130; closing.\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,683 | distributed.core | INFO] Connection to tcp://127.0.0.1:43469 has been closed.\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,684 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:40229', name: 0, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718283937.6842837')\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,688 | distributed.core | INFO] Connection to tcp://127.0.0.1:43469 has been closed.\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,691 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:46181', name: 1, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718283937.691155')\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,693 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:43397', name: 3, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718283937.6930306')\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,697 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:34751', name: 2, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718283937.6971138')\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,698 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:41451', name: 4, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718283937.698235')\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,702 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:42820; closing.\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,702 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:42790; closing.\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,706 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:43237', name: 6, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718283937.7057886')\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,708 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:46683', name: 5, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718283937.7077289')\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,710 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:42776; closing.\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,717 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:46457', name: 7, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718283937.7168643')\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:37,718 | distributed.scheduler | INFO] Lost all workers\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:38,096 | distributed.scheduler | INFO] Scheduler closing due to unknown reason...\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:38,096 | distributed.scheduler | INFO] Scheduler closing all comms\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:38,100 | fondant.component.executor | INFO] Saving output manifest to /.fondant/feature_extraction_pipeline/feature_extraction_pipeline-20240613150520/generate_protein_sequence_checksum_component/manifest.json\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-13 13:05:38,100 | fondant.component.executor | INFO] Writing cache key with manifest reference to /.fondant/feature_extraction_pipeline/cache/5a301223e6ef93aa419c2bb7096d4d56.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[Kgenerate_protein_sequence_checksum_component-1 exited with code 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ifeatureomega_component-1                       | [2024-06-13 13:05:41,651 | matplotlib.font_manager | INFO] generated new fontManager\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:41,913 | fondant.cli | INFO] Component `IFeatureOmegaComponent` found in module main\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:41,929 | fondant.component.executor | INFO] Caching disabled for the component\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:41,929 | root | INFO] Executing component\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:42,379 | distributed.http.proxy | INFO] To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:42,403 | distributed.scheduler | INFO] State start\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:42,408 | distributed.scheduler | INFO]   Scheduler at:     tcp://127.0.0.1:34763\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:42,408 | distributed.scheduler | INFO]   dashboard at:  http://127.0.0.1:8787/status\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:42,408 | distributed.scheduler | INFO] Registering Worker plugin shuffle\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:42,434 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:40193'\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:42,437 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:44347'\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:42,441 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:46657'\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:42,443 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:40439'\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:42,452 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:37729'\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:42,456 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:43795'\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:42,461 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:38573'\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:42,466 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:35961'\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,190 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:41397\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,190 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:41397\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,191 | distributed.worker | INFO]           Worker name:                          0\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,191 | distributed.worker | INFO]          dashboard at:            127.0.0.1:34207\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,191 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:34763\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,191 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,191 | distributed.worker | INFO]               Threads:                          1\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,191 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,191 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-7e17xqhv\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,191 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,207 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:41397', name: 0, status: init, memory: 0, processing: 0>\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,214 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:41397\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,215 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:52834\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,216 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,217 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:34763\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,218 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,220 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:34763\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,241 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:44693\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,242 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:44693\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,242 | distributed.worker | INFO]           Worker name:                          7\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,243 | distributed.worker | INFO]          dashboard at:            127.0.0.1:43569\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,243 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:34763\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,243 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,243 | distributed.worker | INFO]               Threads:                          1\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,244 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,244 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-7v8s4cmv\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,244 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,250 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:44693', name: 7, status: init, memory: 0, processing: 0>\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,252 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:44693\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,252 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:52844\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,252 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,253 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:34763\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,253 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,255 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:34763\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,285 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:37587\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,285 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:37587\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,286 | distributed.worker | INFO]           Worker name:                          3\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,287 | distributed.worker | INFO]          dashboard at:            127.0.0.1:37097\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,287 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:34763\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,287 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,287 | distributed.worker | INFO]               Threads:                          1\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,287 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,287 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-_j1_fdm8\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,287 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,295 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:37587', name: 3, status: init, memory: 0, processing: 0>\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,296 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:37587\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,296 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:52848\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,297 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,299 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:34763\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,299 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,300 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:34763\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,328 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:43863\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,328 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:43863\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,328 | distributed.worker | INFO]           Worker name:                          4\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,328 | distributed.worker | INFO]          dashboard at:            127.0.0.1:37483\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,328 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:34763\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,328 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,328 | distributed.worker | INFO]               Threads:                          1\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,328 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,329 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-8dp0z8da\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,329 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,337 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:43863', name: 4, status: init, memory: 0, processing: 0>\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,337 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:43863\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,337 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:52864\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,339 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,339 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:34763\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,340 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,340 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:34763\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,356 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:36103\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,356 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:36103\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,356 | distributed.worker | INFO]           Worker name:                          1\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,356 | distributed.worker | INFO]          dashboard at:            127.0.0.1:42371\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,356 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:34763\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,356 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,356 | distributed.worker | INFO]               Threads:                          1\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,356 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,357 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-6paj09vt\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,357 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,362 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:36103', name: 1, status: init, memory: 0, processing: 0>\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,363 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:36103\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,363 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:52876\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,364 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,364 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:34763\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,365 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,365 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:34763\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,387 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:44715\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,387 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:44715\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,387 | distributed.worker | INFO]           Worker name:                          6\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,387 | distributed.worker | INFO]          dashboard at:            127.0.0.1:35279\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,387 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:34763\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,387 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,387 | distributed.worker | INFO]               Threads:                          1\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,387 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,387 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-hqg0epu6\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,387 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,393 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:44715', name: 6, status: init, memory: 0, processing: 0>\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,393 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:44715\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,393 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:52892\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,394 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,395 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:34763\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,395 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,396 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:34763\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,456 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:41825\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,456 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:41825\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,457 | distributed.worker | INFO]           Worker name:                          5\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,457 | distributed.worker | INFO]          dashboard at:            127.0.0.1:33021\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,457 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:34763\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,457 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,457 | distributed.worker | INFO]               Threads:                          1\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,457 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,457 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-yfjohk25\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,457 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,462 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:41825', name: 5, status: init, memory: 0, processing: 0>\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,462 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:41825\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,463 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:52906\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,464 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,464 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:34763\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,464 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,465 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:34763\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,468 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:42391\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,469 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:42391\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,469 | distributed.worker | INFO]           Worker name:                          2\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,469 | distributed.worker | INFO]          dashboard at:            127.0.0.1:39347\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,469 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:34763\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,469 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,469 | distributed.worker | INFO]               Threads:                          1\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,469 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,469 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-vmo_lgf4\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,469 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,474 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:42391', name: 2, status: init, memory: 0, processing: 0>\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,475 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:42391\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,475 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:52918\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,475 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,476 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:34763\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,476 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,476 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:34763\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,505 | distributed.scheduler | INFO] Receive client connection: Client-a4db99b7-2985-11ef-8001-0242ac120002\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,505 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:52928\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,610 | fondant.component.data_io | INFO] Total number of rows is 5.\n",
      "ifeatureomega_component-1                       | Repartitioning the data from <dask.utils.IndexCallable object at 0x7f162b8ece50> partitions to have 2 such that the number of partitions per row is approximately5\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,610 | fondant.component.data_io | WARNING] Setting the `input partition rows` has caused the system to not utilize all available workers 2 out of 8 are used.\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,610 | root | INFO] Columns of dataframe: ['sequence', 'sequence_checksum']\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,835 | root | INFO] Creating write task for: /.fondant/feature_extraction_pipeline/feature_extraction_pipeline-20240613150520/ifeatureomega_component\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:44,836 | root | INFO] Writing data...\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:46,526 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:40193'. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:46,526 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:46,526 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:44347'. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:46,527 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:46,527 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:46657'. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:46,527 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:41397. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:46,527 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:46,527 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:40439'. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:46,527 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:36103. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:46,527 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:46,528 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:37729'. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:46,528 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:42391. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:46,528 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:46,528 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:43795'. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:46,528 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:46,529 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:38573'. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:46,529 | distributed.core | INFO] Connection to tcp://127.0.0.1:34763 has been closed.\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:46,529 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:37587. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:46,529 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:46,529 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:35961'. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:46,529 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:43863. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:46,530 | distributed.core | INFO] Connection to tcp://127.0.0.1:34763 has been closed.\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:46,530 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:46,530 | distributed.core | INFO] Connection to tcp://127.0.0.1:34763 has been closed.\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:46,531 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:44715. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:46,531 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:41825. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:46,531 | distributed.core | INFO] Connection to tcp://127.0.0.1:34763 has been closed.\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:46,532 | distributed.core | INFO] Connection to tcp://127.0.0.1:34763 has been closed.\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:46,532 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:44693. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:46,532 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:52834; closing.\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:46,533 | distributed.core | INFO] Connection to tcp://127.0.0.1:34763 has been closed.\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:46,533 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:52876; closing.\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:46,533 | distributed.core | INFO] Connection to tcp://127.0.0.1:34763 has been closed.\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:46,533 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:52918; closing.\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:46,533 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:52848; closing.\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:46,533 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:52864; closing.\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:46,534 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:41397', name: 0, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718283946.5341437')\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:46,534 | distributed.core | INFO] Connection to tcp://127.0.0.1:34763 has been closed.\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:46,534 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:36103', name: 1, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718283946.5346673')\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:46,535 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:42391', name: 2, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718283946.535046')\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:46,535 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:37587', name: 3, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718283946.5353034')\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:46,535 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:43863', name: 4, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718283946.5355392')\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:46,537 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:52892; closing.\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:46,537 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:52906; closing.\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:46,539 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:44715', name: 6, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718283946.5394065')\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:46,540 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:41825', name: 5, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718283946.5401647')\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:46,540 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:52844; closing.\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:46,541 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:44693', name: 7, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718283946.5418456')\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:46,542 | distributed.scheduler | INFO] Lost all workers\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:47,231 | distributed.scheduler | INFO] Scheduler closing due to unknown reason...\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:47,232 | distributed.scheduler | INFO] Scheduler closing all comms\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:47,238 | fondant.component.executor | INFO] Saving output manifest to /.fondant/feature_extraction_pipeline/feature_extraction_pipeline-20240613150520/ifeatureomega_component/manifest.json\n",
      "ifeatureomega_component-1                       | [2024-06-13 13:05:47,239 | fondant.component.executor | INFO] Writing cache key with manifest reference to /.fondant/feature_extraction_pipeline/cache/44b22757d2f1e0f13fe50dc2411c045c.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[Kifeatureomega_component-1 exited with code 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "filter_pdb_component-1                          | [2024-06-13 13:05:49,998 | fondant.cli | INFO] Component `FilterPDBComponent` found in module main\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:50,006 | fondant.component.executor | INFO] Caching disabled for the component\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:50,006 | root | INFO] Executing component\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:50,311 | distributed.http.proxy | INFO] To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:50,344 | distributed.scheduler | INFO] State start\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:50,349 | distributed.scheduler | INFO]   Scheduler at:     tcp://127.0.0.1:44069\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:50,350 | distributed.scheduler | INFO]   dashboard at:  http://127.0.0.1:8787/status\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:50,350 | distributed.scheduler | INFO] Registering Worker plugin shuffle\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:50,390 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:40877'\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:50,400 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:39539'\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:50,404 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:36237'\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:50,406 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:34905'\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:50,416 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:39757'\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:50,419 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:40077'\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:50,426 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:43413'\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:50,431 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:33049'\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,422 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:33317\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,422 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:33317\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,422 | distributed.worker | INFO]           Worker name:                          0\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,422 | distributed.worker | INFO]          dashboard at:            127.0.0.1:33683\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,422 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:44069\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,422 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,422 | distributed.worker | INFO]               Threads:                          1\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,422 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,423 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-mtqiebsc\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,423 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,442 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:33317', name: 0, status: init, memory: 0, processing: 0>\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,449 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:33317\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,449 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:51574\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,451 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,452 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:44069\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,452 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,456 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:44069\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,668 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:41743\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,669 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:41743\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,669 | distributed.worker | INFO]           Worker name:                          7\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,669 | distributed.worker | INFO]          dashboard at:            127.0.0.1:37017\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,669 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:44069\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,669 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,669 | distributed.worker | INFO]               Threads:                          1\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,669 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,670 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-bxn8oqam\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,670 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,684 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:41743', name: 7, status: init, memory: 0, processing: 0>\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,699 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:41743\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,700 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:51584\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,706 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,713 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:44069\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,713 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,714 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:44069\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,788 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:41271\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,788 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:41271\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,788 | distributed.worker | INFO]           Worker name:                          4\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,788 | distributed.worker | INFO]          dashboard at:            127.0.0.1:39277\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,788 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:44069\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,788 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,789 | distributed.worker | INFO]               Threads:                          1\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,789 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,789 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-5t2w2uve\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,789 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,789 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:34175\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,790 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:34175\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,790 | distributed.worker | INFO]           Worker name:                          2\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,790 | distributed.worker | INFO]          dashboard at:            127.0.0.1:39173\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,790 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:44069\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,790 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,790 | distributed.worker | INFO]               Threads:                          1\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,790 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,790 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-647wa483\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,790 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,797 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:41271', name: 4, status: init, memory: 0, processing: 0>\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,798 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:41271\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,798 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:51588\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,798 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,799 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:44069\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,799 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,800 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:34175', name: 2, status: init, memory: 0, processing: 0>\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,800 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:44069\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,800 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:34175\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,800 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:51598\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,801 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,803 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:44069\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,803 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,804 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:44069\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,865 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:41025\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,865 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:41025\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,865 | distributed.worker | INFO]           Worker name:                          3\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,865 | distributed.worker | INFO]          dashboard at:            127.0.0.1:35709\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,866 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:44069\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,866 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,866 | distributed.worker | INFO]               Threads:                          1\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,866 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,866 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-5l201f8w\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,866 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,872 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:41025', name: 3, status: init, memory: 0, processing: 0>\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,873 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:41025\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,873 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:51612\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,873 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,874 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:44069\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,874 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,875 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:44069\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,904 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:35703\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,904 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:35703\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,905 | distributed.worker | INFO]           Worker name:                          6\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,905 | distributed.worker | INFO]          dashboard at:            127.0.0.1:34927\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,905 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:44069\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,905 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,905 | distributed.worker | INFO]               Threads:                          1\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,905 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,905 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-_mzzpbn0\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,905 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,910 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:35703', name: 6, status: init, memory: 0, processing: 0>\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,911 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:35703\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,911 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:51622\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,912 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,912 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:44069\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,912 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,913 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:44069\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,945 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:41853\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,945 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:41853\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,945 | distributed.worker | INFO]           Worker name:                          1\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,945 | distributed.worker | INFO]          dashboard at:            127.0.0.1:41383\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,945 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:44069\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,945 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,945 | distributed.worker | INFO]               Threads:                          1\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,945 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,945 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-siuw3_1f\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,945 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,949 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:41853', name: 1, status: init, memory: 0, processing: 0>\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,950 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:41853\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,950 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:51630\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,950 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,951 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:44069\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,951 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:52,952 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:44069\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,092 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:45745\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,092 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:45745\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,092 | distributed.worker | INFO]           Worker name:                          5\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,092 | distributed.worker | INFO]          dashboard at:            127.0.0.1:33809\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,092 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:44069\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,092 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,092 | distributed.worker | INFO]               Threads:                          1\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,092 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,092 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-__e46cvx\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,092 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,096 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:45745', name: 5, status: init, memory: 0, processing: 0>\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,097 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:45745\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,097 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:51642\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,098 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,098 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:44069\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,098 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,099 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:44069\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,114 | distributed.scheduler | INFO] Receive client connection: Client-a9fd3f9a-2985-11ef-8001-0242ac120002\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,115 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:51652\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,167 | fondant.component.data_io | INFO] The number of partitions of the input dataframe is 2. The available number of workers is 8.\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,168 | fondant.component.data_io | INFO] Repartitioning the data to 8 partitions before processing to maximize worker usage\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,169 | root | INFO] Columns of dataframe: ['sequence', 'sequence_checksum']\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,193 | root | INFO] Creating write task for: /.fondant/feature_extraction_pipeline/feature_extraction_pipeline-20240613150520/filter_pdb_component\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,193 | root | INFO] Writing data...\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,650 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:40877'. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,650 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,651 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:39539'. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,651 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,652 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:36237'. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,652 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:33317. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,653 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,653 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:34905'. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,653 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:41853. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,654 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,654 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:39757'. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,656 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,656 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:40077'. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,656 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:41025. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,657 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,657 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:43413'. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,656 | distributed.core | INFO] Connection to tcp://127.0.0.1:44069 has been closed.\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,657 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:41271. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,658 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:34175. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,658 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,658 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:33049'. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,658 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:45745. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,658 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,659 | distributed.core | INFO] Connection to tcp://127.0.0.1:44069 has been closed.\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,660 | distributed.core | INFO] Connection to tcp://127.0.0.1:44069 has been closed.\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,660 | distributed.core | INFO] Connection to tcp://127.0.0.1:44069 has been closed.\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,660 | distributed.core | INFO] Connection to tcp://127.0.0.1:44069 has been closed.\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,661 | distributed.core | INFO] Connection to tcp://127.0.0.1:44069 has been closed.\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,662 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:41743. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,662 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:51574; closing.\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,662 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:51630; closing.\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,663 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:51612; closing.\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,663 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:51588; closing.\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,664 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:33317', name: 0, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718283953.6644683')\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,665 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:41853', name: 1, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718283953.6654122')\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,666 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:41025', name: 3, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718283953.6659813')\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,666 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:35703. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,666 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:41271', name: 4, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718283953.6663494')\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,666 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:51598; closing.\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,667 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:51642; closing.\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,665 | distributed.core | INFO] Connection to tcp://127.0.0.1:44069 has been closed.\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,668 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:34175', name: 2, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718283953.6680517')\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,668 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:45745', name: 5, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718283953.6685567')\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,668 | distributed.core | INFO] Connection to tcp://127.0.0.1:44069 has been closed.\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,671 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:51584; closing.\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,671 | distributed.batched | INFO] Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:44069 remote=tcp://127.0.0.1:51598>\n",
      "filter_pdb_component-1                          | Traceback (most recent call last):\n",
      "filter_pdb_component-1                          |   File \"/usr/local/lib/python3.11/site-packages/distributed/batched.py\", line 115, in _background_send\n",
      "filter_pdb_component-1                          |     nbytes = yield coro\n",
      "filter_pdb_component-1                          |              ^^^^^^^^^^\n",
      "filter_pdb_component-1                          |   File \"/usr/local/lib/python3.11/site-packages/tornado/gen.py\", line 767, in run\n",
      "filter_pdb_component-1                          |     value = future.result()\n",
      "filter_pdb_component-1                          |             ^^^^^^^^^^^^^^^\n",
      "filter_pdb_component-1                          |   File \"/usr/local/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 262, in write\n",
      "filter_pdb_component-1                          |     raise CommClosedError()\n",
      "filter_pdb_component-1                          | distributed.comm.core.CommClosedError\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,675 | distributed.batched | INFO] Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:44069 remote=tcp://127.0.0.1:51642>\n",
      "filter_pdb_component-1                          | Traceback (most recent call last):\n",
      "filter_pdb_component-1                          |   File \"/usr/local/lib/python3.11/site-packages/distributed/batched.py\", line 115, in _background_send\n",
      "filter_pdb_component-1                          |     nbytes = yield coro\n",
      "filter_pdb_component-1                          |              ^^^^^^^^^^\n",
      "filter_pdb_component-1                          |   File \"/usr/local/lib/python3.11/site-packages/tornado/gen.py\", line 767, in run\n",
      "filter_pdb_component-1                          |     value = future.result()\n",
      "filter_pdb_component-1                          |             ^^^^^^^^^^^^^^^\n",
      "filter_pdb_component-1                          |   File \"/usr/local/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 262, in write\n",
      "filter_pdb_component-1                          |     raise CommClosedError()\n",
      "filter_pdb_component-1                          | distributed.comm.core.CommClosedError\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,676 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:41743', name: 7, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718283953.6765726')\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,677 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:51622; closing.\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,677 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:35703', name: 6, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718283953.677889')\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:53,678 | distributed.scheduler | INFO] Lost all workers\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:54,124 | distributed.scheduler | INFO] Scheduler closing due to unknown reason...\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:54,125 | distributed.scheduler | INFO] Scheduler closing all comms\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:54,130 | fondant.component.executor | INFO] Saving output manifest to /.fondant/feature_extraction_pipeline/feature_extraction_pipeline-20240613150520/filter_pdb_component/manifest.json\n",
      "filter_pdb_component-1                          | [2024-06-13 13:05:54,130 | fondant.component.executor | INFO] Writing cache key with manifest reference to /.fondant/feature_extraction_pipeline/cache/f6f5d5996b5645d3b1368d06d74e99a2.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[Kfilter_pdb_component-1 exited with code 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:56,527 | fondant.cli | INFO] Component `PredictProtein3DStructureComponent` found in module main\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:56,535 | fondant.component.executor | INFO] Caching disabled for the component\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:56,535 | root | INFO] Executing component\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:56,920 | distributed.http.proxy | INFO] To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:56,949 | distributed.scheduler | INFO] State start\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:56,955 | distributed.scheduler | INFO]   Scheduler at:     tcp://127.0.0.1:40643\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:56,955 | distributed.scheduler | INFO]   dashboard at:  http://127.0.0.1:8787/status\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:56,955 | distributed.scheduler | INFO] Registering Worker plugin shuffle\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:56,982 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:33005'\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:56,988 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:41553'\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:56,991 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:42269'\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:56,993 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:37835'\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:57,012 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:42181'\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:57,022 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:35273'\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:57,024 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:36345'\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:57,027 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:44499'\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,705 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:33333\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,705 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:33333\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,705 | distributed.worker | INFO]           Worker name:                          4\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,705 | distributed.worker | INFO]          dashboard at:            127.0.0.1:39019\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,705 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:40643\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,705 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,705 | distributed.worker | INFO]               Threads:                          1\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,705 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,706 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-7kxz45g6\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,706 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,714 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:33333', name: 4, status: init, memory: 0, processing: 0>\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,719 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:33333\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,719 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:58332\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,721 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,721 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:40643\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,722 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,724 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:40643\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,756 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:33345\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,756 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:33345\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,756 | distributed.worker | INFO]           Worker name:                          6\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,756 | distributed.worker | INFO]          dashboard at:            127.0.0.1:39037\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,756 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:40643\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,756 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,756 | distributed.worker | INFO]               Threads:                          1\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,756 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,756 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-ryridmac\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,757 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,763 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:33345', name: 6, status: init, memory: 0, processing: 0>\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,764 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:33345\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,764 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:58338\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,766 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,769 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:40643\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,769 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,771 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:40643\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,816 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:42645\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,816 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:42645\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,816 | distributed.worker | INFO]           Worker name:                          1\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,816 | distributed.worker | INFO]          dashboard at:            127.0.0.1:41591\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,816 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:40643\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,816 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,816 | distributed.worker | INFO]               Threads:                          1\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,816 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,817 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-l0kcwh3r\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,817 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,824 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:42645', name: 1, status: init, memory: 0, processing: 0>\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,825 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:42645\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,825 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:58344\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,826 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,827 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:40643\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,827 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,828 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:40643\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,911 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:32813\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,911 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:32813\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,911 | distributed.worker | INFO]           Worker name:                          0\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,911 | distributed.worker | INFO]          dashboard at:            127.0.0.1:37897\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,912 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:40643\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,912 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,912 | distributed.worker | INFO]               Threads:                          1\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,912 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,912 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-w3839b8x\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,912 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,918 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:32813', name: 0, status: init, memory: 0, processing: 0>\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,918 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:32813\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,919 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:58358\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,919 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,920 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:40643\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,920 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,921 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:40643\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,927 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:40083\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,928 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:40083\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,928 | distributed.worker | INFO]           Worker name:                          5\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,928 | distributed.worker | INFO]          dashboard at:            127.0.0.1:43457\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,928 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:40643\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,928 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,928 | distributed.worker | INFO]               Threads:                          1\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,928 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,928 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-zg5dszbn\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,928 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,936 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:40083', name: 5, status: init, memory: 0, processing: 0>\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,937 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:40083\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,937 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:58368\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,938 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,938 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:40643\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,939 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,940 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:40643\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,951 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:34139\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,951 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:34139\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,951 | distributed.worker | INFO]           Worker name:                          2\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,951 | distributed.worker | INFO]          dashboard at:            127.0.0.1:40091\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,951 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:40643\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,951 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,951 | distributed.worker | INFO]               Threads:                          1\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,951 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,951 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-g609xpmv\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,951 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,955 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:46343\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,955 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:46343\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,955 | distributed.worker | INFO]           Worker name:                          7\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,955 | distributed.worker | INFO]          dashboard at:            127.0.0.1:41461\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,955 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:40643\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,955 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,955 | distributed.worker | INFO]               Threads:                          1\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,955 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,955 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-ffuhfu3g\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,955 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,959 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:34139', name: 2, status: init, memory: 0, processing: 0>\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,959 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:34139\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,960 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:58372\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,960 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,961 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:40643\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,961 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,961 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:46343', name: 7, status: init, memory: 0, processing: 0>\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,961 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:46343\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,962 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:58374\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,962 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:40643\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,962 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,963 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:40643\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,963 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,964 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:40643\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,995 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:37473\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,995 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:37473\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,995 | distributed.worker | INFO]           Worker name:                          3\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,995 | distributed.worker | INFO]          dashboard at:            127.0.0.1:45931\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,995 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:40643\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,995 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,995 | distributed.worker | INFO]               Threads:                          1\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,995 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,995 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-uocag5_c\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:58,995 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,000 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:37473', name: 3, status: init, memory: 0, processing: 0>\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,000 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:37473\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,000 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:58384\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,001 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,002 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:40643\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,002 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,002 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:40643\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,057 | distributed.scheduler | INFO] Receive client connection: Client-ad8801f5-2985-11ef-8001-0242ac120002\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,057 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:58390\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,093 | fondant.component.data_io | INFO] The number of partitions of the input dataframe is 5. The available number of workers is 8.\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,094 | fondant.component.data_io | INFO] Repartitioning the data to 8 partitions before processing to maximize worker usage\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,094 | root | INFO] Columns of dataframe: ['sequence', 'sequence_checksum', 'pdb_string']\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,117 | root | INFO] Creating write task for: /.fondant/feature_extraction_pipeline/feature_extraction_pipeline-20240613150520/predict_protein_3d_structure_component\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,118 | root | INFO] Writing data...\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,476 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:33005'. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,476 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,476 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:41553'. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,477 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,477 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:42269'. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,477 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:32813. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,477 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,477 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:37835'. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,477 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:42645. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,478 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,478 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:42181'. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,478 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:34139. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,478 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,479 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:35273'. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,479 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,479 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:36345'. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,479 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,479 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:44499'. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,479 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,479 | distributed.core | INFO] Connection to tcp://127.0.0.1:40643 has been closed.\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,479 | distributed.core | INFO] Connection to tcp://127.0.0.1:40643 has been closed.\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,480 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:40083. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,480 | distributed.core | INFO] Connection to tcp://127.0.0.1:40643 has been closed.\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,480 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:33333. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,480 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:46343. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,481 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:58358; closing.\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,481 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:58344; closing.\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,481 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:58372; closing.\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,481 | distributed.core | INFO] Connection to tcp://127.0.0.1:40643 has been closed.\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,481 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:32813', name: 0, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718283959.4818213')\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,482 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:42645', name: 1, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718283959.482371')\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,482 | distributed.core | INFO] Connection to tcp://127.0.0.1:40643 has been closed.\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,482 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:34139', name: 2, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718283959.4827616')\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,483 | distributed.core | INFO] Connection to tcp://127.0.0.1:40643 has been closed.\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,483 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:37473. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,483 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:58368; closing.\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,485 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:40083', name: 5, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718283959.485128')\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,485 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:33345. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,486 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:58332; closing.\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,487 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:33333', name: 4, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718283959.4873676')\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,488 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:58374; closing.\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,489 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:46343', name: 7, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718283959.4897106')\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,490 | distributed.core | INFO] Connection to tcp://127.0.0.1:40643 has been closed.\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,494 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:58338; closing.\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,497 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:33345', name: 6, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718283959.4971118')\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,503 | distributed.core | INFO] Connection to tcp://127.0.0.1:40643 has been closed.\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,507 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:58384; closing.\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,509 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:37473', name: 3, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718283959.5088794')\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,510 | distributed.scheduler | INFO] Lost all workers\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,925 | distributed.scheduler | INFO] Scheduler closing due to unknown reason...\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,926 | distributed.scheduler | INFO] Scheduler closing all comms\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,933 | fondant.component.executor | INFO] Saving output manifest to /.fondant/feature_extraction_pipeline/feature_extraction_pipeline-20240613150520/predict_protein_3d_structure_component/manifest.json\n",
      "predict_protein_3d_structure_component-1        | [2024-06-13 13:05:59,933 | fondant.component.executor | INFO] Writing cache key with manifest reference to /.fondant/feature_extraction_pipeline/cache/ef84054d8b46c910a0bd53e4868455b3.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[Kpredict_protein_3d_structure_component-1 exited with code 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "store_pdb_component-1                           | [2024-06-13 13:06:02,282 | fondant.cli | INFO] Component `StorePDBComponent` found in module main\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:02,289 | fondant.component.executor | INFO] Caching disabled for the component\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:02,289 | root | INFO] Executing component\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:02,629 | distributed.http.proxy | INFO] To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:02,655 | distributed.scheduler | INFO] State start\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:02,659 | distributed.scheduler | INFO]   Scheduler at:     tcp://127.0.0.1:37565\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:02,659 | distributed.scheduler | INFO]   dashboard at:  http://127.0.0.1:8787/status\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:02,659 | distributed.scheduler | INFO] Registering Worker plugin shuffle\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:02,684 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:42075'\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:02,691 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:39817'\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:02,693 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:45631'\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:02,695 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:34679'\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:02,713 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:39853'\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:02,717 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:38555'\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:02,719 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:44689'\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:02,723 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:33323'\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,355 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:36225\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,355 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:36225\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,355 | distributed.worker | INFO]           Worker name:                          4\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,356 | distributed.worker | INFO]          dashboard at:            127.0.0.1:40195\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,357 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:37565\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,357 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,358 | distributed.worker | INFO]               Threads:                          1\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,358 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,358 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-9on2bzmi\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,359 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,371 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:36225', name: 4, status: init, memory: 0, processing: 0>\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,378 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,378 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:36225\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,391 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:50414\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,392 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:37565\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,393 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,394 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:37565\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,410 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:32995\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,411 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:32995\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,411 | distributed.worker | INFO]           Worker name:                          3\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,411 | distributed.worker | INFO]          dashboard at:            127.0.0.1:39679\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,411 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:37565\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,411 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,411 | distributed.worker | INFO]               Threads:                          1\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,411 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,411 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-2xt_tude\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,411 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,422 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:32995', name: 3, status: init, memory: 0, processing: 0>\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,423 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:32995\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,424 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:50416\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,425 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,425 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:37565\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,425 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,436 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:37565\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,439 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:37667\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,439 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:37667\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,439 | distributed.worker | INFO]           Worker name:                          0\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,439 | distributed.worker | INFO]          dashboard at:            127.0.0.1:41611\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,439 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:37565\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,439 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,439 | distributed.worker | INFO]               Threads:                          1\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,439 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,440 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-blqaanxo\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,440 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,449 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:37667', name: 0, status: init, memory: 0, processing: 0>\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,450 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:37667\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,450 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:50432\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,454 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,460 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:37565\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,460 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,462 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:37565\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,489 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:33015\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,489 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:33015\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,489 | distributed.worker | INFO]           Worker name:                          2\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,489 | distributed.worker | INFO]          dashboard at:            127.0.0.1:41859\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,489 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:37565\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,489 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,489 | distributed.worker | INFO]               Threads:                          1\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,489 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,489 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-acpgaoxu\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,490 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,492 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:35635\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,492 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:35635\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,492 | distributed.worker | INFO]           Worker name:                          6\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,492 | distributed.worker | INFO]          dashboard at:            127.0.0.1:40385\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,493 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:37565\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,493 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,493 | distributed.worker | INFO]               Threads:                          1\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,493 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,493 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-yqotr0l3\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,493 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,495 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:33015', name: 2, status: init, memory: 0, processing: 0>\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,496 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:33015\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,496 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:50446\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,497 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,497 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:37565\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,498 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,498 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:37565\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,500 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:35635', name: 6, status: init, memory: 0, processing: 0>\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,500 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:35635\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,500 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:50456\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,501 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,501 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:37565\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,501 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,502 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:37565\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,503 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:43055\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,503 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:43055\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,503 | distributed.worker | INFO]           Worker name:                          7\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,503 | distributed.worker | INFO]          dashboard at:            127.0.0.1:35535\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,503 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:37565\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,503 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,503 | distributed.worker | INFO]               Threads:                          1\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,503 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,503 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-gil69zaq\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,504 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,508 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:43055', name: 7, status: init, memory: 0, processing: 0>\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,508 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:43055\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,508 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:50460\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,509 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,510 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:37565\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,510 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,511 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:37565\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,531 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:45809\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,531 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:45809\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,531 | distributed.worker | INFO]           Worker name:                          5\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,531 | distributed.worker | INFO]          dashboard at:            127.0.0.1:45851\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,531 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:37565\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,531 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,531 | distributed.worker | INFO]               Threads:                          1\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,531 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,531 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-5hcu15f3\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,531 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,535 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:45809', name: 5, status: init, memory: 0, processing: 0>\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,536 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:45809\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,536 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:50474\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,536 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,537 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:37565\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,537 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,537 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:37565\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,551 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:32947\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,551 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:32947\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,551 | distributed.worker | INFO]           Worker name:                          1\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,551 | distributed.worker | INFO]          dashboard at:            127.0.0.1:44379\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,551 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:37565\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,551 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,551 | distributed.worker | INFO]               Threads:                          1\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,551 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,551 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-94d5j65t\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,551 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,556 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:32947', name: 1, status: init, memory: 0, processing: 0>\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,556 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:32947\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,556 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:50476\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,557 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,557 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:37565\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,557 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,558 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:37565\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,605 | distributed.scheduler | INFO] Receive client connection: Client-b0d69218-2985-11ef-8001-0242ac120002\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,605 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:50478\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,640 | fondant.component.data_io | INFO] The number of partitions of the input dataframe is 5. The available number of workers is 8.\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,641 | fondant.component.data_io | INFO] Repartitioning the data to 8 partitions before processing to maximize worker usage\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,642 | root | INFO] Columns of dataframe: ['sequence', 'sequence_checksum', 'pdb_string']\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,667 | root | INFO] Creating write task for: /.fondant/feature_extraction_pipeline/feature_extraction_pipeline-20240613150520/store_pdb_component\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:04,667 | root | INFO] Writing data...\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:05,067 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:42075'. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:05,068 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:05,069 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:39817'. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:05,069 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:05,070 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:45631'. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:05,070 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:37667. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:05,071 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:05,071 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:34679'. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:05,071 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:32947. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:05,072 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:05,072 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:39853'. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:05,073 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:05,073 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:38555'. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:05,073 | distributed.core | INFO] Connection to tcp://127.0.0.1:37565 has been closed.\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:05,074 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:32995. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:05,074 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:33015. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:05,074 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:05,075 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:44689'. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:05,075 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:05,075 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:33323'. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:05,076 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:05,078 | distributed.core | INFO] Connection to tcp://127.0.0.1:37565 has been closed.\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:05,078 | distributed.core | INFO] Connection to tcp://127.0.0.1:37565 has been closed.\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:05,081 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:45809. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:05,082 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:50432; closing.\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:05,082 | distributed.core | INFO] Connection to tcp://127.0.0.1:37565 has been closed.\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:05,083 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:50476; closing.\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:05,085 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:43055. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:05,086 | distributed.core | INFO] Connection to tcp://127.0.0.1:37565 has been closed.\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:05,089 | distributed.core | INFO] Connection to tcp://127.0.0.1:37565 has been closed.\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:05,093 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:50416; closing.\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:05,093 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:50446; closing.\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:05,095 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:37667', name: 0, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718283965.0949845')\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:05,097 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:35635. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:05,104 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:36225. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:05,106 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:32947', name: 1, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718283965.1051373')\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:05,107 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:32995', name: 3, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718283965.107223')\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:05,108 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:33015', name: 2, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718283965.1082053')\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:05,108 | distributed.core | INFO] Connection to tcp://127.0.0.1:37565 has been closed.\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:05,110 | distributed.core | INFO] Connection to tcp://127.0.0.1:37565 has been closed.\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:05,115 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:50474; closing.\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:05,116 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:50460; closing.\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:05,120 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:45809', name: 5, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718283965.120248')\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:05,121 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:43055', name: 7, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718283965.121025')\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:05,121 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:50456; closing.\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:05,121 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:50414; closing.\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:05,122 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:35635', name: 6, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718283965.1226823')\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:05,123 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:36225', name: 4, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718283965.1232364')\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:05,123 | distributed.scheduler | INFO] Lost all workers\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:05,518 | distributed.scheduler | INFO] Scheduler closing due to unknown reason...\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:05,518 | distributed.scheduler | INFO] Scheduler closing all comms\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:05,522 | fondant.component.executor | INFO] Saving output manifest to /.fondant/feature_extraction_pipeline/feature_extraction_pipeline-20240613150520/store_pdb_component/manifest.json\n",
      "store_pdb_component-1                           | [2024-06-13 13:06:05,522 | fondant.component.executor | INFO] Writing cache key with manifest reference to /.fondant/feature_extraction_pipeline/cache/40d4c0b7c5966e8ea2b51d57928a20ea.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[Kstore_pdb_component-1 exited with code 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "msa_component-1                                 | [2024-06-13 13:06:07,899 | fondant.cli | INFO] Component `MSAComponent` found in module main\n",
      "msa_component-1                                 | [2024-06-13 13:06:07,906 | fondant.component.executor | INFO] Caching disabled for the component\n",
      "msa_component-1                                 | [2024-06-13 13:06:07,906 | root | INFO] Executing component\n",
      "msa_component-1                                 | [2024-06-13 13:06:08,220 | distributed.http.proxy | INFO] To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy\n",
      "msa_component-1                                 | [2024-06-13 13:06:08,244 | distributed.scheduler | INFO] State start\n",
      "msa_component-1                                 | [2024-06-13 13:06:08,247 | distributed.scheduler | INFO]   Scheduler at:     tcp://127.0.0.1:33531\n",
      "msa_component-1                                 | [2024-06-13 13:06:08,248 | distributed.scheduler | INFO]   dashboard at:  http://127.0.0.1:8787/status\n",
      "msa_component-1                                 | [2024-06-13 13:06:08,248 | distributed.scheduler | INFO] Registering Worker plugin shuffle\n",
      "msa_component-1                                 | [2024-06-13 13:06:08,269 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:44887'\n",
      "msa_component-1                                 | [2024-06-13 13:06:08,275 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:34077'\n",
      "msa_component-1                                 | [2024-06-13 13:06:08,278 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:39845'\n",
      "msa_component-1                                 | [2024-06-13 13:06:08,279 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:40145'\n",
      "msa_component-1                                 | [2024-06-13 13:06:08,291 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:43315'\n",
      "msa_component-1                                 | [2024-06-13 13:06:08,293 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:38463'\n",
      "msa_component-1                                 | [2024-06-13 13:06:08,295 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:40063'\n",
      "msa_component-1                                 | [2024-06-13 13:06:08,303 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:33035'\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,924 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:37517\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,924 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:37517\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,924 | distributed.worker | INFO]           Worker name:                          0\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,924 | distributed.worker | INFO]          dashboard at:            127.0.0.1:38699\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,924 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:33531\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,924 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,924 | distributed.worker | INFO]               Threads:                          1\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,924 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,925 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-iix4h0yi\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,925 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,931 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:37517', name: 0, status: init, memory: 0, processing: 0>\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,937 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:40695\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,937 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:40695\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,937 | distributed.worker | INFO]           Worker name:                          4\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,937 | distributed.worker | INFO]          dashboard at:            127.0.0.1:44749\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,937 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:33531\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,937 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,937 | distributed.worker | INFO]               Threads:                          1\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,937 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,938 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-0v0g6jrs\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,938 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,938 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:38953\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,938 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:38953\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,938 | distributed.worker | INFO]           Worker name:                          3\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,938 | distributed.worker | INFO]          dashboard at:            127.0.0.1:35633\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,938 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:33531\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,938 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,938 | distributed.worker | INFO]               Threads:                          1\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,939 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,939 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-t7i1q6kk\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,939 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,940 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:37517\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,940 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:51818\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,942 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,943 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:33531\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,943 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,944 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:33531\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,949 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:40695', name: 4, status: init, memory: 0, processing: 0>\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,950 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:40695\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,950 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:51828\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,951 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,952 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:33531\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,952 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,952 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:38953', name: 3, status: init, memory: 0, processing: 0>\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,953 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:33531\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,954 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,955 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:33531\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,955 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,953 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:38953\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,955 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:51834\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,957 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:35019\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,958 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:35019\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,958 | distributed.worker | INFO]           Worker name:                          1\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,958 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:33531\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,958 | distributed.worker | INFO]          dashboard at:            127.0.0.1:45957\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,958 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:33531\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,959 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,959 | distributed.worker | INFO]               Threads:                          1\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,959 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,959 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-rgwnx08m\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,959 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,968 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:35019', name: 1, status: init, memory: 0, processing: 0>\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,969 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:35019\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,969 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:51838\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,969 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,970 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:33531\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,970 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-06-13 13:06:09,971 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:33531\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,013 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:37633\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,013 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:37633\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,013 | distributed.worker | INFO]           Worker name:                          2\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,013 | distributed.worker | INFO]          dashboard at:            127.0.0.1:33013\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,013 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:33531\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,013 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,013 | distributed.worker | INFO]               Threads:                          1\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,013 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,013 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-dfw75t1s\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,013 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,019 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:37633', name: 2, status: init, memory: 0, processing: 0>\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,020 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:37633\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,020 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:51852\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,020 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,021 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:33531\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,021 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,022 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:33531\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,061 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:39247\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,061 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:39247\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,061 | distributed.worker | INFO]           Worker name:                          5\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,061 | distributed.worker | INFO]          dashboard at:            127.0.0.1:46615\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,061 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:33531\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,061 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,061 | distributed.worker | INFO]               Threads:                          1\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,061 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,061 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-o2h4anru\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,061 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,069 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:39247', name: 5, status: init, memory: 0, processing: 0>\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,069 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:39247\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,070 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:51864\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,070 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,072 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:33531\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,072 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,073 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:33531\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,083 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:43421\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,083 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:43421\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,083 | distributed.worker | INFO]           Worker name:                          6\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,083 | distributed.worker | INFO]          dashboard at:            127.0.0.1:41587\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,083 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:33531\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,083 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,084 | distributed.worker | INFO]               Threads:                          1\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,084 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,084 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-ixz3qqh2\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,084 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,090 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:43421', name: 6, status: init, memory: 0, processing: 0>\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,091 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:43421\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,091 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:51868\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,092 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,092 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:33531\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,092 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,093 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:33531\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,276 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:40329\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,276 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:40329\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,276 | distributed.worker | INFO]           Worker name:                          7\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,276 | distributed.worker | INFO]          dashboard at:            127.0.0.1:36607\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,276 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:33531\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,276 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,276 | distributed.worker | INFO]               Threads:                          1\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,276 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,276 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-7mxbsaak\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,276 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,281 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:40329', name: 7, status: init, memory: 0, processing: 0>\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,282 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:40329\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,282 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:51870\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,282 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,283 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:33531\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,283 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,283 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:33531\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,326 | distributed.scheduler | INFO] Receive client connection: Client-b43f6d03-2985-11ef-8001-0242ac120002\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,326 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:51878\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,419 | fondant.component.data_io | INFO] Total number of rows is 5.\n",
      "msa_component-1                                 | Repartitioning the data from <dask.utils.IndexCallable object at 0x7f716f9f2c50> partitions to have 1 such that the number of partitions per row is approximately10000\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,419 | fondant.component.data_io | WARNING] Setting the `input partition rows` has caused the system to not utilize all available workers 1 out of 8 are used.\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,420 | root | INFO] Columns of dataframe: ['sequence', 'sequence_checksum']\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,445 | root | INFO] Creating write task for: /.fondant/feature_extraction_pipeline/feature_extraction_pipeline-20240613150520/msa_component\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,445 | root | INFO] Writing data...\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,799 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:44887'. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,799 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,800 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:34077'. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,800 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,800 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:39845'. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,800 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:37517. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,800 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,801 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:40145'. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,801 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:35019. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,801 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:37633. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,802 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,802 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:43315'. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,802 | distributed.core | INFO] Connection to tcp://127.0.0.1:33531 has been closed.\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,802 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,802 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:38463'. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,802 | distributed.core | INFO] Connection to tcp://127.0.0.1:33531 has been closed.\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,803 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,803 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:40063'. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,803 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,803 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:38953. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,803 | distributed.core | INFO] Connection to tcp://127.0.0.1:33531 has been closed.\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,803 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:33035'. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,803 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,804 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:39247. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,804 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:43421. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,804 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:40695. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,804 | distributed.core | INFO] Connection to tcp://127.0.0.1:33531 has been closed.\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,805 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:51818; closing.\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,805 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:51838; closing.\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,805 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:51852; closing.\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,805 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:51834; closing.\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,806 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:37517', name: 0, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718283970.8060424')\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,806 | distributed.core | INFO] Connection to tcp://127.0.0.1:33531 has been closed.\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,806 | distributed.core | INFO] Connection to tcp://127.0.0.1:33531 has been closed.\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,806 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:35019', name: 1, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718283970.8065298')\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,807 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:40329. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,807 | distributed.core | INFO] Connection to tcp://127.0.0.1:33531 has been closed.\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,807 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:37633', name: 2, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718283970.8071094')\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,809 | distributed.core | INFO] Connection to tcp://127.0.0.1:33531 has been closed.\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,810 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:38953', name: 3, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718283970.8108437')\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,812 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:51864; closing.\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,812 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:51868; closing.\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,817 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:39247', name: 5, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718283970.8171737')\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,818 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:43421', name: 6, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718283970.8178968')\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,818 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:51828; closing.\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,819 | distributed.batched | INFO] Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:33531 remote=tcp://127.0.0.1:51864>\n",
      "msa_component-1                                 | Traceback (most recent call last):\n",
      "msa_component-1                                 |   File \"/usr/local/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 297, in write\n",
      "msa_component-1                                 |     raise StreamClosedError()\n",
      "msa_component-1                                 | tornado.iostream.StreamClosedError: Stream is closed\n",
      "msa_component-1                                 | \n",
      "msa_component-1                                 | The above exception was the direct cause of the following exception:\n",
      "msa_component-1                                 | \n",
      "msa_component-1                                 | Traceback (most recent call last):\n",
      "msa_component-1                                 |   File \"/usr/local/lib/python3.11/site-packages/distributed/batched.py\", line 115, in _background_send\n",
      "msa_component-1                                 |     nbytes = yield coro\n",
      "msa_component-1                                 |              ^^^^^^^^^^\n",
      "msa_component-1                                 |   File \"/usr/local/lib/python3.11/site-packages/tornado/gen.py\", line 767, in run\n",
      "msa_component-1                                 |     value = future.result()\n",
      "msa_component-1                                 |             ^^^^^^^^^^^^^^^\n",
      "msa_component-1                                 |   File \"/usr/local/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 307, in write\n",
      "msa_component-1                                 |     convert_stream_closed_error(self, e)\n",
      "msa_component-1                                 |   File \"/usr/local/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "msa_component-1                                 |     raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "msa_component-1                                 | distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:33531 remote=tcp://127.0.0.1:51864>: Stream is closed\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,822 | distributed.batched | INFO] Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:33531 remote=tcp://127.0.0.1:51868>\n",
      "msa_component-1                                 | Traceback (most recent call last):\n",
      "msa_component-1                                 |   File \"/usr/local/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 297, in write\n",
      "msa_component-1                                 |     raise StreamClosedError()\n",
      "msa_component-1                                 | tornado.iostream.StreamClosedError: Stream is closed\n",
      "msa_component-1                                 | \n",
      "msa_component-1                                 | The above exception was the direct cause of the following exception:\n",
      "msa_component-1                                 | \n",
      "msa_component-1                                 | Traceback (most recent call last):\n",
      "msa_component-1                                 |   File \"/usr/local/lib/python3.11/site-packages/distributed/batched.py\", line 115, in _background_send\n",
      "msa_component-1                                 |     nbytes = yield coro\n",
      "msa_component-1                                 |              ^^^^^^^^^^\n",
      "msa_component-1                                 |   File \"/usr/local/lib/python3.11/site-packages/tornado/gen.py\", line 767, in run\n",
      "msa_component-1                                 |     value = future.result()\n",
      "msa_component-1                                 |             ^^^^^^^^^^^^^^^\n",
      "msa_component-1                                 |   File \"/usr/local/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 307, in write\n",
      "msa_component-1                                 |     convert_stream_closed_error(self, e)\n",
      "msa_component-1                                 |   File \"/usr/local/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "msa_component-1                                 |     raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "msa_component-1                                 | distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:33531 remote=tcp://127.0.0.1:51868>: Stream is closed\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,827 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:40695', name: 4, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718283970.8253238')\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,828 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:51870; closing.\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,829 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:40329', name: 7, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718283970.8296435')\n",
      "msa_component-1                                 | [2024-06-13 13:06:10,831 | distributed.scheduler | INFO] Lost all workers\n",
      "msa_component-1                                 | [2024-06-13 13:06:11,241 | distributed.scheduler | INFO] Scheduler closing due to unknown reason...\n",
      "msa_component-1                                 | [2024-06-13 13:06:11,241 | distributed.scheduler | INFO] Scheduler closing all comms\n",
      "msa_component-1                                 | [2024-06-13 13:06:11,246 | fondant.component.executor | INFO] Saving output manifest to /.fondant/feature_extraction_pipeline/feature_extraction_pipeline-20240613150520/msa_component/manifest.json\n",
      "msa_component-1                                 | [2024-06-13 13:06:11,246 | fondant.component.executor | INFO] Writing cache key with manifest reference to /.fondant/feature_extraction_pipeline/cache/e899a2ad0f94efe6bc5fbf060cf98440.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[Kmsa_component-1 exited with code 0\n",
      "Finished pipeline run.\n"
     ]
    }
   ],
   "source": [
    "# import shutil\n",
    "\n",
    "# remove the most recent output folder if the manifest file is removed\n",
    "# without a manifest file in the most recent output folder, the pipeline cannot be run\n",
    "# if OUTPUT_FOLDER and REMOVED_MANIFEST:\n",
    "# \tshutil.rmtree(OUTPUT_FOLDER)\n",
    "# \t# remove cache\n",
    "# \tshutil.rmtree(os.path.join(BASE_PATH, PIPELINE_NAME, \"cache\"))\n",
    "\n",
    "# get current full path to the project\n",
    "mounted_data = os.path.join(os.path.abspath(\"data\"), \":/data\")\n",
    "\n",
    "DockerRunner().run(input=pipeline, extra_volumes=mounted_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "The following results have been taken from the output of the pipeline, which is stored in the `.fondant` directory. This directory contains the output of each component, together with the cache of the previous run. Currently, the pipeline doesn't implement the `write_to_file` component, so the results will be taken individually from the output of each component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-13 16:07:21,635 | root | INFO] Last folder: .fondant/feature_extraction_pipeline/feature_extraction_pipeline-20240613150520\n"
     ]
    }
   ],
   "source": [
    "# find the most recent output folder\n",
    "# get the most recent folder in the folder named: BASE_PATH + PIPELINE_NAME + PIPELINE_NAME-<timestamp>\n",
    "matching_folders = glob.glob(f\"{BASE_PATH}/{PIPELINE_NAME}/{PIPELINE_NAME}-*\")\n",
    "\n",
    "if matching_folders:\n",
    "    last_folder = max(matching_folders, key=os.path.getctime)\n",
    "\n",
    "logging.info(f\"Last folder: {last_folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def merge_parquet_folders(folder_path):\n",
    "    df_list = []\n",
    "\n",
    "    for folder in Path(folder_path).iterdir():\n",
    "        if folder.is_dir():\n",
    "            logging.info(f\"Reading parquet partitions from: {folder}\")\n",
    "            parquet_files = list(folder.glob(\"*.parquet\"))\n",
    "            logging.info(f\"Found {len(parquet_files)} parquet files\")\n",
    "            dfs = [pd.read_parquet(file) for file in parquet_files]\n",
    "            dfs = [x for x in dfs if not x.empty]\n",
    "            if len(dfs) == 0:\n",
    "                continue\n",
    "            df = pd.concat(dfs)\n",
    "            df_list.append(df)\n",
    "\n",
    "    return df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-13 16:08:20,657 | root | INFO] Reading parquet partitions from: .fondant/feature_extraction_pipeline/feature_extraction_pipeline-20240613150520/filter_pdb_component\n",
      "[2024-06-13 16:08:20,658 | root | INFO] Found 8 parquet files\n",
      "[2024-06-13 16:08:20,686 | root | INFO] Reading parquet partitions from: .fondant/feature_extraction_pipeline/feature_extraction_pipeline-20240613150520/predict_protein_3d_structure_component\n",
      "[2024-06-13 16:08:20,687 | root | INFO] Found 8 parquet files\n",
      "[2024-06-13 16:08:20,708 | root | INFO] Reading parquet partitions from: .fondant/feature_extraction_pipeline/feature_extraction_pipeline-20240613150520/biopython_component\n",
      "[2024-06-13 16:08:20,709 | root | INFO] Found 8 parquet files\n",
      "[2024-06-13 16:08:20,737 | root | INFO] Reading parquet partitions from: .fondant/feature_extraction_pipeline/feature_extraction_pipeline-20240613150520/load_from_parquet\n",
      "[2024-06-13 16:08:20,738 | root | INFO] Found 0 parquet files\n",
      "[2024-06-13 16:08:20,738 | root | INFO] Reading parquet partitions from: .fondant/feature_extraction_pipeline/feature_extraction_pipeline-20240613150520/ifeatureomega_component\n",
      "[2024-06-13 16:08:20,739 | root | INFO] Found 2 parquet files\n",
      "[2024-06-13 16:08:20,755 | root | INFO] Reading parquet partitions from: .fondant/feature_extraction_pipeline/feature_extraction_pipeline-20240613150520/generate_protein_sequence_checksum_component\n",
      "[2024-06-13 16:08:20,755 | root | INFO] Found 8 parquet files\n",
      "[2024-06-13 16:08:20,775 | root | INFO] Reading parquet partitions from: .fondant/feature_extraction_pipeline/feature_extraction_pipeline-20240613150520/store_pdb_component\n",
      "[2024-06-13 16:08:20,776 | root | INFO] Found 8 parquet files\n",
      "[2024-06-13 16:08:20,800 | root | INFO] Reading parquet partitions from: .fondant/feature_extraction_pipeline/feature_extraction_pipeline-20240613150520/msa_component\n",
      "[2024-06-13 16:08:20,800 | root | INFO] Found 1 parquet files\n"
     ]
    }
   ],
   "source": [
    "dataframe_list = merge_parquet_folders(last_folder)\n",
    "\n",
    "\n",
    "df_final = pd.concat(dataframe_list, axis=1)\n",
    "df_final = df_final.loc[:,~df_final.columns.duplicated()]\n",
    "\n",
    "# filtering out columns that are not properly stored in a csv\n",
    "columns_to_remove = ['pdb_string']\n",
    "df_final = df_final.drop(columns=columns_to_remove)\n",
    "\n",
    "# write to file\n",
    "df_final.to_csv(f\"{last_folder}/final_output.csv\", index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "protein-feature-extraction-NoVdeDG9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
