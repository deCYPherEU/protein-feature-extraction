{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protein feature extraction pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will contain the pipeline for extracting features from protein sequences. It will be used as a way to show the output without needing to run the `pipeline.py` file locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import logging\n",
    "from fondant.pipeline import Pipeline\n",
    "from fondant.pipeline.runner import DockerRunner\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "from config import MOCK_DATA_PATH_FONDANT\n",
    "\n",
    "# check if the manifest file is removed.\n",
    "REMOVED_MANIFEST = False\n",
    "\n",
    "# check if the output folder exists\n",
    "OUTPUT_FOLDER = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Mock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python utils/generate_mock_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MNQRGMPIQSLVTNVKINRLEENDCIHTRHRVRPGRTDGKNLHAMM...</td>\n",
       "      <td>Seq1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MAGLKPEVPLHDGINKFGKSDFAGQEGPKIVTTTDKALLVANGALK...</td>\n",
       "      <td>Seq2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MVDLKKELKNFVDSDFPGSPKQEAQGIDVRILLSFNNAAFREALII...</td>\n",
       "      <td>Seq3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MELILAKARLEFECDWGLLMLEPCVPPTKIFADRNYAVGVMFESDK...</td>\n",
       "      <td>Seq4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MRVLCDGSTGYACAKNTRIRFREKVASVLAKIQGYEQTFPHHMPNM...</td>\n",
       "      <td>Seq5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sequence  name\n",
       "0  MNQRGMPIQSLVTNVKINRLEENDCIHTRHRVRPGRTDGKNLHAMM...  Seq1\n",
       "1  MAGLKPEVPLHDGINKFGKSDFAGQEGPKIVTTTDKALLVANGALK...  Seq2\n",
       "2  MVDLKKELKNFVDSDFPGSPKQEAQGIDVRILLSFNNAAFREALII...  Seq3\n",
       "3  MELILAKARLEFECDWGLLMLEPCVPPTKIFADRNYAVGVMFESDK...  Seq4\n",
       "4  MRVLCDGSTGYACAKNTRIRFREKVASVLAKIQGYEQTFPHHMPNM...  Seq5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show content of the mock data\n",
    "import pandas as pd\n",
    "mock_df = pd.read_parquet(\".\" + MOCK_DATA_PATH_FONDANT)  # dot added to make it relative to the current directory\n",
    "mock_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new pipeline\n",
    "\n",
    "BASE_PATH = \".fondant\"\n",
    "PIPELINE_NAME = \"feature_extraction_pipeline\"\n",
    "\n",
    "pipeline = Pipeline(\n",
    "\tname=PIPELINE_NAME,\n",
    "\tbase_path=BASE_PATH,\n",
    "\tdescription=\"A pipeline to extract features from protein sequences.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-07-17 12:10:36,673 | fondant.pipeline.pipeline | INFO] The consumes section of the component spec is not defined. Can not infer consumes of the OperationSpec. Please define a consumes section in the dataset interface. \n"
     ]
    }
   ],
   "source": [
    "# Read the dataset\n",
    "\n",
    "dataset = pipeline.read(\n",
    "\t\"load_from_parquet\",\n",
    "\targuments={\n",
    "\t\t\"dataset_uri\": MOCK_DATA_PATH_FONDANT,\n",
    "\t},\n",
    "\tproduces={\n",
    "\t\t\"sequence\": pa.string()\n",
    "\t}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components\n",
    "\n",
    "---\n",
    "\n",
    "### generate_protein_sequence_checksum_component\n",
    "\n",
    "This component generates a checksum for the protein sequence.\n",
    "\n",
    "---\n",
    "\n",
    "### biopython_component\n",
    "\n",
    "Extracts features from the protein sequence using Biopython.\n",
    "\n",
    "---\n",
    "\n",
    "### iFeatureOmega_component\n",
    "\n",
    "Extracts features from the protein sequence using the [iFeatureOmega-CLI GitHub repo](https://github.com/Superzchen/iFeatureOmega-CLI). Arguments are used to specify the type of features to extract.\n",
    "\n",
    "---\n",
    "\n",
    "### filter_pdb_component\n",
    "\n",
    "Filters PDB files that are already predicted to avoid redundant predictions. Arguments need to be specified before running the pipeline:\n",
    "```json\n",
    "\"storage_type\": \"local\",\n",
    "\"pdb_path\": \"/data/<your-pdb-folder-path>\",\n",
    "\"bucket_name\": \"your-bucket-name\",\n",
    "\"project_id\": \"your-project-id\",\n",
    "\"google_cloud_credentials_path\": \"/data/<your-credentials>.json\"\n",
    "```\n",
    "\n",
    "If only using local, keep bucket_name, project_id, and google_cloud_credentials_path as empty strings. Using remote requires a Google Cloud Storage bucket with credentials and a project ID.\n",
    "\n",
    "---\n",
    "\n",
    "### predict_protein_3D_structure_component\n",
    "\n",
    "Predicts the 3D structure of the protein using ESMFold. This component requires a `.env` file with the following variables:\n",
    "```env\n",
    "HF_API_KEY=\"\"\n",
    "HF_ENDPOINT_URL=\"\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### store_pdb_component\n",
    "\n",
    "Stores the PDB files in the provided storage_type. Arguments need to be specified before running the pipeline:\n",
    "```json\n",
    "\"storage_type\": \"local\",\n",
    "\"pdb_path\": \"/data/<your-pdb-folder-path>\",\n",
    "\"bucket_name\": \"your-bucket-name\",\n",
    "\"project_id\": \"your-project-id\",\n",
    "\"google_cloud_credentials_path\": \"/data/<your-credentials>.json\"\n",
    "```\n",
    "\n",
    "If only using local, keep bucket_name, project_id, and google_cloud_credentials_path as empty strings. Using remote requires a Google Cloud Storage bucket with credentials and a project ID.\n",
    "\n",
    "---\n",
    "\n",
    "### msa_component\n",
    "\n",
    "Generates the multiple sequence alignment for the protein sequence using [Clustal Omega](http://www.clustal.org/omega/). It's recommended to use a smaller number of sequences or none at all due to potential time consumption.\n",
    "\n",
    "---\n",
    "\n",
    "### unikp_component\n",
    "\n",
    "Uses the UniKP endpoint on HuggingFace to predict the kinetic parameters of a protein sequence and substrate (SMILES) combination. See README for the description of the contents of this file.\n",
    "\n",
    "```yaml\n",
    "\"protein_smiles_path\": \"/data/<path_protein_smiles>\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### peptide_component\n",
    "\n",
    "Calculates the features from the protein sequence using the `peptides` package.\n",
    "\n",
    "---\n",
    "\n",
    "### deepTMpred_component\n",
    "\n",
    "Predicts the transmembrane regions of the protein sequence using the [DeepTMpred GitHub repository](https://github.com/ISYSLAB-HUST/DeepTMpred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-07-17 12:10:36,687 | fondant.pipeline.pipeline | WARNING] Component `Biopython component` has an image tag set to latest. Caching for the component will be disabled to prevent unpredictable behavior due to images updates\n",
      "[2024-07-17 12:10:36,691 | fondant.pipeline.pipeline | WARNING] Component `Generate Protein Sequence Checksum Component` has an image tag set to latest. Caching for the component will be disabled to prevent unpredictable behavior due to images updates\n"
     ]
    },
    {
     "ename": "InvalidComponentSpec",
     "evalue": "{'type': 'map_'} is not valid under any of the given schemas\n\nFailed validating 'anyOf' in schema['properties']['produces']['additionalProperties']:\n    {'anyOf': [{'$ref': '#/definitions/field'},\n               {'$ref': '#/definitions/additionalProperties'}]}\n\nOn instance['produces']['unikp_kinetic_prediction']:\n    {'type': 'map_'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidComponentSpec\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./components/biopython_component\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./components/generate_protein_sequence_checksum_component\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;43;03m# ).apply(\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;43;03m#     \"./components/iFeatureOmega_component\",\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;43;03m#     # currently forcing the number of rows to 5, but there needs to be a better way to do this, see readme for more info\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;43;03m#     input_partition_rows=5,\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;43;03m#     arguments={\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;43;03m#         \"descriptors\": [\"AAC\", \"CTDC\", \"CTDT\"]\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;43;03m#     }\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;43;03m# ).apply(\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;43;03m#     \"./components/filter_pdb_component\",\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;43;03m#     arguments={\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;43;03m#         \"method\": \"local\",\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;43;03m#         \"local_pdb_path\": \"/data/pdb_files\",\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;43;03m#         \"bucket_name\": \"\",\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;43;03m#         \"project_id\": \"\",\u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;43;03m#         \"google_cloud_credentials_path\": \"\"\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;43;03m#     }\u001b[39;49;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;43;03m# ).apply(\u001b[39;49;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;43;03m#     \"./components/predict_protein_3D_structure_component\",\u001b[39;49;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;43;03m# ).apply(\u001b[39;49;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;43;03m#     \"./components/store_pdb_component\",\u001b[39;49;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;43;03m#     arguments={\u001b[39;49;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;43;03m#         \"method\": \"local\",\u001b[39;49;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;43;03m#         \"local_pdb_path\": \"/data/pdb_files/\",\u001b[39;49;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;43;03m#         \"bucket_name\": \"elated-chassis-400207_dbtl_pipeline_outputs\",\u001b[39;49;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;43;03m#         \"project_id\": \"elated-chassis-400207\",\u001b[39;49;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;43;03m#         \"google_cloud_credentials_path\": \"/data/google_cloud_credentials.json\"\u001b[39;49;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;43;03m#     }\u001b[39;49;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;43;03m# ).apply(\u001b[39;49;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;43;03m#     \"./components/msa_component\",\u001b[39;49;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;43;03m#     input_partition_rows='10000'\u001b[39;49;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;43;03m# ).apply(\u001b[39;49;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;43;03m#     \"./components/pdb_features_component\"\u001b[39;49;00m\n\u001b[1;32m     37\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./components/unikp_component\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43marguments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtarget_molecule_smiles\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/data/target_molecule_smiles.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./components/peptide_features_component\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# ).apply(\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m#     \"./components/DeepTMpred_component\"\u001b[39;00m\n\u001b[1;32m     46\u001b[0m )\n",
      "File \u001b[0;32m~/Software/deCYPher/protein-feature-extraction/.venv/lib/python3.10/site-packages/fondant/pipeline/pipeline.py:798\u001b[0m, in \u001b[0;36mDataset.apply\u001b[0;34m(self, ref, consumes, produces, arguments, input_partition_rows, resources, cache)\u001b[0m\n\u001b[1;32m    700\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m    701\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    702\u001b[0m     ref: t\u001b[38;5;241m.\u001b[39mAny,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    709\u001b[0m     cache: t\u001b[38;5;241m.\u001b[39mOptional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    710\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    711\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    712\u001b[0m \u001b[38;5;124;03m    Apply the provided component on the dataset.\u001b[39;00m\n\u001b[1;32m    713\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    796\u001b[0m \u001b[38;5;124;03m        An intermediate dataset.\u001b[39;00m\n\u001b[1;32m    797\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 798\u001b[0m     operation \u001b[38;5;241m=\u001b[39m \u001b[43mComponentOp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_ref\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m        \u001b[49m\u001b[43mref\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproduces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproduces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconsumes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconsumes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m        \u001b[49m\u001b[43marguments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marguments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_partition_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_partition_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresources\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(operation)\n",
      "File \u001b[0;32m~/Software/deCYPher/protein-feature-extraction/.venv/lib/python3.10/site-packages/fondant/pipeline/pipeline.py:324\u001b[0m, in \u001b[0;36mComponentOp.from_ref\u001b[0;34m(cls, ref, fields, **kwargs)\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidLightweightComponent(msg)\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ref, (\u001b[38;5;28mstr\u001b[39m, Path)):\n\u001b[0;32m--> 324\u001b[0m     operation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_component_yaml\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m        \u001b[49m\u001b[43mref\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mInvalid reference type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(ref)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;124m        Expected a string, Path, or a lightweight component class.\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n",
      "File \u001b[0;32m~/Software/deCYPher/protein-feature-extraction/.venv/lib/python3.10/site-packages/fondant/pipeline/pipeline.py:191\u001b[0m, in \u001b[0;36mComponentOp.from_component_yaml\u001b[0;34m(cls, path, fields, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    189\u001b[0m     component_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_registry_path(\u001b[38;5;28mstr\u001b[39m(path))\n\u001b[0;32m--> 191\u001b[0m component_spec \u001b[38;5;241m=\u001b[39m \u001b[43mComponentSpec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcomponent_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOMPONENT_SPEC_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m image \u001b[38;5;241m=\u001b[39m Image(\n\u001b[1;32m    196\u001b[0m     base_image\u001b[38;5;241m=\u001b[39mcomponent_spec\u001b[38;5;241m.\u001b[39mimage,\n\u001b[1;32m    197\u001b[0m )\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\n\u001b[1;32m    200\u001b[0m     image\u001b[38;5;241m=\u001b[39mimage,\n\u001b[1;32m    201\u001b[0m     component_spec\u001b[38;5;241m=\u001b[39mcomponent_spec,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    205\u001b[0m )\n",
      "File \u001b[0;32m~/Software/deCYPher/protein-feature-extraction/.venv/lib/python3.10/site-packages/fondant/core/component_spec.py:161\u001b[0m, in \u001b[0;36mComponentSpec.from_file\u001b[0;34m(cls, path)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file_:\n\u001b[1;32m    160\u001b[0m     specification \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39msafe_load(file_)\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspecification\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Software/deCYPher/protein-feature-extraction/.venv/lib/python3.10/site-packages/fondant/core/component_spec.py:172\u001b[0m, in \u001b[0;36mComponentSpec.from_dict\u001b[0;34m(cls, component_spec_dict)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load the component spec from a dictionary.\"\"\"\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcomponent_spec_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    174\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid component spec: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Software/deCYPher/protein-feature-extraction/.venv/lib/python3.10/site-packages/fondant/core/component_spec.py:125\u001b[0m, in \u001b[0;36mComponentSpec.__init__\u001b[0;34m(self, name, image, description, consumes, produces, previous_index, args, tags)\u001b[0m\n\u001b[1;32m    122\u001b[0m     spec_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m args\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_specification \u001b[38;5;241m=\u001b[39m spec_dict\n\u001b[0;32m--> 125\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_spec\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Software/deCYPher/protein-feature-extraction/.venv/lib/python3.10/site-packages/fondant/core/component_spec.py:154\u001b[0m, in \u001b[0;36mComponentSpec._validate_spec\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    152\u001b[0m     validator\u001b[38;5;241m.\u001b[39mvalidate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_specification)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m jsonschema\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidComponentSpec\u001b[38;5;241m.\u001b[39mcreate_from(e)\n",
      "\u001b[0;31mInvalidComponentSpec\u001b[0m: {'type': 'map_'} is not valid under any of the given schemas\n\nFailed validating 'anyOf' in schema['properties']['produces']['additionalProperties']:\n    {'anyOf': [{'$ref': '#/definitions/field'},\n               {'$ref': '#/definitions/additionalProperties'}]}\n\nOn instance['produces']['unikp_kinetic_prediction']:\n    {'type': 'map_'}"
     ]
    }
   ],
   "source": [
    "_ = dataset.apply(\n",
    "    \"./components/biopython_component\"\n",
    ").apply(\n",
    "    \"./components/generate_protein_sequence_checksum_component\"\n",
    ").apply(\n",
    "    \"./components/iFeatureOmega_component\",\n",
    "    # currently forcing the number of rows to 5, but there needs to be a better way to do this, see readme for more info\n",
    "    input_partition_rows=5,\n",
    "    arguments={\n",
    "        \"descriptors\": [\"AAC\", \"CTDC\", \"CTDT\"]\n",
    "    }\n",
    ").apply(\n",
    "    \"./components/filter_pdb_component\",\n",
    "    arguments={\n",
    "        \"method\": \"local\",\n",
    "        \"local_pdb_path\": \"/data/pdb_files\",\n",
    "        \"bucket_name\": \"\",\n",
    "        \"project_id\": \"\",\n",
    "        \"google_cloud_credentials_path\": \"\"\n",
    "    }\n",
    ").apply(\n",
    "    \"./components/predict_protein_3D_structure_component\",\n",
    ").apply(\n",
    "    \"./components/store_pdb_component\",\n",
    "    arguments={\n",
    "        \"method\": \"local\",\n",
    "        \"local_pdb_path\": \"/data/pdb_files/\",\n",
    "        \"bucket_name\": \"elated-chassis-400207_dbtl_pipeline_outputs\",\n",
    "        \"project_id\": \"elated-chassis-400207\",\n",
    "        \"google_cloud_credentials_path\": \"/data/google_cloud_credentials.json\"\n",
    "    }\n",
    ").apply(\n",
    "    \"./components/msa_component\",\n",
    "    input_partition_rows='10000'\n",
    ").apply(\n",
    "    \"./components/pdb_features_component\"\n",
    "# ).apply(\n",
    "#     \"./components/unikp_component\",\n",
    "#     arguments={\n",
    "#         \"target_molecule_smiles\": \"/data/target_molecule_smiles.json\",\n",
    "#     },\n",
    ").apply(\n",
    "    \"./components/peptide_features_component\"\n",
    "# ).apply(\n",
    "#     \"./components/DeepTMpred_component\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pipeline.py` file needs to be run using the command line. The following command will run the pipeline:\n",
    "\n",
    "```bash\n",
    "fondant < full_path_to_pipeline.py >\\data:/data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "\n",
    "# remove the most recent output folder if the manifest file is removed\n",
    "# without a manifest file in the most recent output folder, the pipeline cannot be run\n",
    "# if OUTPUT_FOLDER and REMOVED_MANIFEST:\n",
    "# \tshutil.rmtree(OUTPUT_FOLDER)\n",
    "# \t# remove cache\n",
    "# \tshutil.rmtree(os.path.join(BASE_PATH, PIPELINE_NAME, \"cache\"))\n",
    "\n",
    "# get current full path to the project\n",
    "mounted_data = os.path.join(os.path.abspath(\"data\"), \":/data\")\n",
    "\n",
    "DockerRunner().run(input=pipeline, extra_volumes=mounted_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "The following results have been taken from the output of the pipeline, which is stored in the `.fondant` directory. This directory contains the output of each component, together with the cache of the previous run. Currently, the pipeline doesn't implement the `write_to_file` component, so the results will be taken individually from the output of each component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the most recent output folder\n",
    "# get the most recent folder in the folder named: BASE_PATH + PIPELINE_NAME + PIPELINE_NAME-<timestamp>\n",
    "matching_folders = glob.glob(f\"{BASE_PATH}/{PIPELINE_NAME}/{PIPELINE_NAME}-*\")\n",
    "\n",
    "if matching_folders:\n",
    "    last_folder = max(matching_folders, key=os.path.getctime)\n",
    "\n",
    "logging.info(f\"Last folder: {last_folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def merge_parquet_folders(folder_path):\n",
    "    df_list = []\n",
    "\n",
    "    for folder in Path(folder_path).iterdir():\n",
    "        if folder.is_dir():\n",
    "            logging.info(f\"Reading parquet partitions from: {folder}\")\n",
    "            parquet_files = list(folder.glob(\"*.parquet\"))\n",
    "            logging.info(f\"Found {len(parquet_files)} parquet files\")\n",
    "            dfs = [pd.read_parquet(file) for file in parquet_files]\n",
    "            dfs = [x for x in dfs if not x.empty]\n",
    "            if len(dfs) == 0:\n",
    "                continue\n",
    "            df = pd.concat(dfs)\n",
    "            df_list.append(df)\n",
    "\n",
    "    return df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_list = merge_parquet_folders(last_folder)\n",
    "\n",
    "\n",
    "df_final = pd.concat(dataframe_list, axis=1)\n",
    "df_final = df_final.loc[:,~df_final.columns.duplicated()]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final[[\"sequence\", \"pdb_string\", \"msa_sequence\"]].to_json(\"test_json.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering out columns that are not properly stored in a csv\n",
    "columns_to_remove = ['pdb_string']\n",
    "df_final = df_final.drop(columns=columns_to_remove)\n",
    "\n",
    "# write to file\n",
    "df_final.to_csv(f\"{last_folder}/final_output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "protein-feature-extraction-NoVdeDG9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
