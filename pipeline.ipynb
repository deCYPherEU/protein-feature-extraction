{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protein feature extraction pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will contain the pipeline for extracting features from protein sequences. It will be used as a way to show the output without needing to run the `pipeline.py` file locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import logging\n",
    "from fondant.pipeline import Pipeline\n",
    "from fondant.pipeline.runner import DockerRunner\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "from config import MOCK_DATA_PATH_FONDANT\n",
    "\n",
    "# check if the manifest file is removed.\n",
    "REMOVED_MANIFEST = False\n",
    "\n",
    "# check if the output folder exists\n",
    "OUTPUT_FOLDER = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Mock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python utils/generate_mock_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MNQRGMPIQSLVTNVKINRLEENDCIHTRHRVRPGRTDGKNLHAMM...</td>\n",
       "      <td>Seq1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MAGLKPEVPLHDGINKFGKSDFAGQEGPKIVTTTDKALLVANGALK...</td>\n",
       "      <td>Seq2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MVDLKKELKNFVDSDFPGSPKQEAQGIDVRILLSFNNAAFREALII...</td>\n",
       "      <td>Seq3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MELILAKARLEFECDWGLLMLEPCVPPTKIFADRNYAVGVMFESDK...</td>\n",
       "      <td>Seq4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MRVLCDGSTGYACAKNTRIRFREKVASVLAKIQGYEQTFPHHMPNM...</td>\n",
       "      <td>Seq5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sequence  name\n",
       "0  MNQRGMPIQSLVTNVKINRLEENDCIHTRHRVRPGRTDGKNLHAMM...  Seq1\n",
       "1  MAGLKPEVPLHDGINKFGKSDFAGQEGPKIVTTTDKALLVANGALK...  Seq2\n",
       "2  MVDLKKELKNFVDSDFPGSPKQEAQGIDVRILLSFNNAAFREALII...  Seq3\n",
       "3  MELILAKARLEFECDWGLLMLEPCVPPTKIFADRNYAVGVMFESDK...  Seq4\n",
       "4  MRVLCDGSTGYACAKNTRIRFREKVASVLAKIQGYEQTFPHHMPNM...  Seq5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show content of the mock data\n",
    "import pandas as pd\n",
    "mock_df = pd.read_parquet(\".\" + MOCK_DATA_PATH_FONDANT)  # dot added to make it relative to the current directory\n",
    "mock_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new pipeline\n",
    "\n",
    "BASE_PATH = \".fondant\"\n",
    "PIPELINE_NAME = \"feature_extraction_pipeline\"\n",
    "\n",
    "pipeline = Pipeline(\n",
    "\tname=PIPELINE_NAME,\n",
    "\tbase_path=BASE_PATH,\n",
    "\tdescription=\"A pipeline to extract features from protein sequences.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-24 17:18:20,798 | fondant.pipeline.pipeline | INFO] The consumes section of the component spec is not defined. Can not infer consumes of the OperationSpec. Please define a consumes section in the dataset interface. \n"
     ]
    }
   ],
   "source": [
    "# Read the dataset\n",
    "\n",
    "dataset = pipeline.read(\n",
    "\t\"load_from_parquet\",\n",
    "\targuments={\n",
    "\t\t\"dataset_uri\": MOCK_DATA_PATH_FONDANT,\n",
    "\t},\n",
    "\tproduces={\n",
    "\t\t\"sequence\": pa.string()\n",
    "\t}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components\n",
    "\n",
    "---\n",
    "\n",
    "### generate_protein_sequence_checksum_component\n",
    "\n",
    "This component generates a checksum for the protein sequence.\n",
    "\n",
    "---\n",
    "\n",
    "### biopython_component\n",
    "\n",
    "Extracts features from the protein sequence using Biopython.\n",
    "\n",
    "---\n",
    "\n",
    "### iFeatureOmega_component\n",
    "\n",
    "Extracts features from the protein sequence using the [iFeatureOmega-CLI GitHub repo](https://github.com/Superzchen/iFeatureOmega-CLI). Arguments are used to specify the type of features to extract.\n",
    "\n",
    "---\n",
    "\n",
    "### filter_pdb_component\n",
    "\n",
    "Filters PDB files that are already predicted to avoid redundant predictions. Arguments need to be specified before running the pipeline:\n",
    "```json\n",
    "\"storage_type\": \"local\",\n",
    "\"pdb_path\": \"/data/<your-pdb-folder-path>\",\n",
    "\"bucket_name\": \"your-bucket-name\",\n",
    "\"project_id\": \"your-project-id\",\n",
    "\"google_cloud_credentials_path\": \"/data/<your-credentials>.json\"\n",
    "```\n",
    "\n",
    "If only using local, keep bucket_name, project_id, and google_cloud_credentials_path as empty strings. Using remote requires a Google Cloud Storage bucket with credentials and a project ID.\n",
    "\n",
    "---\n",
    "\n",
    "### predict_protein_3D_structure_component\n",
    "\n",
    "Predicts the 3D structure of the protein using ESMFold. This component requires a `.env` file with the following variables:\n",
    "```env\n",
    "HF_API_KEY=\"\"\n",
    "HF_ENDPOINT_URL=\"\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### store_pdb_component\n",
    "\n",
    "Stores the PDB files in the provided storage_type. Arguments need to be specified before running the pipeline:\n",
    "```json\n",
    "\"storage_type\": \"local\",\n",
    "\"pdb_path\": \"/data/<your-pdb-folder-path>\",\n",
    "\"bucket_name\": \"your-bucket-name\",\n",
    "\"project_id\": \"your-project-id\",\n",
    "\"google_cloud_credentials_path\": \"/data/<your-credentials>.json\"\n",
    "```\n",
    "\n",
    "If only using local, keep bucket_name, project_id, and google_cloud_credentials_path as empty strings. Using remote requires a Google Cloud Storage bucket with credentials and a project ID.\n",
    "\n",
    "---\n",
    "\n",
    "### msa_component\n",
    "\n",
    "Generates the multiple sequence alignment for the protein sequence using [Clustal Omega](http://www.clustal.org/omega/). It's recommended to use a smaller number of sequences or none at all due to potential time consumption.\n",
    "\n",
    "---\n",
    "\n",
    "### unikp_component\n",
    "\n",
    "Uses the UniKP endpoint on HuggingFace to predict the kinetic parameters of a protein sequence and substrate (SMILES) combination. See README for the description of the contents of this file.\n",
    "\n",
    "```yaml\n",
    "\"protein_smiles_path\": \"/data/<path_protein_smiles>\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### peptide_component\n",
    "\n",
    "Calculates the features from the protein sequence using the `peptides` package.\n",
    "\n",
    "---\n",
    "\n",
    "### deepTMpred_component\n",
    "\n",
    "Predicts the transmembrane regions of the protein sequence using the [DeepTMpred GitHub repository](https://github.com/ISYSLAB-HUST/DeepTMpred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-24 17:18:20,816 | fondant.pipeline.pipeline | WARNING] Component `Biopython component` has an image tag set to latest. Caching for the component will be disabled to prevent unpredictable behavior due to images updates\n",
      "[2024-06-24 17:18:20,825 | fondant.pipeline.pipeline | WARNING] Component `Generate Protein Sequence Checksum Component` has an image tag set to latest. Caching for the component will be disabled to prevent unpredictable behavior due to images updates\n",
      "[2024-06-24 17:18:20,858 | fondant.pipeline.pipeline | WARNING] Component `iFeatureOmega component` has an image tag set to latest. Caching for the component will be disabled to prevent unpredictable behavior due to images updates\n",
      "[2024-06-24 17:18:20,866 | fondant.pipeline.pipeline | WARNING] Component `Filter PDB Component` has an image tag set to latest. Caching for the component will be disabled to prevent unpredictable behavior due to images updates\n",
      "[2024-06-24 17:18:20,874 | fondant.pipeline.pipeline | WARNING] Component `Predict Protein 3D Structure Component` has an image tag set to latest. Caching for the component will be disabled to prevent unpredictable behavior due to images updates\n",
      "[2024-06-24 17:18:20,884 | fondant.pipeline.pipeline | WARNING] Component `Store PDB Component` has an image tag set to latest. Caching for the component will be disabled to prevent unpredictable behavior due to images updates\n",
      "[2024-06-24 17:18:20,892 | fondant.pipeline.pipeline | WARNING] Component `MSA component` has an image tag set to latest. Caching for the component will be disabled to prevent unpredictable behavior due to images updates\n",
      "[2024-06-24 17:18:20,901 | fondant.pipeline.pipeline | WARNING] Component `Peptide Features component` has an image tag set to latest. Caching for the component will be disabled to prevent unpredictable behavior due to images updates\n"
     ]
    }
   ],
   "source": [
    "_ = dataset.apply(\n",
    "    \"./components/biopython_component\"\n",
    ").apply(\n",
    "    \"./components/generate_protein_sequence_checksum_component\"\n",
    ").apply(\n",
    "    \"./components/iFeatureOmega_component\",\n",
    "    # currently forcing the number of rows to 5, but there needs to be a better way to do this, see readme for more info\n",
    "    input_partition_rows=5,\n",
    "    arguments={\n",
    "        \"descriptors\": [\"AAC\", \"CTDC\", \"CTDT\"]\n",
    "    }\n",
    ").apply(\n",
    "    \"./components/filter_pdb_component\",\n",
    "    arguments={\n",
    "        \"method\": \"local\",\n",
    "        \"local_pdb_path\": \"/data/pdb_files\",\n",
    "        \"bucket_name\": \"\",\n",
    "        \"project_id\": \"\",\n",
    "        \"google_cloud_credentials_path\": \"\"\n",
    "    }\n",
    ").apply(\n",
    "    \"./components/predict_protein_3D_structure_component\",\n",
    ").apply(\n",
    "    \"./components/store_pdb_component\",\n",
    "    arguments={\n",
    "        \"method\": \"local\",\n",
    "        \"local_pdb_path\": \"/data/pdb_files/\",\n",
    "        \"bucket_name\": \"elated-chassis-400207_dbtl_pipeline_outputs\",\n",
    "        \"project_id\": \"elated-chassis-400207\",\n",
    "        \"google_cloud_credentials_path\": \"/data/google_cloud_credentials.json\"\n",
    "    }\n",
    ").apply(\n",
    "    \"./components/msa_component\",\n",
    "    input_partition_rows='10000'\n",
    "# ).apply(\n",
    "#     \"./components/pdb_features_component\"\n",
    "# ).apply(\n",
    "#     \"./components/unikp_component\",\n",
    "#     arguments={\n",
    "#         \"protein_smiles_path\": \"/data/protein_smiles.json\",\n",
    "#     },\n",
    ").apply(\n",
    "    \"./components/peptide_features_component\"\n",
    "# ).apply(\n",
    "#     \"./components/DeepTMpred_component\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pipeline.py` file needs to be run using the command line. The following command will run the pipeline:\n",
    "\n",
    "```bash\n",
    "fondant < full_path_to_pipeline.py >\\data:/data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-24 17:18:20,966 | root | INFO] Found reference to un-compiled pipeline... compiling\n",
      "[2024-06-24 17:18:20,966 | fondant.pipeline.compiler | INFO] Compiling feature_extraction_pipeline to .fondant/compose.yaml\n",
      "[2024-06-24 17:18:20,967 | fondant.pipeline.compiler | INFO] Base path found on local system, setting up .fondant as mount volume\n",
      "[2024-06-24 17:18:20,967 | fondant.pipeline.pipeline | INFO] Sorting pipeline component graph topologically.\n",
      "[2024-06-24 17:18:20,990 | fondant.pipeline.pipeline | INFO] All pipeline component specifications match.\n",
      "[2024-06-24 17:18:20,991 | fondant.pipeline.compiler | INFO] Compiling service for load_from_parquet\n",
      "[2024-06-24 17:18:20,991 | fondant.pipeline.compiler | INFO] Compiling service for biopython_component\n",
      "[2024-06-24 17:18:20,991 | fondant.pipeline.compiler | INFO] Found Dockerfile for biopython_component, adding build step.\n",
      "[2024-06-24 17:18:20,992 | fondant.pipeline.compiler | INFO] Compiling service for generate_protein_sequence_checksum_component\n",
      "[2024-06-24 17:18:20,992 | fondant.pipeline.compiler | INFO] Found Dockerfile for generate_protein_sequence_checksum_component, adding build step.\n",
      "[2024-06-24 17:18:20,993 | fondant.pipeline.compiler | INFO] Compiling service for ifeatureomega_component\n",
      "[2024-06-24 17:18:20,993 | fondant.pipeline.compiler | INFO] Found Dockerfile for ifeatureomega_component, adding build step.\n",
      "[2024-06-24 17:18:20,994 | fondant.pipeline.compiler | INFO] Compiling service for filter_pdb_component\n",
      "[2024-06-24 17:18:20,994 | fondant.pipeline.compiler | INFO] Found Dockerfile for filter_pdb_component, adding build step.\n",
      "[2024-06-24 17:18:20,995 | fondant.pipeline.compiler | INFO] Compiling service for predict_protein_3d_structure_component\n",
      "[2024-06-24 17:18:20,995 | fondant.pipeline.compiler | INFO] Found Dockerfile for predict_protein_3d_structure_component, adding build step.\n",
      "[2024-06-24 17:18:20,995 | fondant.pipeline.compiler | INFO] Compiling service for store_pdb_component\n",
      "[2024-06-24 17:18:20,996 | fondant.pipeline.compiler | INFO] Found Dockerfile for store_pdb_component, adding build step.\n",
      "[2024-06-24 17:18:20,996 | fondant.pipeline.compiler | INFO] Compiling service for msa_component\n",
      "[2024-06-24 17:18:20,997 | fondant.pipeline.compiler | INFO] Found Dockerfile for msa_component, adding build step.\n",
      "[2024-06-24 17:18:20,997 | fondant.pipeline.compiler | INFO] Compiling service for peptide_features_component\n",
      "[2024-06-24 17:18:20,998 | fondant.pipeline.compiler | INFO] Found Dockerfile for peptide_features_component, adding build step.\n",
      "[2024-06-24 17:18:21,017 | fondant.pipeline.compiler | INFO] Successfully compiled to .fondant/compose.yaml\n",
      "time=\"2024-06-24T17:18:21+02:00\" level=warning msg=\"/home/pietercoussement/Software/Sandbox/protein-feature-extraction/.fondant/compose.yaml: `version` is obsolete\"\n",
      " load_from_parquet Pulling \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pipeline run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " load_from_parquet Pulled \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0 building with \"desktop-linux\" instance using docker driver\n",
      "\n",
      "#1 [biopython_component internal] load build definition from Dockerfile\n",
      "#1 transferring dockerfile: 480B done\n",
      "#1 DONE 0.0s\n",
      "\n",
      "#2 [biopython_component internal] load metadata for docker.io/fndnt/fondant:0.11.dev5-py3.10\n",
      "#2 DONE 0.0s\n",
      "\n",
      "#3 [biopython_component internal] load .dockerignore\n",
      "#3 transferring context: 2B done\n",
      "#3 DONE 0.0s\n",
      "\n",
      "#4 [biopython_component 1/6] FROM docker.io/fndnt/fondant:0.11.dev5-py3.10\n",
      "#4 DONE 0.0s\n",
      "\n",
      "#5 [biopython_component internal] load build context\n",
      "#5 transferring context: 3.25kB done\n",
      "#5 DONE 0.0s\n",
      "\n",
      "#6 [biopython_component 3/6] COPY requirements.txt ./\n",
      "#6 CACHED\n",
      "\n",
      "#7 [biopython_component 2/6] RUN apt-get update &&     apt-get upgrade -y &&     apt-get install git -y\n",
      "#7 CACHED\n",
      "\n",
      "#8 [biopython_component 4/6] RUN pip3 install --no-cache-dir -r requirements.txt\n",
      "#8 CACHED\n",
      "\n",
      "#9 [biopython_component 5/6] WORKDIR /component/src\n",
      "#9 CACHED\n",
      "\n",
      "#10 [biopython_component 6/6] COPY src/ .\n",
      "#10 CACHED\n",
      "\n",
      "#11 [biopython_component] exporting to image\n",
      "#11 exporting layers done\n",
      "#11 writing image sha256:2c9c5b8de3bcb0e6b229292528a316cbcdecab60c38145e45ffba2465d53822a done\n",
      "#11 naming to docker.io/library/feature_extraction_pipeline-biopython_component done\n",
      "#11 DONE 0.0s\n",
      "\n",
      "#12 [generate_protein_sequence_checksum_component internal] load build definition from Dockerfile\n",
      "#12 transferring dockerfile: 480B done\n",
      "#12 DONE 0.0s\n",
      "\n",
      "#2 [generate_protein_sequence_checksum_component internal] load metadata for docker.io/fndnt/fondant:0.11.dev5-py3.10\n",
      "#2 DONE 0.0s\n",
      "\n",
      "#13 [generate_protein_sequence_checksum_component internal] load .dockerignore\n",
      "#13 transferring context: 2B done\n",
      "#13 DONE 0.0s\n",
      "\n",
      "#4 [generate_protein_sequence_checksum_component 1/6] FROM docker.io/fndnt/fondant:0.11.dev5-py3.10\n",
      "#4 DONE 0.0s\n",
      "\n",
      "#14 [generate_protein_sequence_checksum_component internal] load build context\n",
      "#14 transferring context: 1.17kB done\n",
      "#14 DONE 0.0s\n",
      "\n",
      "#7 [generate_protein_sequence_checksum_component 2/6] RUN apt-get update &&     apt-get upgrade -y &&     apt-get install git -y\n",
      "#7 CACHED\n",
      "\n",
      "#15 [generate_protein_sequence_checksum_component 3/6] COPY requirements.txt ./\n",
      "#15 CACHED\n",
      "\n",
      "#16 [generate_protein_sequence_checksum_component 4/6] RUN pip3 install --no-cache-dir -r requirements.txt\n",
      "#16 CACHED\n",
      "\n",
      "#17 [generate_protein_sequence_checksum_component 5/6] WORKDIR /component/src\n",
      "#17 CACHED\n",
      "\n",
      "#18 [generate_protein_sequence_checksum_component 6/6] COPY src/ .\n",
      "#18 CACHED\n",
      "\n",
      "#19 [generate_protein_sequence_checksum_component] exporting to image\n",
      "#19 exporting layers done\n",
      "#19 writing image sha256:56049561c81c2bee7af20822240fce2d0f54b84851a116476e54bfc4447cf872 done\n",
      "#19 naming to docker.io/library/feature_extraction_pipeline-generate_protein_sequence_checksum_component done\n",
      "#19 DONE 0.0s\n",
      "\n",
      "#20 [ifeatureomega_component internal] load build definition from Dockerfile\n",
      "#20 transferring dockerfile: 668B done\n",
      "#20 DONE 0.0s\n",
      "\n",
      "#2 [ifeatureomega_component internal] load metadata for docker.io/fndnt/fondant:0.11.dev5-py3.10\n",
      "#2 DONE 0.0s\n",
      "\n",
      "#21 [ifeatureomega_component internal] load .dockerignore\n",
      "#21 transferring context: 2B done\n",
      "#21 DONE 0.0s\n",
      "\n",
      "#4 [ifeatureomega_component 1/8] FROM docker.io/fndnt/fondant:0.11.dev5-py3.10\n",
      "#4 DONE 0.0s\n",
      "\n",
      "#22 [ifeatureomega_component internal] load build context\n",
      "#22 transferring context: 3.04kB done\n",
      "#22 DONE 0.0s\n",
      "\n",
      "#23 [ifeatureomega_component 5/8] WORKDIR /component/src\n",
      "#23 CACHED\n",
      "\n",
      "#7 [ifeatureomega_component 2/8] RUN apt-get update &&     apt-get upgrade -y &&     apt-get install git -y\n",
      "#7 CACHED\n",
      "\n",
      "#24 [ifeatureomega_component 6/8] COPY src/ .\n",
      "#24 CACHED\n",
      "\n",
      "#25 [ifeatureomega_component 7/8] RUN git clone https://github.com/Superzchen/iFeatureOmega-CLI\n",
      "#25 CACHED\n",
      "\n",
      "#26 [ifeatureomega_component 4/8] RUN pip3 install --no-cache-dir -r requirements.txt\n",
      "#26 CACHED\n",
      "\n",
      "#27 [ifeatureomega_component 3/8] COPY requirements.txt ./\n",
      "#27 CACHED\n",
      "\n",
      "#28 [ifeatureomega_component 8/8] RUN mv iFeatureOmega-CLI iFeatureOmega_CLI\n",
      "#28 CACHED\n",
      "\n",
      "#29 [ifeatureomega_component] exporting to image\n",
      "#29 exporting layers done\n",
      "#29 writing image sha256:14f2b3861d114a259876e2a263a0e7457c63200f78e84046fc8a42ab11b4d2b2 done\n",
      "#29 naming to docker.io/library/feature_extraction_pipeline-ifeatureomega_component done\n",
      "#29 DONE 0.0s\n",
      "\n",
      "#30 [filter_pdb_component internal] load build definition from Dockerfile\n",
      "#30 transferring dockerfile: 480B done\n",
      "#30 DONE 0.0s\n",
      "\n",
      "#2 [filter_pdb_component internal] load metadata for docker.io/fndnt/fondant:0.11.dev5-py3.10\n",
      "#2 DONE 0.0s\n",
      "\n",
      "#31 [filter_pdb_component internal] load .dockerignore\n",
      "#31 transferring context: 2B done\n",
      "#31 DONE 0.0s\n",
      "\n",
      "#4 [filter_pdb_component 1/6] FROM docker.io/fndnt/fondant:0.11.dev5-py3.10\n",
      "#4 DONE 0.0s\n",
      "\n",
      "#32 [filter_pdb_component internal] load build context\n",
      "#32 transferring context: 3.86kB done\n",
      "#32 DONE 0.0s\n",
      "\n",
      "#7 [filter_pdb_component 2/6] RUN apt-get update &&     apt-get upgrade -y &&     apt-get install git -y\n",
      "#7 CACHED\n",
      "\n",
      "#33 [filter_pdb_component 5/6] WORKDIR /component/src\n",
      "#33 CACHED\n",
      "\n",
      "#34 [filter_pdb_component 4/6] RUN pip3 install --no-cache-dir -r requirements.txt\n",
      "#34 CACHED\n",
      "\n",
      "#35 [filter_pdb_component 3/6] COPY requirements.txt ./\n",
      "#35 CACHED\n",
      "\n",
      "#36 [filter_pdb_component 6/6] COPY src/ .\n",
      "#36 CACHED\n",
      "\n",
      "#37 [filter_pdb_component] exporting to image\n",
      "#37 exporting layers done\n",
      "#37 writing image sha256:594f44baae2dc12b4e7fd485952defd5db83632284723ad4d0b67d98f4fcc9cb done\n",
      "#37 naming to docker.io/library/feature_extraction_pipeline-filter_pdb_component done\n",
      "#37 DONE 0.0s\n",
      "\n",
      "#38 [predict_protein_3d_structure_component internal] load build definition from Dockerfile\n",
      "#38 transferring dockerfile: 539B done\n",
      "#38 DONE 0.0s\n",
      "\n",
      "#2 [predict_protein_3d_structure_component internal] load metadata for docker.io/fndnt/fondant:0.11.dev5-py3.10\n",
      "#2 DONE 0.0s\n",
      "\n",
      "#39 [predict_protein_3d_structure_component internal] load .dockerignore\n",
      "#39 transferring context: 2B done\n",
      "#39 DONE 0.0s\n",
      "\n",
      "#4 [predict_protein_3d_structure_component 1/7] FROM docker.io/fndnt/fondant:0.11.dev5-py3.10\n",
      "#4 DONE 0.0s\n",
      "\n",
      "#40 [predict_protein_3d_structure_component internal] load build context\n",
      "#40 transferring context: 2.62kB done\n",
      "#40 DONE 0.0s\n",
      "\n",
      "#41 [predict_protein_3d_structure_component 3/7] COPY requirements.txt ./\n",
      "#41 CACHED\n",
      "\n",
      "#42 [predict_protein_3d_structure_component 4/7] RUN pip3 install --no-cache-dir -r requirements.txt\n",
      "#42 CACHED\n",
      "\n",
      "#43 [predict_protein_3d_structure_component 6/7] COPY .env .\n",
      "#43 CACHED\n",
      "\n",
      "#7 [predict_protein_3d_structure_component 2/7] RUN apt-get update &&     apt-get upgrade -y &&     apt-get install git -y\n",
      "#7 CACHED\n",
      "\n",
      "#44 [predict_protein_3d_structure_component 5/7] WORKDIR /component/src\n",
      "#44 CACHED\n",
      "\n",
      "#45 [predict_protein_3d_structure_component 7/7] COPY src/ .\n",
      "#45 CACHED\n",
      "\n",
      "#46 [predict_protein_3d_structure_component] exporting to image\n",
      "#46 exporting layers done\n",
      "#46 writing image sha256:7855378ca5a2630309135a50c46c5389ccab710be62a83bfc955b0c0a7661ae2 0.0s done\n",
      "#46 naming to docker.io/library/feature_extraction_pipeline-predict_protein_3d_structure_component\n",
      "#46 naming to docker.io/library/feature_extraction_pipeline-predict_protein_3d_structure_component done\n",
      "#46 DONE 0.0s\n",
      "\n",
      "#47 [store_pdb_component internal] load build definition from Dockerfile\n",
      "#47 transferring dockerfile: 480B done\n",
      "#47 DONE 0.0s\n",
      "\n",
      "#2 [store_pdb_component internal] load metadata for docker.io/fndnt/fondant:0.11.dev5-py3.10\n",
      "#2 DONE 0.0s\n",
      "\n",
      "#48 [store_pdb_component internal] load .dockerignore\n",
      "#48 transferring context: 2B done\n",
      "#48 DONE 0.0s\n",
      "\n",
      "#4 [store_pdb_component 1/6] FROM docker.io/fndnt/fondant:0.11.dev5-py3.10\n",
      "#4 DONE 0.0s\n",
      "\n",
      "#49 [store_pdb_component internal] load build context\n",
      "#49 transferring context: 3.16kB done\n",
      "#49 DONE 0.0s\n",
      "\n",
      "#50 [store_pdb_component 5/6] WORKDIR /component/src\n",
      "#50 CACHED\n",
      "\n",
      "#7 [store_pdb_component 2/6] RUN apt-get update &&     apt-get upgrade -y &&     apt-get install git -y\n",
      "#7 CACHED\n",
      "\n",
      "#51 [store_pdb_component 4/6] RUN pip3 install --no-cache-dir -r requirements.txt\n",
      "#51 CACHED\n",
      "\n",
      "#52 [store_pdb_component 3/6] COPY requirements.txt ./\n",
      "#52 CACHED\n",
      "\n",
      "#53 [store_pdb_component 6/6] COPY src/ .\n",
      "#53 CACHED\n",
      "\n",
      "#54 [store_pdb_component] exporting to image\n",
      "#54 exporting layers done\n",
      "#54 writing image sha256:e8aacba11440121ed5266c9ee0f96e778a186e8c0baf1f3d2f6649e826b7b1a5 done\n",
      "#54 naming to docker.io/library/feature_extraction_pipeline-store_pdb_component done\n",
      "#54 DONE 0.0s\n",
      "\n",
      "#55 [msa_component internal] load build definition from Dockerfile\n",
      "#55 transferring dockerfile: 700B done\n",
      "#55 DONE 0.0s\n",
      "\n",
      "#2 [msa_component internal] load metadata for docker.io/fndnt/fondant:0.11.dev5-py3.10\n",
      "#2 DONE 0.0s\n",
      "\n",
      "#56 [msa_component internal] load .dockerignore\n",
      "#56 transferring context: 2B done\n",
      "#56 DONE 0.0s\n",
      "\n",
      "#4 [msa_component 1/7] FROM docker.io/fndnt/fondant:0.11.dev5-py3.10\n",
      "#4 DONE 0.0s\n",
      "\n",
      "#57 [msa_component internal] load build context\n",
      "#57 transferring context: 4.33kB done\n",
      "#57 DONE 0.0s\n",
      "\n",
      "#58 [msa_component 3/7] COPY requirements.txt ./\n",
      "#58 CACHED\n",
      "\n",
      "#59 [msa_component 5/7] WORKDIR /component/src\n",
      "#59 CACHED\n",
      "\n",
      "#60 [msa_component 4/7] RUN pip3 install --no-cache-dir -r requirements.txt\n",
      "#60 CACHED\n",
      "\n",
      "#61 [msa_component 6/7] RUN wget http://www.clustal.org/omega/clustalo-1.2.4-Ubuntu-x86_64 &&     mv clustalo-1.2.4-Ubuntu-x86_64 clustalo && \tchmod +x clustalo && \tmv clustalo /usr/local/bin/\n",
      "#61 CACHED\n",
      "\n",
      "#62 [msa_component 2/7] RUN apt-get update &&     apt-get upgrade -y &&     apt-get install git wget -y\n",
      "#62 CACHED\n",
      "\n",
      "#63 [msa_component 7/7] COPY src/ .\n",
      "#63 CACHED\n",
      "\n",
      "#64 [msa_component] exporting to image\n",
      "#64 exporting layers done\n",
      "#64 writing image sha256:bf964f1bf557bd37afb6aee018b57aefa6436e1df8565088bd09e4bfc7e2405d done\n",
      "#64 naming to docker.io/library/feature_extraction_pipeline-msa_component done\n",
      "#64 DONE 0.0s\n",
      "\n",
      "#65 [peptide_features_component internal] load build definition from Dockerfile\n",
      "#65 transferring dockerfile: 480B done\n",
      "#65 DONE 0.1s\n",
      "\n",
      "#2 [peptide_features_component internal] load metadata for docker.io/fndnt/fondant:0.11.dev5-py3.10\n",
      "#2 DONE 0.0s\n",
      "\n",
      "#66 [peptide_features_component internal] load .dockerignore\n",
      "#66 transferring context: 2B done\n",
      "#66 DONE 0.0s\n",
      "\n",
      "#4 [peptide_features_component 1/6] FROM docker.io/fndnt/fondant:0.11.dev5-py3.10\n",
      "#4 DONE 0.0s\n",
      "\n",
      "#67 [peptide_features_component internal] load build context\n",
      "#67 transferring context: 2.35kB done\n",
      "#67 DONE 0.0s\n",
      "\n",
      "#7 [peptide_features_component 2/6] RUN apt-get update &&     apt-get upgrade -y &&     apt-get install git -y\n",
      "#7 CACHED\n",
      "\n",
      "#68 [peptide_features_component 5/6] WORKDIR /component/src\n",
      "#68 CACHED\n",
      "\n",
      "#69 [peptide_features_component 3/6] COPY requirements.txt ./\n",
      "#69 CACHED\n",
      "\n",
      "#70 [peptide_features_component 4/6] RUN pip3 install --no-cache-dir -r requirements.txt\n",
      "#70 CACHED\n",
      "\n",
      "#71 [peptide_features_component 6/6] COPY src/ .\n",
      "#71 CACHED\n",
      "\n",
      "#72 [peptide_features_component] exporting to image\n",
      "#72 exporting layers done\n",
      "#72 writing image sha256:50afff57e09aa3d6457da0f12c13d81dc033b29f3ad3449c74110e0b5cf6a46b done\n",
      "#72 naming to docker.io/library/feature_extraction_pipeline-peptide_features_component done\n",
      "#72 DONE 0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Container feature_extraction_pipeline-deeptm_prediction_component-1  Stopping\n",
      " Container feature_extraction_pipeline-deeptm_prediction_component-1  Stopped\n",
      " Container feature_extraction_pipeline-deeptm_prediction_component-1  Removing\n",
      " Container feature_extraction_pipeline-deeptm_prediction_component-1  Removed\n",
      " Container feature_extraction_pipeline-load_from_parquet-1  Recreate\n",
      " Container feature_extraction_pipeline-load_from_parquet-1  Recreated\n",
      " Container feature_extraction_pipeline-biopython_component-1  Recreate\n",
      " Container feature_extraction_pipeline-biopython_component-1  Recreated\n",
      " Container feature_extraction_pipeline-generate_protein_sequence_checksum_component-1  Recreate\n",
      " Container feature_extraction_pipeline-generate_protein_sequence_checksum_component-1  Recreated\n",
      " Container feature_extraction_pipeline-ifeatureomega_component-1  Recreate\n",
      " Container feature_extraction_pipeline-ifeatureomega_component-1  Recreated\n",
      " Container feature_extraction_pipeline-filter_pdb_component-1  Recreate\n",
      " Container feature_extraction_pipeline-filter_pdb_component-1  Recreated\n",
      " Container feature_extraction_pipeline-predict_protein_3d_structure_component-1  Recreate\n",
      " Container feature_extraction_pipeline-predict_protein_3d_structure_component-1  Recreated\n",
      " Container feature_extraction_pipeline-store_pdb_component-1  Recreate\n",
      " Container feature_extraction_pipeline-store_pdb_component-1  Recreated\n",
      " Container feature_extraction_pipeline-msa_component-1  Recreate\n",
      " Container feature_extraction_pipeline-msa_component-1  Recreated\n",
      " Container feature_extraction_pipeline-peptide_features_component-1  Recreate\n",
      " Container feature_extraction_pipeline-peptide_features_component-1  Recreated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attaching to biopython_component-1, filter_pdb_component-1, generate_protein_sequence_checksum_component-1, ifeatureomega_component-1, load_from_parquet-1, msa_component-1, peptide_features_component-1, predict_protein_3d_structure_component-1, store_pdb_component-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load_from_parquet-1                             | [2024-06-24 15:18:27,915 | fondant.cli | INFO] Component `LoadFromParquet` found in module main\n",
      "load_from_parquet-1                             | [2024-06-24 15:18:27,919 | fondant.component.executor | INFO] Skipping component execution\n",
      "load_from_parquet-1                             | [2024-06-24 15:18:27,920 | fondant.component.executor | INFO] Matching execution detected for component. The last execution of the component originated from `feature_extraction_pipeline-20240613115444`.\n",
      "load_from_parquet-1                             | [2024-06-24 15:18:27,921 | fondant.component.executor | INFO] Saving output manifest to /.fondant/feature_extraction_pipeline/feature_extraction_pipeline-20240624171820/load_from_parquet/manifest.json\n",
      "load_from_parquet-1                             | [2024-06-24 15:18:27,921 | fondant.component.executor | INFO] Writing cache key with manifest reference to /.fondant/feature_extraction_pipeline/cache/d41a53a1be34f7fd0a29002364c9f666.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[Kload_from_parquet-1 exited with code 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "biopython_component-1                           | [2024-06-24 15:18:29,687 | fondant.cli | INFO] Component `BiopythonComponent` found in module main\n",
      "biopython_component-1                           | [2024-06-24 15:18:29,690 | fondant.component.executor | INFO] Caching disabled for the component\n",
      "biopython_component-1                           | [2024-06-24 15:18:29,690 | root | INFO] Executing component\n",
      "biopython_component-1                           | [2024-06-24 15:18:29,967 | distributed.http.proxy | INFO] To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy\n",
      "biopython_component-1                           | [2024-06-24 15:18:29,986 | distributed.scheduler | INFO] State start\n",
      "biopython_component-1                           | [2024-06-24 15:18:29,991 | distributed.scheduler | INFO]   Scheduler at:     tcp://127.0.0.1:43155\n",
      "biopython_component-1                           | [2024-06-24 15:18:29,992 | distributed.scheduler | INFO]   dashboard at:  http://127.0.0.1:8787/status\n",
      "biopython_component-1                           | [2024-06-24 15:18:29,992 | distributed.scheduler | INFO] Registering Worker plugin shuffle\n",
      "biopython_component-1                           | [2024-06-24 15:18:30,011 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:35357'\n",
      "biopython_component-1                           | [2024-06-24 15:18:30,017 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:41531'\n",
      "biopython_component-1                           | [2024-06-24 15:18:30,019 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:32985'\n",
      "biopython_component-1                           | [2024-06-24 15:18:30,020 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:37097'\n",
      "biopython_component-1                           | [2024-06-24 15:18:30,034 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:46837'\n",
      "biopython_component-1                           | [2024-06-24 15:18:30,036 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:34129'\n",
      "biopython_component-1                           | [2024-06-24 15:18:30,038 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:34145'\n",
      "biopython_component-1                           | [2024-06-24 15:18:30,046 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:40555'\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,289 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:35571\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,289 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:35571\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,289 | distributed.worker | INFO]           Worker name:                          0\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,289 | distributed.worker | INFO]          dashboard at:            127.0.0.1:41843\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,289 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:43155\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,289 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,289 | distributed.worker | INFO]               Threads:                          1\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,289 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,289 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-k4z6py14\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,289 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,294 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:35571', name: 0, status: init, memory: 0, processing: 0>\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,297 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:35571\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,298 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:38148\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,301 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,301 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:43155\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,301 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,302 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:43155\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,306 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:46397\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,306 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:46397\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,306 | distributed.worker | INFO]           Worker name:                          7\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,306 | distributed.worker | INFO]          dashboard at:            127.0.0.1:40095\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,306 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:43155\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,306 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,307 | distributed.worker | INFO]               Threads:                          1\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,307 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,308 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-7pu0del5\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,308 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,314 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:46397', name: 7, status: init, memory: 0, processing: 0>\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,315 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:46397\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,315 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:38152\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,315 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,316 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:43155\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,316 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,318 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:43155\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,320 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:43677\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,321 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:43677\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,321 | distributed.worker | INFO]           Worker name:                          5\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,321 | distributed.worker | INFO]          dashboard at:            127.0.0.1:33185\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,321 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:43155\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,321 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,321 | distributed.worker | INFO]               Threads:                          1\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,321 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,321 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-aub2l939\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,322 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,325 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:43677', name: 5, status: init, memory: 0, processing: 0>\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,326 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:43677\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,326 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:38168\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,326 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,327 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:43155\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,327 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,327 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:43155\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,389 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:43773\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,389 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:43773\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,389 | distributed.worker | INFO]           Worker name:                          4\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,389 | distributed.worker | INFO]          dashboard at:            127.0.0.1:39739\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,389 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:43155\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,390 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,390 | distributed.worker | INFO]               Threads:                          1\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,390 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,390 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-77bg7hn1\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,390 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,394 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:43773', name: 4, status: init, memory: 0, processing: 0>\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,395 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:43773\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,395 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:38184\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,395 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,396 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:43155\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,396 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,397 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:43155\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,405 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:40895\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,405 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:40895\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,405 | distributed.worker | INFO]           Worker name:                          6\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,405 | distributed.worker | INFO]          dashboard at:            127.0.0.1:43535\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,405 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:43155\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,405 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,405 | distributed.worker | INFO]               Threads:                          1\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,405 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,405 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-uuf36vsy\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,405 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,408 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:40895', name: 6, status: init, memory: 0, processing: 0>\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,409 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:40895\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,409 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:38188\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,409 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,409 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:43155\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,409 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,410 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:43155\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,411 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:33493\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,411 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:33493\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,411 | distributed.worker | INFO]           Worker name:                          3\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,411 | distributed.worker | INFO]          dashboard at:            127.0.0.1:36379\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,411 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:43155\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,411 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,411 | distributed.worker | INFO]               Threads:                          1\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,411 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,411 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-ztbzr98g\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,411 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,416 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:33493', name: 3, status: init, memory: 0, processing: 0>\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,416 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:33493\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,416 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:38202\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,417 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,417 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:43155\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,418 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,418 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:43155\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,436 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:33533\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,436 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:33533\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,436 | distributed.worker | INFO]           Worker name:                          1\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,436 | distributed.worker | INFO]          dashboard at:            127.0.0.1:42255\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,436 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:43155\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,436 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,436 | distributed.worker | INFO]               Threads:                          1\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,436 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,436 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-l_6zljho\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,436 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,439 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:33533', name: 1, status: init, memory: 0, processing: 0>\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,439 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:33533\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,439 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:38214\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,440 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,440 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:43155\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,440 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,441 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:43155\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,453 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:43269\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,453 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:43269\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,453 | distributed.worker | INFO]           Worker name:                          2\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,453 | distributed.worker | INFO]          dashboard at:            127.0.0.1:44269\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,453 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:43155\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,453 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,453 | distributed.worker | INFO]               Threads:                          1\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,453 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,453 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-eapik_57\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,453 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,456 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:43269', name: 2, status: init, memory: 0, processing: 0>\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,457 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:43269\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,457 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:38216\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,457 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,457 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:43155\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,457 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,458 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:43155\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,512 | distributed.scheduler | INFO] Receive client connection: Client-041af509-323d-11ef-8001-0242ac120002\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,513 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:38218\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,581 | fondant.component.data_io | INFO] The number of partitions of the input dataframe is 1. The available number of workers is 8.\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,582 | fondant.component.data_io | INFO] Repartitioning the data to 8 partitions before processing to maximize worker usage\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,583 | root | INFO] Columns of dataframe: ['sequence']\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,616 | root | INFO] Creating write task for: /.fondant/feature_extraction_pipeline/feature_extraction_pipeline-20240624171820/biopython_component\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,616 | root | INFO] Writing data...\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,925 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:35357'. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,925 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,926 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:41531'. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,926 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,927 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:32985'. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,927 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:35571. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,927 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,928 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:37097'. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,928 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,929 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:33533. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,928 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:46837'. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,930 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,930 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:34129'. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,931 | distributed.core | INFO] Connection to tcp://127.0.0.1:43155 has been closed.\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,932 | distributed.core | INFO] Connection to tcp://127.0.0.1:43155 has been closed.\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,932 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,933 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:34145'. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,933 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:43773. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,933 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:33493. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,934 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,934 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:43677. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,934 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:40555'. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,937 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:40895. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,939 | distributed.core | INFO] Connection to tcp://127.0.0.1:43155 has been closed.\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,940 | distributed.core | INFO] Connection to tcp://127.0.0.1:43155 has been closed.\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,942 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:43269. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,944 | distributed.core | INFO] Connection to tcp://127.0.0.1:43155 has been closed.\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,945 | distributed.core | INFO] Connection to tcp://127.0.0.1:43155 has been closed.\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,949 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,950 | distributed.core | INFO] Connection to tcp://127.0.0.1:43155 has been closed.\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,953 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:38148; closing.\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,954 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:38214; closing.\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,954 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:38202; closing.\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,955 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:38184; closing.\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,955 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:38168; closing.\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,955 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:38216; closing.\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,956 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:38188; closing.\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,957 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:46397. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,957 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:35571', name: 0, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1719242311.9576814')\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,958 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:33533', name: 1, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1719242311.9587166')\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,959 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:33493', name: 3, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1719242311.9595082')\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,960 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:43773', name: 4, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1719242311.9602273')\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,961 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:43677', name: 5, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1719242311.9609435')\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,961 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:43269', name: 2, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1719242311.961712')\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,962 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:40895', name: 6, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1719242311.9625373')\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,969 | distributed.core | INFO] Connection to tcp://127.0.0.1:43155 has been closed.\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,972 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:38152; closing.\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,973 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:46397', name: 7, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1719242311.9732075')\n",
      "biopython_component-1                           | [2024-06-24 15:18:31,973 | distributed.scheduler | INFO] Lost all workers\n",
      "biopython_component-1                           | [2024-06-24 15:18:32,360 | distributed.scheduler | INFO] Scheduler closing due to unknown reason...\n",
      "biopython_component-1                           | [2024-06-24 15:18:32,361 | distributed.scheduler | INFO] Scheduler closing all comms\n",
      "biopython_component-1                           | [2024-06-24 15:18:32,364 | fondant.component.executor | INFO] Saving output manifest to /.fondant/feature_extraction_pipeline/feature_extraction_pipeline-20240624171820/biopython_component/manifest.json\n",
      "biopython_component-1                           | [2024-06-24 15:18:32,364 | fondant.component.executor | INFO] Writing cache key with manifest reference to /.fondant/feature_extraction_pipeline/cache/6299637a94f3236310f16b42153e9731.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[Kbiopython_component-1 exited with code 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:34,335 | fondant.cli | INFO] Component `GenerateProteinSequenceChecksumComponent` found in module main\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:34,338 | fondant.component.executor | INFO] Caching disabled for the component\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:34,338 | root | INFO] Executing component\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:34,607 | distributed.http.proxy | INFO] To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:34,626 | distributed.scheduler | INFO] State start\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:34,631 | distributed.scheduler | INFO]   Scheduler at:     tcp://127.0.0.1:39847\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:34,631 | distributed.scheduler | INFO]   dashboard at:  http://127.0.0.1:8787/status\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:34,632 | distributed.scheduler | INFO] Registering Worker plugin shuffle\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:34,651 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:38943'\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:34,656 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:38903'\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:34,658 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:36271'\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:34,659 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:44253'\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:34,677 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:37669'\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:34,680 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:42975'\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:34,682 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:40697'\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:34,685 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:34627'\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,847 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:39301\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,848 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:39301\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,848 | distributed.worker | INFO]           Worker name:                          0\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,848 | distributed.worker | INFO]          dashboard at:            127.0.0.1:43425\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,848 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:39847\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,848 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,848 | distributed.worker | INFO]               Threads:                          1\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,848 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,848 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-9mrnwnqh\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,848 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,856 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:39301', name: 0, status: init, memory: 0, processing: 0>\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,860 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,860 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:39301\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,861 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:57492\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,863 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:39847\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,864 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,868 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:39847\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,902 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:38453\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,902 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:38453\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,902 | distributed.worker | INFO]           Worker name:                          7\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,903 | distributed.worker | INFO]          dashboard at:            127.0.0.1:38175\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,903 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:39847\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,903 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,903 | distributed.worker | INFO]               Threads:                          1\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,903 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,903 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-24v_cy7r\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,903 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,909 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:38453', name: 7, status: init, memory: 0, processing: 0>\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,910 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:38453\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,910 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:57506\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,911 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,911 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:39847\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,911 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,912 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:39847\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,935 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:45187\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,935 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:45187\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,935 | distributed.worker | INFO]           Worker name:                          1\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,935 | distributed.worker | INFO]          dashboard at:            127.0.0.1:44053\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,935 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:39847\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,935 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,935 | distributed.worker | INFO]               Threads:                          1\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,935 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,936 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-tb3_jkeb\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,936 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,941 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:45187', name: 1, status: init, memory: 0, processing: 0>\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,941 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:45187\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,941 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:57514\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,942 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,943 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:39847\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,943 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,944 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:39847\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,964 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:33909\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,964 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:33909\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,964 | distributed.worker | INFO]           Worker name:                          2\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,964 | distributed.worker | INFO]          dashboard at:            127.0.0.1:46617\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,964 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:39847\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,964 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,964 | distributed.worker | INFO]               Threads:                          1\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,964 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,964 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-k8eose4j\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,964 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,964 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:43597\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,964 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:43597\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,964 | distributed.worker | INFO]           Worker name:                          5\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,965 | distributed.worker | INFO]          dashboard at:            127.0.0.1:36395\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,965 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:39847\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,965 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,965 | distributed.worker | INFO]               Threads:                          1\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,965 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,965 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-4dfng1oa\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,965 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,969 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:33909', name: 2, status: init, memory: 0, processing: 0>\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,970 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:33909\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,970 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:57522\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,971 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:43597', name: 5, status: init, memory: 0, processing: 0>\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,971 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,971 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:43597\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,971 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:57526\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,972 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:39847\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,972 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,972 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,972 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:39847\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,973 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:39847\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,973 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,974 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:39847\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,975 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:44185\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,975 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:44185\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,975 | distributed.worker | INFO]           Worker name:                          6\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,975 | distributed.worker | INFO]          dashboard at:            127.0.0.1:39945\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,975 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:39847\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,975 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,975 | distributed.worker | INFO]               Threads:                          1\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,975 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,975 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-6qc5bpxr\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,975 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,979 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:44185', name: 6, status: init, memory: 0, processing: 0>\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,979 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:44185\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,979 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:57542\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,980 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,980 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:39847\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,980 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:35,981 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:39847\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,009 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:38859\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,009 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:38859\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,009 | distributed.worker | INFO]           Worker name:                          4\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,009 | distributed.worker | INFO]          dashboard at:            127.0.0.1:42731\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,009 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:39847\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,009 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,009 | distributed.worker | INFO]               Threads:                          1\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,009 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,009 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-wlfp0ork\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,009 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,013 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:38859', name: 4, status: init, memory: 0, processing: 0>\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,014 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:38859\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,014 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:57548\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,014 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,015 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:39847\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,015 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,015 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:39847\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,090 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:36505\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,090 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:36505\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,090 | distributed.worker | INFO]           Worker name:                          3\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,090 | distributed.worker | INFO]          dashboard at:            127.0.0.1:38201\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,090 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:39847\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,090 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,090 | distributed.worker | INFO]               Threads:                          1\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,090 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,090 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-2vzx329l\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,090 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,093 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:36505', name: 3, status: init, memory: 0, processing: 0>\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,094 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:36505\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,094 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:57552\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,094 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,095 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:39847\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,095 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,095 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:39847\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,135 | distributed.scheduler | INFO] Receive client connection: Client-06dcf411-323d-11ef-8001-0242ac120002\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,135 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:57556\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,164 | fondant.component.data_io | INFO] The number of partitions of the input dataframe is 5. The available number of workers is 8.\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,165 | fondant.component.data_io | INFO] Repartitioning the data to 8 partitions before processing to maximize worker usage\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,165 | root | INFO] Columns of dataframe: ['sequence']\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,177 | root | INFO] Creating write task for: /.fondant/feature_extraction_pipeline/feature_extraction_pipeline-20240624171820/generate_protein_sequence_checksum_component\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,177 | root | INFO] Writing data...\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,436 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:38943'. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,437 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,437 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:38903'. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,437 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,438 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:36271'. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,438 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,438 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:44253'. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,439 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,439 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:37669'. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,439 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:33909. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,440 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,440 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:42975'. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,441 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:39301. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,442 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:45187. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,442 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,442 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:40697'. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,443 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,443 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:36505. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,443 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:34627'. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,443 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:38859. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,443 | distributed.core | INFO] Connection to tcp://127.0.0.1:39847 has been closed.\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,444 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,445 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:44185. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,445 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:43597. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,446 | distributed.core | INFO] Connection to tcp://127.0.0.1:39847 has been closed.\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,447 | distributed.core | INFO] Connection to tcp://127.0.0.1:39847 has been closed.\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,447 | distributed.core | INFO] Connection to tcp://127.0.0.1:39847 has been closed.\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,450 | distributed.core | INFO] Connection to tcp://127.0.0.1:39847 has been closed.\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,451 | distributed.core | INFO] Connection to tcp://127.0.0.1:39847 has been closed.\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,451 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:38453. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,457 | distributed.core | INFO] Connection to tcp://127.0.0.1:39847 has been closed.\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,455 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:57522; closing.\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,459 | distributed.core | INFO] Connection to tcp://127.0.0.1:39847 has been closed.\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,458 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:57492; closing.\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,461 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:57552; closing.\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,461 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:57548; closing.\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,465 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:33909', name: 2, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1719242316.4650097')\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,466 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:39301', name: 0, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1719242316.4662821')\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,467 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:36505', name: 3, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1719242316.46693')\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,467 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:38859', name: 4, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1719242316.467556')\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,468 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:57542; closing.\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,468 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:57514; closing.\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,469 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:57526; closing.\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,470 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:44185', name: 6, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1719242316.4708343')\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,471 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:45187', name: 1, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1719242316.4716089')\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,472 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:43597', name: 5, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1719242316.47254')\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,473 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:57506; closing.\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,474 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:38453', name: 7, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1719242316.4747674')\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,475 | distributed.scheduler | INFO] Lost all workers\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,476 | distributed.batched | INFO] Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:39847 remote=tcp://127.0.0.1:57526>\n",
      "generate_protein_sequence_checksum_component-1  | Traceback (most recent call last):\n",
      "generate_protein_sequence_checksum_component-1  |   File \"/usr/local/lib/python3.11/site-packages/distributed/batched.py\", line 115, in _background_send\n",
      "generate_protein_sequence_checksum_component-1  |     nbytes = yield coro\n",
      "generate_protein_sequence_checksum_component-1  |              ^^^^^^^^^^\n",
      "generate_protein_sequence_checksum_component-1  |   File \"/usr/local/lib/python3.11/site-packages/tornado/gen.py\", line 767, in run\n",
      "generate_protein_sequence_checksum_component-1  |     value = future.result()\n",
      "generate_protein_sequence_checksum_component-1  |             ^^^^^^^^^^^^^^^\n",
      "generate_protein_sequence_checksum_component-1  |   File \"/usr/local/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 262, in write\n",
      "generate_protein_sequence_checksum_component-1  |     raise CommClosedError()\n",
      "generate_protein_sequence_checksum_component-1  | distributed.comm.core.CommClosedError\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,493 | distributed.batched | INFO] Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:39847 remote=tcp://127.0.0.1:57542>\n",
      "generate_protein_sequence_checksum_component-1  | Traceback (most recent call last):\n",
      "generate_protein_sequence_checksum_component-1  |   File \"/usr/local/lib/python3.11/site-packages/distributed/batched.py\", line 115, in _background_send\n",
      "generate_protein_sequence_checksum_component-1  |     nbytes = yield coro\n",
      "generate_protein_sequence_checksum_component-1  |              ^^^^^^^^^^\n",
      "generate_protein_sequence_checksum_component-1  |   File \"/usr/local/lib/python3.11/site-packages/tornado/gen.py\", line 767, in run\n",
      "generate_protein_sequence_checksum_component-1  |     value = future.result()\n",
      "generate_protein_sequence_checksum_component-1  |             ^^^^^^^^^^^^^^^\n",
      "generate_protein_sequence_checksum_component-1  |   File \"/usr/local/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 262, in write\n",
      "generate_protein_sequence_checksum_component-1  |     raise CommClosedError()\n",
      "generate_protein_sequence_checksum_component-1  | distributed.comm.core.CommClosedError\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,494 | distributed.batched | INFO] Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:39847 remote=tcp://127.0.0.1:57514>\n",
      "generate_protein_sequence_checksum_component-1  | Traceback (most recent call last):\n",
      "generate_protein_sequence_checksum_component-1  |   File \"/usr/local/lib/python3.11/site-packages/distributed/batched.py\", line 115, in _background_send\n",
      "generate_protein_sequence_checksum_component-1  |     nbytes = yield coro\n",
      "generate_protein_sequence_checksum_component-1  |              ^^^^^^^^^^\n",
      "generate_protein_sequence_checksum_component-1  |   File \"/usr/local/lib/python3.11/site-packages/tornado/gen.py\", line 767, in run\n",
      "generate_protein_sequence_checksum_component-1  |     value = future.result()\n",
      "generate_protein_sequence_checksum_component-1  |             ^^^^^^^^^^^^^^^\n",
      "generate_protein_sequence_checksum_component-1  |   File \"/usr/local/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 262, in write\n",
      "generate_protein_sequence_checksum_component-1  |     raise CommClosedError()\n",
      "generate_protein_sequence_checksum_component-1  | distributed.comm.core.CommClosedError\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,851 | distributed.scheduler | INFO] Scheduler closing due to unknown reason...\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,851 | distributed.scheduler | INFO] Scheduler closing all comms\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,854 | fondant.component.executor | INFO] Saving output manifest to /.fondant/feature_extraction_pipeline/feature_extraction_pipeline-20240624171820/generate_protein_sequence_checksum_component/manifest.json\n",
      "generate_protein_sequence_checksum_component-1  | [2024-06-24 15:18:36,854 | fondant.component.executor | INFO] Writing cache key with manifest reference to /.fondant/feature_extraction_pipeline/cache/5a301223e6ef93aa419c2bb7096d4d56.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[Kgenerate_protein_sequence_checksum_component-1 exited with code 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ifeatureomega_component-1                       | [2024-06-24 15:18:39,910 | matplotlib.font_manager | INFO] generated new fontManager\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:40,204 | fondant.cli | INFO] Component `IFeatureOmegaComponent` found in module main\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:40,211 | fondant.component.executor | INFO] Caching disabled for the component\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:40,211 | root | INFO] Executing component\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:40,494 | distributed.http.proxy | INFO] To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:40,513 | distributed.scheduler | INFO] State start\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:40,517 | distributed.scheduler | INFO]   Scheduler at:     tcp://127.0.0.1:40455\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:40,517 | distributed.scheduler | INFO]   dashboard at:  http://127.0.0.1:8787/status\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:40,517 | distributed.scheduler | INFO] Registering Worker plugin shuffle\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:40,536 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:42197'\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:40,540 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:38371'\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:40,542 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:37655'\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:40,543 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:45445'\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:40,558 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:36959'\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:40,561 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:39493'\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:40,564 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:41443'\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:40,569 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:38335'\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:41,899 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:35269\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:41,899 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:35269\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:41,899 | distributed.worker | INFO]           Worker name:                          1\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:41,899 | distributed.worker | INFO]          dashboard at:            127.0.0.1:40221\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:41,899 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:40455\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:41,899 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:41,899 | distributed.worker | INFO]               Threads:                          1\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:41,899 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:41,899 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-5ym3w4fj\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:41,899 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:41,909 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:35269', name: 1, status: init, memory: 0, processing: 0>\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:41,916 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:35269\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:41,917 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:48374\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:41,918 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:41,919 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:40455\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:41,920 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:41,921 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:40455\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,019 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:39233\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,019 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:39233\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,019 | distributed.worker | INFO]           Worker name:                          0\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,019 | distributed.worker | INFO]          dashboard at:            127.0.0.1:36737\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,019 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:40455\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,019 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,019 | distributed.worker | INFO]               Threads:                          1\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,019 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,019 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-lwc1ywv7\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,019 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,025 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:39233', name: 0, status: init, memory: 0, processing: 0>\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,026 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:39233\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,026 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:48380\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,026 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,027 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:40455\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,027 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,029 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:40455\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,068 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:40617\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,068 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:40813\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,069 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:40617\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,069 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:40813\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,069 | distributed.worker | INFO]           Worker name:                          3\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,069 | distributed.worker | INFO]          dashboard at:            127.0.0.1:45919\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,069 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:40455\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,069 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,069 | distributed.worker | INFO]               Threads:                          1\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,069 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,069 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-d_gg7rgo\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,069 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,069 | distributed.worker | INFO]           Worker name:                          4\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,070 | distributed.worker | INFO]          dashboard at:            127.0.0.1:38333\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,070 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:40455\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,070 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,070 | distributed.worker | INFO]               Threads:                          1\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,070 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,070 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-emf4pg5z\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,070 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,074 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:40813', name: 3, status: init, memory: 0, processing: 0>\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,074 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:40813\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,074 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:48384\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,075 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,075 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:40455\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,076 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,076 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:40617', name: 4, status: init, memory: 0, processing: 0>\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,076 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:40455\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,077 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,078 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:40455\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,078 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,078 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:40617\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,078 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:48392\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,079 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:40455\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,081 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:42747\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,081 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:42747\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,081 | distributed.worker | INFO]           Worker name:                          5\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,081 | distributed.worker | INFO]          dashboard at:            127.0.0.1:45615\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,081 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:40455\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,081 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,081 | distributed.worker | INFO]               Threads:                          1\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,081 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,081 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-s1g_cw4v\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,081 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,085 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:42747', name: 5, status: init, memory: 0, processing: 0>\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,085 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:42747\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,085 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:48398\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,086 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,086 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:40455\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,086 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,086 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:40455\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,112 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:42295\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,112 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:42295\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,112 | distributed.worker | INFO]           Worker name:                          2\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,112 | distributed.worker | INFO]          dashboard at:            127.0.0.1:38513\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,112 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:40455\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,112 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,112 | distributed.worker | INFO]               Threads:                          1\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,112 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,112 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-e8x9mz_r\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,113 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,117 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:42295', name: 2, status: init, memory: 0, processing: 0>\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,118 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:42295\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,118 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:48410\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,118 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,119 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:40455\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,119 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,119 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:40455\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,149 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:44887\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,149 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:44887\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,149 | distributed.worker | INFO]           Worker name:                          7\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,149 | distributed.worker | INFO]          dashboard at:            127.0.0.1:43331\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,149 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:40455\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,149 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,149 | distributed.worker | INFO]               Threads:                          1\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,150 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,150 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-su3fohli\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,150 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,154 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:44887', name: 7, status: init, memory: 0, processing: 0>\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,154 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:44887\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,154 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:48424\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,155 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,155 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:40455\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,155 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,156 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:40455\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,198 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:35753\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,198 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:35753\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,198 | distributed.worker | INFO]           Worker name:                          6\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,198 | distributed.worker | INFO]          dashboard at:            127.0.0.1:37157\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,198 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:40455\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,198 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,198 | distributed.worker | INFO]               Threads:                          1\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,198 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,198 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-gpd542k5\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,198 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,202 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:35753', name: 6, status: init, memory: 0, processing: 0>\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,202 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:35753\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,202 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:48440\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,203 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,203 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:40455\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,203 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,204 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:40455\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,242 | distributed.scheduler | INFO] Receive client connection: Client-0a811daf-323d-11ef-8001-0242ac120002\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,243 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:48454\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,353 | fondant.component.data_io | INFO] Total number of rows is 5.\n",
      "ifeatureomega_component-1                       | Repartitioning the data from <dask.utils.IndexCallable object at 0x7f1f053256f0> partitions to have 2 such that the number of partitions per row is approximately5\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,353 | fondant.component.data_io | WARNING] Setting the `input partition rows` has caused the system to not utilize all available workers 2 out of 8 are used.\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,354 | root | INFO] Columns of dataframe: ['sequence', 'sequence_checksum']\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,541 | root | INFO] Creating write task for: /.fondant/feature_extraction_pipeline/feature_extraction_pipeline-20240624171820/ifeatureomega_component\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:42,541 | root | INFO] Writing data...\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:43,909 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:42197'. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:43,910 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:43,910 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:38371'. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:43,910 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:43,911 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:37655'. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:43,911 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:43,911 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:45445'. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:43,913 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:39233. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:43,913 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:43,913 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:42295. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:43,913 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:36959'. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:43,914 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:43,915 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:39493'. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:43,917 | distributed.core | INFO] Connection to tcp://127.0.0.1:40455 has been closed.\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:43,917 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:40813. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:43,917 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:43,918 | distributed.core | INFO] Connection to tcp://127.0.0.1:40455 has been closed.\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:43,918 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:41443'. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:43,918 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:40617. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:43,919 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:42747. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:43,919 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:43,922 | distributed.core | INFO] Connection to tcp://127.0.0.1:40455 has been closed.\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:43,920 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:38335'. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:43,922 | distributed.core | INFO] Connection to tcp://127.0.0.1:40455 has been closed.\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:43,924 | distributed.core | INFO] Connection to tcp://127.0.0.1:40455 has been closed.\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:43,924 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:35753. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:43,924 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:43,928 | distributed.core | INFO] Connection to tcp://127.0.0.1:40455 has been closed.\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:43,929 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:35269. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:43,933 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:44887. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:43,935 | distributed.core | INFO] Connection to tcp://127.0.0.1:40455 has been closed.\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:43,936 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:48380; closing.\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:43,936 | distributed.core | INFO] Connection to tcp://127.0.0.1:40455 has been closed.\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:43,941 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:48410; closing.\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:43,942 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:48384; closing.\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:43,943 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:48392; closing.\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:43,943 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:48398; closing.\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:43,954 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:39233', name: 0, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1719242323.9538205')\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:43,958 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:42295', name: 2, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1719242323.9587274')\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:43,960 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:40813', name: 3, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1719242323.9597526')\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:43,968 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:40617', name: 4, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1719242323.9683738')\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:43,972 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:42747', name: 5, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1719242323.9727604')\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:43,974 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:48440; closing.\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:43,989 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:35753', name: 6, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1719242323.9895475')\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:43,993 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:48374; closing.\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:44,001 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:35269', name: 1, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1719242324.0013096')\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:44,002 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:48424; closing.\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:44,002 | distributed.batched | INFO] Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:40455 remote=tcp://127.0.0.1:48440>\n",
      "ifeatureomega_component-1                       | Traceback (most recent call last):\n",
      "ifeatureomega_component-1                       |   File \"/usr/local/lib/python3.11/site-packages/distributed/batched.py\", line 115, in _background_send\n",
      "ifeatureomega_component-1                       |     nbytes = yield coro\n",
      "ifeatureomega_component-1                       |              ^^^^^^^^^^\n",
      "ifeatureomega_component-1                       |   File \"/usr/local/lib/python3.11/site-packages/tornado/gen.py\", line 767, in run\n",
      "ifeatureomega_component-1                       |     value = future.result()\n",
      "ifeatureomega_component-1                       |             ^^^^^^^^^^^^^^^\n",
      "ifeatureomega_component-1                       |   File \"/usr/local/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 262, in write\n",
      "ifeatureomega_component-1                       |     raise CommClosedError()\n",
      "ifeatureomega_component-1                       | distributed.comm.core.CommClosedError\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:44,016 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:44887', name: 7, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1719242324.016626')\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:44,017 | distributed.scheduler | INFO] Lost all workers\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:44,544 | distributed.scheduler | INFO] Scheduler closing due to unknown reason...\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:44,544 | distributed.scheduler | INFO] Scheduler closing all comms\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:44,548 | fondant.component.executor | INFO] Saving output manifest to /.fondant/feature_extraction_pipeline/feature_extraction_pipeline-20240624171820/ifeatureomega_component/manifest.json\n",
      "ifeatureomega_component-1                       | [2024-06-24 15:18:44,549 | fondant.component.executor | INFO] Writing cache key with manifest reference to /.fondant/feature_extraction_pipeline/cache/44b22757d2f1e0f13fe50dc2411c045c.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[Kifeatureomega_component-1 exited with code 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "filter_pdb_component-1                          | [2024-06-24 15:18:46,766 | fondant.cli | INFO] Component `FilterPDBComponent` found in module main\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:46,772 | fondant.component.executor | INFO] Caching disabled for the component\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:46,772 | root | INFO] Executing component\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:47,079 | distributed.http.proxy | INFO] To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:47,097 | distributed.scheduler | INFO] State start\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:47,110 | distributed.scheduler | INFO]   Scheduler at:     tcp://127.0.0.1:39747\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:47,110 | distributed.scheduler | INFO]   dashboard at:  http://127.0.0.1:8787/status\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:47,110 | distributed.scheduler | INFO] Registering Worker plugin shuffle\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:47,129 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:40055'\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:47,135 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:36749'\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:47,137 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:43647'\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:47,139 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:33567'\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:47,164 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:36775'\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:47,166 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:36621'\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:47,168 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:39303'\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:47,173 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:35551'\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,350 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:45407\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,350 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:45407\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,350 | distributed.worker | INFO]           Worker name:                          1\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,350 | distributed.worker | INFO]          dashboard at:            127.0.0.1:46851\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,350 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:39747\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,350 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,350 | distributed.worker | INFO]               Threads:                          1\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,350 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,350 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-hycu3y6e\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,350 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,356 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:45407', name: 1, status: init, memory: 0, processing: 0>\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,360 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:45407\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,360 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:54720\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,362 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,363 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:39747\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,363 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,366 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:39747\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,416 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:34911\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,416 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:34911\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,416 | distributed.worker | INFO]           Worker name:                          2\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,416 | distributed.worker | INFO]          dashboard at:            127.0.0.1:35635\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,416 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:39747\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,416 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,416 | distributed.worker | INFO]               Threads:                          1\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,416 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,417 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-ya5ik_30\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,417 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,420 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:38639\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,421 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:38639\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,421 | distributed.worker | INFO]           Worker name:                          0\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,421 | distributed.worker | INFO]          dashboard at:            127.0.0.1:45387\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,421 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:39747\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,421 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,421 | distributed.worker | INFO]               Threads:                          1\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,421 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,422 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-m1r2r126\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,422 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,423 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:34911', name: 2, status: init, memory: 0, processing: 0>\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,423 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:34911\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,423 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:54734\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,424 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,424 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:39747\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,424 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,425 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:39747\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,430 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:38639', name: 0, status: init, memory: 0, processing: 0>\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,432 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,432 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:38639\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,432 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:54744\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,432 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:39747\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,432 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,434 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:39747\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,435 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:46241\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,436 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:46241\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,436 | distributed.worker | INFO]           Worker name:                          3\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,436 | distributed.worker | INFO]          dashboard at:            127.0.0.1:46033\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,436 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:39747\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,436 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,436 | distributed.worker | INFO]               Threads:                          1\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,436 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,436 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-qhniuevr\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,436 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,439 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:44929\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,440 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:44929\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,440 | distributed.worker | INFO]           Worker name:                          5\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,440 | distributed.worker | INFO]          dashboard at:            127.0.0.1:46297\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,440 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:39747\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,440 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,440 | distributed.worker | INFO]               Threads:                          1\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,440 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,440 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-qszqr4dp\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,440 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,442 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:46241', name: 3, status: init, memory: 0, processing: 0>\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,442 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:46241\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,442 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:54748\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,443 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,443 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:39747\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,443 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,444 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:39747\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,447 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:44929', name: 5, status: init, memory: 0, processing: 0>\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,447 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:44929\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,447 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:54754\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,448 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,449 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:39747\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,449 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,450 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:39747\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,543 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:45099\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,543 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:45099\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,543 | distributed.worker | INFO]           Worker name:                          7\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,543 | distributed.worker | INFO]          dashboard at:            127.0.0.1:35129\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,543 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:39747\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,543 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,543 | distributed.worker | INFO]               Threads:                          1\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,543 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,543 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-_awqz13c\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,543 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,548 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:45099', name: 7, status: init, memory: 0, processing: 0>\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,549 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:45099\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,549 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:54756\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,550 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,550 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:39747\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,550 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,551 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:39747\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,567 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:32855\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,567 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:32855\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,567 | distributed.worker | INFO]           Worker name:                          6\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,567 | distributed.worker | INFO]          dashboard at:            127.0.0.1:42075\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,567 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:39747\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,568 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,568 | distributed.worker | INFO]               Threads:                          1\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,568 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,568 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-wyy9b64e\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,568 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,572 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:32855', name: 6, status: init, memory: 0, processing: 0>\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,573 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:32855\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,573 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:54766\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,573 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,574 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:39747\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,574 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,574 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:39747\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,611 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:35057\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,611 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:35057\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,611 | distributed.worker | INFO]           Worker name:                          4\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,611 | distributed.worker | INFO]          dashboard at:            127.0.0.1:42531\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,611 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:39747\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,611 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,611 | distributed.worker | INFO]               Threads:                          1\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,611 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,611 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-7r1fltwz\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,611 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,615 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:35057', name: 4, status: init, memory: 0, processing: 0>\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,616 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:35057\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,616 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:54776\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,616 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,617 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:39747\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,617 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,617 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:39747\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,670 | distributed.scheduler | INFO] Receive client connection: Client-0e55d7fe-323d-11ef-8001-0242ac120002\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,670 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:54792\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,729 | fondant.component.data_io | INFO] The number of partitions of the input dataframe is 2. The available number of workers is 8.\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,730 | fondant.component.data_io | INFO] Repartitioning the data to 8 partitions before processing to maximize worker usage\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,730 | root | INFO] Columns of dataframe: ['sequence', 'sequence_checksum']\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,747 | root | INFO] Creating write task for: /.fondant/feature_extraction_pipeline/feature_extraction_pipeline-20240624171820/filter_pdb_component\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:48,747 | root | INFO] Writing data...\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:49,119 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:40055'. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:49,119 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:49,120 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:36749'. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:49,120 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:49,120 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:38639. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:49,120 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:43647'. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:49,121 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:49,121 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:33567'. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:49,121 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:45407. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:49,122 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:49,122 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:36775'. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:49,122 | distributed.core | INFO] Connection to tcp://127.0.0.1:39747 has been closed.\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:49,123 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:34911. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:49,123 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:49,123 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:36621'. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:49,123 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:49,123 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:46241. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:49,123 | distributed.core | INFO] Connection to tcp://127.0.0.1:39747 has been closed.\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:49,123 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:39303'. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:49,124 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:49,124 | distributed.core | INFO] Connection to tcp://127.0.0.1:39747 has been closed.\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:49,125 | distributed.core | INFO] Connection to tcp://127.0.0.1:39747 has been closed.\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:49,125 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:35551'. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:49,125 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:49,125 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:32855. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:49,125 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:35057. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:49,125 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:44929. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:49,126 | distributed.core | INFO] Connection to tcp://127.0.0.1:39747 has been closed.\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:49,127 | distributed.core | INFO] Connection to tcp://127.0.0.1:39747 has been closed.\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:49,127 | distributed.core | INFO] Connection to tcp://127.0.0.1:39747 has been closed.\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:49,134 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:54744; closing.\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:49,137 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:54720; closing.\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:49,137 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:54734; closing.\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:49,137 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:54748; closing.\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:49,138 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:38639', name: 0, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1719242329.1387897')\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:49,139 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:45407', name: 1, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1719242329.1393824')\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:49,139 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:34911', name: 2, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1719242329.1397552')\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:49,140 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:46241', name: 3, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1719242329.1400409')\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:49,140 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:54766; closing.\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:49,140 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:54776; closing.\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:49,140 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:54754; closing.\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:49,140 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:45099. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:49,141 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:32855', name: 6, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1719242329.1412938')\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:49,141 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:35057', name: 4, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1719242329.141589')\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:49,141 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:44929', name: 5, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1719242329.1418545')\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:49,142 | distributed.core | INFO] Connection to tcp://127.0.0.1:39747 has been closed.\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:49,142 | distributed.batched | INFO] Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:39747 remote=tcp://127.0.0.1:54766>\n",
      "filter_pdb_component-1                          | Traceback (most recent call last):\n",
      "filter_pdb_component-1                          |   File \"/usr/local/lib/python3.11/site-packages/distributed/batched.py\", line 115, in _background_send\n",
      "filter_pdb_component-1                          |     nbytes = yield coro\n",
      "filter_pdb_component-1                          |              ^^^^^^^^^^\n",
      "filter_pdb_component-1                          |   File \"/usr/local/lib/python3.11/site-packages/tornado/gen.py\", line 767, in run\n",
      "filter_pdb_component-1                          |     value = future.result()\n",
      "filter_pdb_component-1                          |             ^^^^^^^^^^^^^^^\n",
      "filter_pdb_component-1                          |   File \"/usr/local/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 262, in write\n",
      "filter_pdb_component-1                          |     raise CommClosedError()\n",
      "filter_pdb_component-1                          | distributed.comm.core.CommClosedError\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:49,143 | distributed.batched | INFO] Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:39747 remote=tcp://127.0.0.1:54776>\n",
      "filter_pdb_component-1                          | Traceback (most recent call last):\n",
      "filter_pdb_component-1                          |   File \"/usr/local/lib/python3.11/site-packages/distributed/batched.py\", line 115, in _background_send\n",
      "filter_pdb_component-1                          |     nbytes = yield coro\n",
      "filter_pdb_component-1                          |              ^^^^^^^^^^\n",
      "filter_pdb_component-1                          |   File \"/usr/local/lib/python3.11/site-packages/tornado/gen.py\", line 767, in run\n",
      "filter_pdb_component-1                          |     value = future.result()\n",
      "filter_pdb_component-1                          |             ^^^^^^^^^^^^^^^\n",
      "filter_pdb_component-1                          |   File \"/usr/local/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 262, in write\n",
      "filter_pdb_component-1                          |     raise CommClosedError()\n",
      "filter_pdb_component-1                          | distributed.comm.core.CommClosedError\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:49,144 | distributed.batched | INFO] Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:39747 remote=tcp://127.0.0.1:54754>\n",
      "filter_pdb_component-1                          | Traceback (most recent call last):\n",
      "filter_pdb_component-1                          |   File \"/usr/local/lib/python3.11/site-packages/distributed/batched.py\", line 115, in _background_send\n",
      "filter_pdb_component-1                          |     nbytes = yield coro\n",
      "filter_pdb_component-1                          |              ^^^^^^^^^^\n",
      "filter_pdb_component-1                          |   File \"/usr/local/lib/python3.11/site-packages/tornado/gen.py\", line 767, in run\n",
      "filter_pdb_component-1                          |     value = future.result()\n",
      "filter_pdb_component-1                          |             ^^^^^^^^^^^^^^^\n",
      "filter_pdb_component-1                          |   File \"/usr/local/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 262, in write\n",
      "filter_pdb_component-1                          |     raise CommClosedError()\n",
      "filter_pdb_component-1                          | distributed.comm.core.CommClosedError\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:49,145 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:54756; closing.\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:49,146 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:45099', name: 7, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1719242329.1459906')\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:49,146 | distributed.scheduler | INFO] Lost all workers\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:49,543 | distributed.scheduler | INFO] Scheduler closing due to unknown reason...\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:49,543 | distributed.scheduler | INFO] Scheduler closing all comms\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:49,547 | fondant.component.executor | INFO] Saving output manifest to /.fondant/feature_extraction_pipeline/feature_extraction_pipeline-20240624171820/filter_pdb_component/manifest.json\n",
      "filter_pdb_component-1                          | [2024-06-24 15:18:49,547 | fondant.component.executor | INFO] Writing cache key with manifest reference to /.fondant/feature_extraction_pipeline/cache/f6f5d5996b5645d3b1368d06d74e99a2.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[Kfilter_pdb_component-1 exited with code 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:51,410 | fondant.cli | INFO] Component `PredictProtein3DStructureComponent` found in module main\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:51,415 | fondant.component.executor | INFO] Caching disabled for the component\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:51,415 | root | INFO] Executing component\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:51,700 | distributed.http.proxy | INFO] To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:51,720 | distributed.scheduler | INFO] State start\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:51,724 | distributed.scheduler | INFO]   Scheduler at:     tcp://127.0.0.1:35995\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:51,724 | distributed.scheduler | INFO]   dashboard at:  http://127.0.0.1:8787/status\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:51,724 | distributed.scheduler | INFO] Registering Worker plugin shuffle\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:51,743 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:44495'\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:51,749 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:34431'\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:51,751 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:34321'\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:51,753 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:35891'\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:51,766 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:38829'\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:51,768 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:36913'\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:51,770 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:43457'\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:51,774 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:41829'\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:52,989 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:39491\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:52,989 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:39491\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:52,989 | distributed.worker | INFO]           Worker name:                          1\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:52,989 | distributed.worker | INFO]          dashboard at:            127.0.0.1:32771\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:52,989 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:35995\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:52,989 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:52,989 | distributed.worker | INFO]               Threads:                          1\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:52,989 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:52,989 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-chv2r921\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:52,989 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:52,995 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:39491', name: 1, status: init, memory: 0, processing: 0>\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:52,999 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:39491\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:52,999 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:38636\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,001 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,003 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:35995\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,003 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,004 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:35995\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,048 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:44443\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,048 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:44443\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,048 | distributed.worker | INFO]           Worker name:                          2\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,049 | distributed.worker | INFO]          dashboard at:            127.0.0.1:44971\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,049 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:35995\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,049 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,049 | distributed.worker | INFO]               Threads:                          1\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,049 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,049 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-am0eweq1\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,050 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,055 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:44443', name: 2, status: init, memory: 0, processing: 0>\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,057 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:44443\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,057 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:38638\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,059 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,060 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:35995\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,061 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,062 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:35995\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,065 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:39459\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,066 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:39459\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,066 | distributed.worker | INFO]           Worker name:                          6\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,066 | distributed.worker | INFO]          dashboard at:            127.0.0.1:42193\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,066 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:35995\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,066 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,066 | distributed.worker | INFO]               Threads:                          1\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,066 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,066 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-yd9fjqo8\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,066 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,071 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:33485\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,071 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:39459', name: 6, status: init, memory: 0, processing: 0>\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,071 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:33485\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,071 | distributed.worker | INFO]           Worker name:                          0\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,071 | distributed.worker | INFO]          dashboard at:            127.0.0.1:44491\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,072 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:35995\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,072 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,072 | distributed.worker | INFO]               Threads:                          1\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,072 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,072 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-_908vw7a\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,072 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,072 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:39459\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,072 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:38654\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,072 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,073 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:35995\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,073 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,074 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:35995\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,076 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:33485', name: 0, status: init, memory: 0, processing: 0>\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,077 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:33485\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,077 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:38668\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,077 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,078 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:35995\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,078 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,079 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:35995\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,093 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:38977\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,093 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:38977\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,093 | distributed.worker | INFO]           Worker name:                          3\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,093 | distributed.worker | INFO]          dashboard at:            127.0.0.1:46357\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,093 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:35995\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,093 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,093 | distributed.worker | INFO]               Threads:                          1\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,093 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,093 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-eh_p4mv5\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,093 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,097 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:38977', name: 3, status: init, memory: 0, processing: 0>\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,097 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:38977\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,098 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:38672\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,098 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,098 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:35995\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,099 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,099 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:35995\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,120 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:35873\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,121 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:35873\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,121 | distributed.worker | INFO]           Worker name:                          5\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,121 | distributed.worker | INFO]          dashboard at:            127.0.0.1:44791\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,121 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:35995\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,121 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,121 | distributed.worker | INFO]               Threads:                          1\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,121 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,121 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-vuoqii_g\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,121 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,125 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:35873', name: 5, status: init, memory: 0, processing: 0>\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,125 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:35873\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,125 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:38682\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,126 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,126 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:35995\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,126 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,127 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:35995\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,149 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:42689\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,149 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:42689\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,149 | distributed.worker | INFO]           Worker name:                          4\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,149 | distributed.worker | INFO]          dashboard at:            127.0.0.1:38141\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,149 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:35995\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,149 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,149 | distributed.worker | INFO]               Threads:                          1\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,149 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,149 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-vvvhrqly\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,149 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,153 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:42689', name: 4, status: init, memory: 0, processing: 0>\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,153 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:42689\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,153 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:38692\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,154 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,154 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:35995\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,154 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,154 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:35995\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,165 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:43883\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,165 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:43883\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,165 | distributed.worker | INFO]           Worker name:                          7\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,166 | distributed.worker | INFO]          dashboard at:            127.0.0.1:36115\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,166 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:35995\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,166 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,166 | distributed.worker | INFO]               Threads:                          1\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,166 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,166 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-57amww9j\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,166 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,169 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:43883', name: 7, status: init, memory: 0, processing: 0>\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,169 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:43883\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,169 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:38700\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,170 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,170 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:35995\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,170 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,170 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:35995\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,178 | distributed.scheduler | INFO] Receive client connection: Client-1105bad7-323d-11ef-8001-0242ac120002\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,178 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:38710\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,203 | fondant.component.data_io | INFO] The number of partitions of the input dataframe is 5. The available number of workers is 8.\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,204 | fondant.component.data_io | INFO] Repartitioning the data to 8 partitions before processing to maximize worker usage\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,204 | root | INFO] Columns of dataframe: ['sequence', 'sequence_checksum', 'pdb_string']\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,221 | root | INFO] Creating write task for: /.fondant/feature_extraction_pipeline/feature_extraction_pipeline-20240624171820/predict_protein_3d_structure_component\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,221 | root | INFO] Writing data...\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,497 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:44495'. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,497 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,498 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:34431'. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,499 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,499 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:34321'. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,499 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:33485. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,500 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,500 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:35891'. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,500 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:39491. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,501 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,501 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:38829'. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,501 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,501 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:44443. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,502 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:36913'. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,502 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,502 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:43457'. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,503 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,503 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:42689. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,504 | distributed.core | INFO] Connection to tcp://127.0.0.1:35995 has been closed.\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,504 | distributed.core | INFO] Connection to tcp://127.0.0.1:35995 has been closed.\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,504 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:35873. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,505 | distributed.core | INFO] Connection to tcp://127.0.0.1:35995 has been closed.\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,506 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:41829'. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,506 | distributed.core | INFO] Connection to tcp://127.0.0.1:35995 has been closed.\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,506 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,507 | distributed.core | INFO] Connection to tcp://127.0.0.1:35995 has been closed.\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,509 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:39459. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,512 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:38668; closing.\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,514 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:38636; closing.\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,514 | distributed.core | INFO] Connection to tcp://127.0.0.1:35995 has been closed.\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,515 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:38638; closing.\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,515 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:43883. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,516 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:38692; closing.\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,517 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:38682; closing.\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,517 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:33485', name: 0, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1719242333.517732')\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,517 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:38977. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,519 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:39491', name: 1, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1719242333.5194135')\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,520 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:44443', name: 2, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1719242333.5200372')\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,520 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:42689', name: 4, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1719242333.5205398')\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,521 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:35873', name: 5, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1719242333.520965')\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,522 | distributed.core | INFO] Connection to tcp://127.0.0.1:35995 has been closed.\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,522 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:38654; closing.\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,523 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:39459', name: 6, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1719242333.5238633')\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,527 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:38672; closing.\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,528 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:38977', name: 3, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1719242333.528561')\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,535 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:38700; closing.\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,535 | distributed.core | INFO] Connection to tcp://127.0.0.1:35995 has been closed.\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,536 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:43883', name: 7, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1719242333.5364156')\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,536 | distributed.scheduler | INFO] Lost all workers\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,914 | distributed.scheduler | INFO] Scheduler closing due to unknown reason...\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,914 | distributed.scheduler | INFO] Scheduler closing all comms\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,917 | fondant.component.executor | INFO] Saving output manifest to /.fondant/feature_extraction_pipeline/feature_extraction_pipeline-20240624171820/predict_protein_3d_structure_component/manifest.json\n",
      "predict_protein_3d_structure_component-1        | [2024-06-24 15:18:53,917 | fondant.component.executor | INFO] Writing cache key with manifest reference to /.fondant/feature_extraction_pipeline/cache/ef84054d8b46c910a0bd53e4868455b3.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[Kpredict_protein_3d_structure_component-1 exited with code 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "store_pdb_component-1                           | [2024-06-24 15:18:55,612 | fondant.cli | INFO] Component `StorePDBComponent` found in module main\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:55,619 | fondant.component.executor | INFO] Caching disabled for the component\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:55,619 | root | INFO] Executing component\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:55,875 | distributed.http.proxy | INFO] To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:55,894 | distributed.scheduler | INFO] State start\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:55,898 | distributed.scheduler | INFO]   Scheduler at:     tcp://127.0.0.1:45693\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:55,898 | distributed.scheduler | INFO]   dashboard at:  http://127.0.0.1:8787/status\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:55,899 | distributed.scheduler | INFO] Registering Worker plugin shuffle\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:55,917 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:35575'\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:55,923 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:40927'\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:55,924 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:40437'\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:55,926 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:34951'\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:55,940 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:40501'\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:55,941 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:41753'\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:55,943 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:33911'\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:55,945 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:39833'\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,237 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:35765\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,238 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:35765\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,238 | distributed.worker | INFO]           Worker name:                          2\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,238 | distributed.worker | INFO]          dashboard at:            127.0.0.1:37087\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,238 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:45693\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,238 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,238 | distributed.worker | INFO]               Threads:                          1\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,238 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,238 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-149jqfsk\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,238 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,244 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:44293\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,244 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:44293\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,244 | distributed.worker | INFO]           Worker name:                          6\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,244 | distributed.worker | INFO]          dashboard at:            127.0.0.1:34955\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,244 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:45693\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,244 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,244 | distributed.worker | INFO]               Threads:                          1\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,245 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,245 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-7psygvz1\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,245 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,244 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:35765', name: 2, status: init, memory: 0, processing: 0>\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,249 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:35765\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,249 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:50226\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,250 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,251 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:45693\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,251 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,251 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:45693\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,253 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:38055\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,253 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:38055\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,253 | distributed.worker | INFO]           Worker name:                          0\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,253 | distributed.worker | INFO]          dashboard at:            127.0.0.1:32913\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,253 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:45693\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,253 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,253 | distributed.worker | INFO]               Threads:                          1\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,253 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,253 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-8yzcf5xw\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,253 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,255 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:44293', name: 6, status: init, memory: 0, processing: 0>\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,255 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:44293\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,255 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:50232\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,256 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,256 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:45693\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,256 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,257 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:45693\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,259 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:38055', name: 0, status: init, memory: 0, processing: 0>\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,260 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:38055\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,260 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:50248\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,260 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,261 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:45693\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,261 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,262 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:45693\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,270 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:35403\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,270 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:35403\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,270 | distributed.worker | INFO]           Worker name:                          7\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,270 | distributed.worker | INFO]          dashboard at:            127.0.0.1:34243\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,270 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:45693\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,271 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,271 | distributed.worker | INFO]               Threads:                          1\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,271 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,271 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-5_j3srw2\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,271 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,275 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:42581\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,275 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:42581\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,275 | distributed.worker | INFO]           Worker name:                          3\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,275 | distributed.worker | INFO]          dashboard at:            127.0.0.1:34823\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,275 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:45693\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,275 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,275 | distributed.worker | INFO]               Threads:                          1\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,275 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,275 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-0libyj7e\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,275 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,276 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:35403', name: 7, status: init, memory: 0, processing: 0>\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,277 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:35403\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,277 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:50254\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,277 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,278 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:45693\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,278 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,279 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:45693\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,281 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:42581', name: 3, status: init, memory: 0, processing: 0>\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,282 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:42581\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,282 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:50266\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,283 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,283 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:45693\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,283 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,284 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:37001\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,284 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:37001\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,284 | distributed.worker | INFO]           Worker name:                          1\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,284 | distributed.worker | INFO]          dashboard at:            127.0.0.1:33515\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,284 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:45693\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,284 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:45693\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,284 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,284 | distributed.worker | INFO]               Threads:                          1\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,284 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,284 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-zxtmasq5\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,284 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,289 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:37001', name: 1, status: init, memory: 0, processing: 0>\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,289 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:37001\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,289 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:50278\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,290 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,290 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:45693\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,290 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,291 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:45693\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,301 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:37985\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,301 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:37985\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,301 | distributed.worker | INFO]           Worker name:                          5\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,301 | distributed.worker | INFO]          dashboard at:            127.0.0.1:33747\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,301 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:45693\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,301 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,301 | distributed.worker | INFO]               Threads:                          1\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,301 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,301 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-1hyk6ltg\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,301 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,304 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:37985', name: 5, status: init, memory: 0, processing: 0>\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,305 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:37985\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,305 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:50292\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,305 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,306 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:45693\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,306 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,306 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:45693\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,331 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:34429\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,331 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:34429\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,331 | distributed.worker | INFO]           Worker name:                          4\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,331 | distributed.worker | INFO]          dashboard at:            127.0.0.1:46513\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,331 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:45693\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,331 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,331 | distributed.worker | INFO]               Threads:                          1\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,331 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,332 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-cj115fn1\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,332 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,335 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:34429', name: 4, status: init, memory: 0, processing: 0>\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,335 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:34429\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,335 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:50300\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,336 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,336 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:45693\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,336 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,337 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:45693\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,344 | distributed.scheduler | INFO] Receive client connection: Client-13816405-323d-11ef-8001-0242ac120002\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,344 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:50308\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,370 | fondant.component.data_io | INFO] The number of partitions of the input dataframe is 5. The available number of workers is 8.\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,372 | fondant.component.data_io | INFO] Repartitioning the data to 8 partitions before processing to maximize worker usage\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,372 | root | INFO] Columns of dataframe: ['sequence', 'sequence_checksum', 'pdb_string']\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,389 | root | INFO] Creating write task for: /.fondant/feature_extraction_pipeline/feature_extraction_pipeline-20240624171820/store_pdb_component\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,389 | root | INFO] Writing data...\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,757 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:35575'. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,757 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,757 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:40927'. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,758 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,758 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:40437'. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,759 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:38055. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,759 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,759 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:34951'. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,760 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,761 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:40501'. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,761 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:37001. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,762 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,763 | distributed.core | INFO] Connection to tcp://127.0.0.1:45693 has been closed.\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,763 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:41753'. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,763 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,764 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:33911'. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,764 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:35765. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,764 | distributed.core | INFO] Connection to tcp://127.0.0.1:45693 has been closed.\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,766 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,766 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:39833'. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,767 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:42581. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,767 | distributed.core | INFO] Connection to tcp://127.0.0.1:45693 has been closed.\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,768 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,770 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:44293. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,772 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:35403. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,772 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:37985. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,773 | distributed.core | INFO] Connection to tcp://127.0.0.1:45693 has been closed.\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,774 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:50248; closing.\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,774 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:50278; closing.\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,773 | distributed.core | INFO] Connection to tcp://127.0.0.1:45693 has been closed.\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,774 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:50226; closing.\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,775 | distributed.core | INFO] Connection to tcp://127.0.0.1:45693 has been closed.\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,775 | distributed.core | INFO] Connection to tcp://127.0.0.1:45693 has been closed.\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,777 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:38055', name: 0, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1719242337.7772005')\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,780 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:37001', name: 1, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1719242337.7798445')\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,780 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:34429. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,781 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:35765', name: 2, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1719242337.781774')\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,783 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:50266; closing.\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,784 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:50232; closing.\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,786 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:42581', name: 3, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1719242337.7864263')\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,787 | distributed.core | INFO] Connection to tcp://127.0.0.1:45693 has been closed.\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,787 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:44293', name: 6, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1719242337.7877674')\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,788 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:50254; closing.\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,789 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:50292; closing.\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,791 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:35403', name: 7, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1719242337.7911978')\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,792 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:37985', name: 5, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1719242337.7919946')\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,794 | distributed.batched | INFO] Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:45693 remote=tcp://127.0.0.1:50266>\n",
      "store_pdb_component-1                           | Traceback (most recent call last):\n",
      "store_pdb_component-1                           |   File \"/usr/local/lib/python3.11/site-packages/distributed/batched.py\", line 115, in _background_send\n",
      "store_pdb_component-1                           |     nbytes = yield coro\n",
      "store_pdb_component-1                           |              ^^^^^^^^^^\n",
      "store_pdb_component-1                           |   File \"/usr/local/lib/python3.11/site-packages/tornado/gen.py\", line 767, in run\n",
      "store_pdb_component-1                           |     value = future.result()\n",
      "store_pdb_component-1                           |             ^^^^^^^^^^^^^^^\n",
      "store_pdb_component-1                           |   File \"/usr/local/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 262, in write\n",
      "store_pdb_component-1                           |     raise CommClosedError()\n",
      "store_pdb_component-1                           | distributed.comm.core.CommClosedError\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,797 | distributed.batched | INFO] Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:45693 remote=tcp://127.0.0.1:50232>\n",
      "store_pdb_component-1                           | Traceback (most recent call last):\n",
      "store_pdb_component-1                           |   File \"/usr/local/lib/python3.11/site-packages/distributed/batched.py\", line 115, in _background_send\n",
      "store_pdb_component-1                           |     nbytes = yield coro\n",
      "store_pdb_component-1                           |              ^^^^^^^^^^\n",
      "store_pdb_component-1                           |   File \"/usr/local/lib/python3.11/site-packages/tornado/gen.py\", line 767, in run\n",
      "store_pdb_component-1                           |     value = future.result()\n",
      "store_pdb_component-1                           |             ^^^^^^^^^^^^^^^\n",
      "store_pdb_component-1                           |   File \"/usr/local/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 262, in write\n",
      "store_pdb_component-1                           |     raise CommClosedError()\n",
      "store_pdb_component-1                           | distributed.comm.core.CommClosedError\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,798 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:50300; closing.\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,809 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:34429', name: 4, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1719242337.8095274')\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:57,810 | distributed.scheduler | INFO] Lost all workers\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:58,176 | distributed.scheduler | INFO] Scheduler closing due to unknown reason...\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:58,176 | distributed.scheduler | INFO] Scheduler closing all comms\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:58,180 | fondant.component.executor | INFO] Saving output manifest to /.fondant/feature_extraction_pipeline/feature_extraction_pipeline-20240624171820/store_pdb_component/manifest.json\n",
      "store_pdb_component-1                           | [2024-06-24 15:18:58,180 | fondant.component.executor | INFO] Writing cache key with manifest reference to /.fondant/feature_extraction_pipeline/cache/40d4c0b7c5966e8ea2b51d57928a20ea.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[Kstore_pdb_component-1 exited with code 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "msa_component-1                                 | [2024-06-24 15:18:59,836 | fondant.cli | INFO] Component `MSAComponent` found in module main\n",
      "msa_component-1                                 | [2024-06-24 15:18:59,841 | fondant.component.executor | INFO] Caching disabled for the component\n",
      "msa_component-1                                 | [2024-06-24 15:18:59,841 | root | INFO] Executing component\n",
      "msa_component-1                                 | [2024-06-24 15:19:00,112 | distributed.http.proxy | INFO] To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy\n",
      "msa_component-1                                 | [2024-06-24 15:19:00,131 | distributed.scheduler | INFO] State start\n",
      "msa_component-1                                 | [2024-06-24 15:19:00,135 | distributed.scheduler | INFO]   Scheduler at:     tcp://127.0.0.1:45005\n",
      "msa_component-1                                 | [2024-06-24 15:19:00,135 | distributed.scheduler | INFO]   dashboard at:  http://127.0.0.1:8787/status\n",
      "msa_component-1                                 | [2024-06-24 15:19:00,135 | distributed.scheduler | INFO] Registering Worker plugin shuffle\n",
      "msa_component-1                                 | [2024-06-24 15:19:00,154 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:46141'\n",
      "msa_component-1                                 | [2024-06-24 15:19:00,160 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:33025'\n",
      "msa_component-1                                 | [2024-06-24 15:19:00,163 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:43479'\n",
      "msa_component-1                                 | [2024-06-24 15:19:00,165 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:43847'\n",
      "msa_component-1                                 | [2024-06-24 15:19:00,176 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:40831'\n",
      "msa_component-1                                 | [2024-06-24 15:19:00,179 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:34061'\n",
      "msa_component-1                                 | [2024-06-24 15:19:00,185 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:39663'\n",
      "msa_component-1                                 | [2024-06-24 15:19:00,187 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:40793'\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,434 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:38725\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,434 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:38725\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,434 | distributed.worker | INFO]           Worker name:                          0\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,434 | distributed.worker | INFO]          dashboard at:            127.0.0.1:37065\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,434 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:45005\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,435 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,435 | distributed.worker | INFO]               Threads:                          1\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,435 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,435 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-hb_d0vlt\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,435 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,440 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:38725', name: 0, status: init, memory: 0, processing: 0>\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,441 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:46873\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,441 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:46873\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,441 | distributed.worker | INFO]           Worker name:                          1\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,441 | distributed.worker | INFO]          dashboard at:            127.0.0.1:39101\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,441 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:45005\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,441 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,441 | distributed.worker | INFO]               Threads:                          1\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,441 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,442 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-luhcob_3\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,442 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,444 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:38725\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,445 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:33400\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,445 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,446 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:45005\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,446 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,447 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:45005\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,451 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:46873', name: 1, status: init, memory: 0, processing: 0>\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,452 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:46873\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,452 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:33410\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,453 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,454 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:45005\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,454 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,455 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:45005\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,459 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:46229\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,459 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:46229\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,460 | distributed.worker | INFO]           Worker name:                          4\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,460 | distributed.worker | INFO]          dashboard at:            127.0.0.1:34825\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,460 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:45005\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,460 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,460 | distributed.worker | INFO]               Threads:                          1\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,461 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,462 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-ugkg83yx\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,462 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,466 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:46229', name: 4, status: init, memory: 0, processing: 0>\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,467 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:46229\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,467 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:33422\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,468 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,469 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:45005\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,469 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,470 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:45005\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,483 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:37057\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,483 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:37057\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,483 | distributed.worker | INFO]           Worker name:                          2\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,483 | distributed.worker | INFO]          dashboard at:            127.0.0.1:39677\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,483 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:45005\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,483 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,483 | distributed.worker | INFO]               Threads:                          1\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,483 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,483 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-x7j_b8sz\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,483 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,488 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:37057', name: 2, status: init, memory: 0, processing: 0>\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,489 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:37057\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,489 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:33424\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,489 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,490 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:45005\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,490 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,491 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:45005\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,530 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:45031\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,530 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:45031\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,530 | distributed.worker | INFO]           Worker name:                          5\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,530 | distributed.worker | INFO]          dashboard at:            127.0.0.1:41951\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,530 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:45005\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,530 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,530 | distributed.worker | INFO]               Threads:                          1\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,530 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,531 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-d8nosktk\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,531 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,535 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:45031', name: 5, status: init, memory: 0, processing: 0>\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,536 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:45031\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,536 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:33438\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,537 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,537 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:45005\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,537 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,538 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:45005\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,563 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:44631\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,563 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:44631\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,563 | distributed.worker | INFO]           Worker name:                          7\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,563 | distributed.worker | INFO]          dashboard at:            127.0.0.1:44607\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,563 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:45005\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,563 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,563 | distributed.worker | INFO]               Threads:                          1\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,563 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,563 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-tnu0u6_9\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,563 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,567 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:44631', name: 7, status: init, memory: 0, processing: 0>\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,568 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:36281\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,568 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:36281\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,568 | distributed.worker | INFO]           Worker name:                          6\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,568 | distributed.worker | INFO]          dashboard at:            127.0.0.1:33417\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,568 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:45005\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,568 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,568 | distributed.worker | INFO]               Threads:                          1\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,568 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,568 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:44631\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,568 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-mirfgqaq\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,568 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:33452\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,568 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,569 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,569 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:45005\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,569 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,570 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:45005\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,571 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:36281', name: 6, status: init, memory: 0, processing: 0>\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,572 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:36281\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,572 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:33458\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,572 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,573 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:45005\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,573 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,573 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:45005\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,575 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:37683\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,575 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:37683\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,575 | distributed.worker | INFO]           Worker name:                          3\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,575 | distributed.worker | INFO]          dashboard at:            127.0.0.1:37085\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,575 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:45005\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,575 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,575 | distributed.worker | INFO]               Threads:                          1\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,575 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,575 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-kw6e02v6\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,575 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,578 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:37683', name: 3, status: init, memory: 0, processing: 0>\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,579 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:37683\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,579 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:33468\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,579 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,580 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:45005\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,580 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,580 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:45005\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,590 | distributed.scheduler | INFO] Receive client connection: Client-16095d1b-323d-11ef-8001-0242ac120002\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,590 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:33480\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,676 | fondant.component.data_io | INFO] Total number of rows is 5.\n",
      "msa_component-1                                 | Repartitioning the data from <dask.utils.IndexCallable object at 0x7ff7ddca6920> partitions to have 1 such that the number of partitions per row is approximately10000\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,676 | fondant.component.data_io | WARNING] Setting the `input partition rows` has caused the system to not utilize all available workers 1 out of 8 are used.\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,676 | root | INFO] Columns of dataframe: ['sequence', 'sequence_checksum']\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,695 | root | INFO] Creating write task for: /.fondant/feature_extraction_pipeline/feature_extraction_pipeline-20240624171820/msa_component\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,695 | root | INFO] Writing data...\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,975 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:46141'. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,975 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,975 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:33025'. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,976 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,976 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:43479'. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,977 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:38725. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,978 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,978 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:43847'. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,978 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:46873. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,978 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,979 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:40831'. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,980 | distributed.core | INFO] Connection to tcp://127.0.0.1:45005 has been closed.\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,981 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:37057. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,981 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:37683. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,981 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,982 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:34061'. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,982 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,983 | distributed.core | INFO] Connection to tcp://127.0.0.1:45005 has been closed.\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,984 | distributed.core | INFO] Connection to tcp://127.0.0.1:45005 has been closed.\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,984 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:45031. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,982 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:39663'. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,984 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,984 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:40793'. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,985 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,985 | distributed.core | INFO] Connection to tcp://127.0.0.1:45005 has been closed.\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,986 | distributed.core | INFO] Connection to tcp://127.0.0.1:45005 has been closed.\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,987 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:44631. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,987 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:36281. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,988 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:46229. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,989 | distributed.core | INFO] Connection to tcp://127.0.0.1:45005 has been closed.\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,991 | distributed.core | INFO] Connection to tcp://127.0.0.1:45005 has been closed.\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,991 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:33400; closing.\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,992 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:33410; closing.\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,992 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:33424; closing.\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,992 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:33468; closing.\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,992 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:33438; closing.\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,993 | distributed.core | INFO] Connection to tcp://127.0.0.1:45005 has been closed.\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,993 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:38725', name: 0, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1719242341.9930522')\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,993 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:46873', name: 1, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1719242341.993545')\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,993 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:37057', name: 2, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1719242341.9938557')\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,994 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:37683', name: 3, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1719242341.9940815')\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,994 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:45031', name: 5, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1719242341.9946928')\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,995 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:33452; closing.\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,996 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:33458; closing.\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,996 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:44631', name: 7, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1719242341.9966426')\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,996 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:36281', name: 6, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1719242341.9969456')\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,997 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:33422; closing.\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,997 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:46229', name: 4, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1719242341.9976034')\n",
      "msa_component-1                                 | [2024-06-24 15:19:01,997 | distributed.scheduler | INFO] Lost all workers\n",
      "msa_component-1                                 | [2024-06-24 15:19:02,336 | distributed.scheduler | INFO] Scheduler closing due to unknown reason...\n",
      "msa_component-1                                 | [2024-06-24 15:19:02,336 | distributed.scheduler | INFO] Scheduler closing all comms\n",
      "msa_component-1                                 | [2024-06-24 15:19:02,339 | fondant.component.executor | INFO] Saving output manifest to /.fondant/feature_extraction_pipeline/feature_extraction_pipeline-20240624171820/msa_component/manifest.json\n",
      "msa_component-1                                 | [2024-06-24 15:19:02,339 | fondant.component.executor | INFO] Writing cache key with manifest reference to /.fondant/feature_extraction_pipeline/cache/e899a2ad0f94efe6bc5fbf060cf98440.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[Kmsa_component-1 exited with code 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "peptide_features_component-1                    | [2024-06-24 15:19:04,093 | fondant.cli | INFO] Component `PeptideFeaturesComponent` found in module main\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:04,099 | fondant.component.executor | INFO] Caching disabled for the component\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:04,099 | root | INFO] Executing component\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:04,365 | distributed.http.proxy | INFO] To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:04,385 | distributed.scheduler | INFO] State start\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:04,389 | distributed.scheduler | INFO]   Scheduler at:     tcp://127.0.0.1:38037\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:04,390 | distributed.scheduler | INFO]   dashboard at:  http://127.0.0.1:8787/status\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:04,390 | distributed.scheduler | INFO] Registering Worker plugin shuffle\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:04,410 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:41467'\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:04,416 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:36433'\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:04,419 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:40565'\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:04,420 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:46749'\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:04,434 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:33223'\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:04,436 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:37847'\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:04,439 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:33077'\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:04,441 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:44949'\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,752 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:36337\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,752 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:36337\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,752 | distributed.worker | INFO]           Worker name:                          1\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,752 | distributed.worker | INFO]          dashboard at:            127.0.0.1:34989\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,752 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:38037\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,752 | distributed.worker | INFO] -------------------------------------------------\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,752 | distributed.worker | INFO]               Threads:                          1\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,752 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,752 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-iyjplxzd\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,752 | distributed.worker | INFO] -------------------------------------------------\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,758 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:36337', name: 1, status: init, memory: 0, processing: 0>\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,769 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:36337\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,769 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:36990\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,771 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,772 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:38037\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,772 | distributed.worker | INFO] -------------------------------------------------\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,776 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:38037\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,780 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:40225\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,781 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:40225\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,781 | distributed.worker | INFO]           Worker name:                          6\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,782 | distributed.worker | INFO]          dashboard at:            127.0.0.1:33617\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,783 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:38037\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,783 | distributed.worker | INFO] -------------------------------------------------\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,783 | distributed.worker | INFO]               Threads:                          1\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,783 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,783 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-_5ln_zia\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,783 | distributed.worker | INFO] -------------------------------------------------\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,789 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:40225', name: 6, status: init, memory: 0, processing: 0>\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,790 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:40225\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,790 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:37002\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,791 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,792 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:38037\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,792 | distributed.worker | INFO] -------------------------------------------------\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,792 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:38037\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,797 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:45101\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,797 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:45101\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,797 | distributed.worker | INFO]           Worker name:                          3\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,797 | distributed.worker | INFO]          dashboard at:            127.0.0.1:46013\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,797 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:38037\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,797 | distributed.worker | INFO] -------------------------------------------------\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,797 | distributed.worker | INFO]               Threads:                          1\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,797 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,798 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-8kn9wf65\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,798 | distributed.worker | INFO] -------------------------------------------------\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,805 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:45101', name: 3, status: init, memory: 0, processing: 0>\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,806 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:45101\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,806 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:37018\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,806 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,807 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:38037\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,807 | distributed.worker | INFO] -------------------------------------------------\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,809 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:38037\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,810 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:38769\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,810 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:38769\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,811 | distributed.worker | INFO]           Worker name:                          4\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,811 | distributed.worker | INFO]          dashboard at:            127.0.0.1:43675\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,811 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:38037\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,811 | distributed.worker | INFO] -------------------------------------------------\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,811 | distributed.worker | INFO]               Threads:                          1\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,811 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,811 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-kpxk54pl\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,811 | distributed.worker | INFO] -------------------------------------------------\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,814 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:37215\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,814 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:37215\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,814 | distributed.worker | INFO]           Worker name:                          5\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,814 | distributed.worker | INFO]          dashboard at:            127.0.0.1:42889\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,815 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:38037\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,815 | distributed.worker | INFO] -------------------------------------------------\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,815 | distributed.worker | INFO]               Threads:                          1\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,815 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,815 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-mlidbtvb\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,815 | distributed.worker | INFO] -------------------------------------------------\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,817 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:38769', name: 4, status: init, memory: 0, processing: 0>\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,818 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:38769\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,818 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:40107\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,819 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:37028\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,819 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:40107\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,819 | distributed.worker | INFO]           Worker name:                          7\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,819 | distributed.worker | INFO]          dashboard at:            127.0.0.1:40541\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,819 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:38037\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,819 | distributed.worker | INFO] -------------------------------------------------\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,819 | distributed.worker | INFO]               Threads:                          1\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,819 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,819 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-449v_bwx\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,819 | distributed.worker | INFO] -------------------------------------------------\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,819 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,820 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:37215', name: 5, status: init, memory: 0, processing: 0>\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,820 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:38037\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,820 | distributed.worker | INFO] -------------------------------------------------\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,820 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:37215\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,820 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:37030\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,821 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:38037\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,821 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,822 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:38037\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,822 | distributed.worker | INFO] -------------------------------------------------\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,822 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:38037\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,824 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:40107', name: 7, status: init, memory: 0, processing: 0>\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,825 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:40107\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,825 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:37040\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,825 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,826 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:38037\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,826 | distributed.worker | INFO] -------------------------------------------------\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,826 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:38037\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,905 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:35167\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,905 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:35167\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,905 | distributed.worker | INFO]           Worker name:                          0\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,905 | distributed.worker | INFO]          dashboard at:            127.0.0.1:36969\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,905 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:38037\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,906 | distributed.worker | INFO] -------------------------------------------------\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,906 | distributed.worker | INFO]               Threads:                          1\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,906 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,906 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-iygo97uv\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,906 | distributed.worker | INFO] -------------------------------------------------\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,909 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:44221\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,909 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:44221\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,909 | distributed.worker | INFO]           Worker name:                          2\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,909 | distributed.worker | INFO]          dashboard at:            127.0.0.1:38723\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,909 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:38037\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,909 | distributed.worker | INFO] -------------------------------------------------\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,909 | distributed.worker | INFO]               Threads:                          1\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,909 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,909 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-k666q7ki\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,909 | distributed.worker | INFO] -------------------------------------------------\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,910 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:35167', name: 0, status: init, memory: 0, processing: 0>\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,910 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:35167\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,910 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:37056\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,911 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,911 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:38037\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,911 | distributed.worker | INFO] -------------------------------------------------\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,912 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:38037\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,915 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:44221', name: 2, status: init, memory: 0, processing: 0>\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,915 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:44221\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,916 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:37068\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,916 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,917 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:38037\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,917 | distributed.worker | INFO] -------------------------------------------------\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,917 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:38037\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,940 | distributed.scheduler | INFO] Receive client connection: Client-18a11f60-323d-11ef-8001-0242ac120002\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,940 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:37072\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,960 | fondant.component.data_io | INFO] The number of partitions of the input dataframe is 1. The available number of workers is 8.\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,961 | fondant.component.data_io | INFO] Repartitioning the data to 8 partitions before processing to maximize worker usage\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:05,961 | root | INFO] Columns of dataframe: ['sequence']\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:06,017 | root | INFO] Creating write task for: /.fondant/feature_extraction_pipeline/feature_extraction_pipeline-20240624171820/peptide_features_component\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:06,017 | root | INFO] Writing data...\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:06,265 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:41467'. Reason: nanny-close\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:06,265 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:06,265 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:36433'. Reason: nanny-close\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:06,266 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:06,266 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:40565'. Reason: nanny-close\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:06,267 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:06,267 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:46749'. Reason: nanny-close\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:06,268 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:35167. Reason: nanny-close\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:06,268 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:06,269 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:33223'. Reason: nanny-close\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:06,269 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:36337. Reason: nanny-close\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:06,269 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:44221. Reason: nanny-close\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:06,269 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:06,270 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:37847'. Reason: nanny-close\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:06,270 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:45101. Reason: nanny-close\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:06,271 | distributed.core | INFO] Connection to tcp://127.0.0.1:38037 has been closed.\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:06,271 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:06,271 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:33077'. Reason: nanny-close\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:06,271 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:38769. Reason: nanny-close\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:06,271 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:06,272 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:44949'. Reason: nanny-close\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:06,272 | distributed.core | INFO] Connection to tcp://127.0.0.1:38037 has been closed.\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:06,273 | distributed.core | INFO] Connection to tcp://127.0.0.1:38037 has been closed.\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:06,273 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:06,274 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:37215. Reason: nanny-close\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:06,274 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:40225. Reason: nanny-close\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:06,274 | distributed.core | INFO] Connection to tcp://127.0.0.1:38037 has been closed.\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:06,275 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:40107. Reason: nanny-close\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:06,275 | distributed.core | INFO] Connection to tcp://127.0.0.1:38037 has been closed.\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:06,277 | distributed.core | INFO] Connection to tcp://127.0.0.1:38037 has been closed.\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:06,278 | distributed.core | INFO] Connection to tcp://127.0.0.1:38037 has been closed.\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:06,279 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:37056; closing.\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:06,281 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:36990; closing.\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:06,281 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:37068; closing.\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:06,281 | distributed.core | INFO] Connection to tcp://127.0.0.1:38037 has been closed.\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:06,282 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:37018; closing.\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:06,282 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:37028; closing.\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:06,284 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:35167', name: 0, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1719242346.2844787')\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:06,286 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:36337', name: 1, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1719242346.2858605')\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:06,286 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:44221', name: 2, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1719242346.286854')\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:06,287 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:45101', name: 3, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1719242346.287734')\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:06,288 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:38769', name: 4, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1719242346.288585')\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:06,289 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:37030; closing.\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:06,290 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:37002; closing.\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:06,294 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:37040; closing.\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:06,298 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:37215', name: 5, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1719242346.297815')\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:06,299 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:40225', name: 6, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1719242346.2991934')\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:06,300 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:40107', name: 7, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1719242346.3002932')\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:06,300 | distributed.scheduler | INFO] Lost all workers\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:06,306 | distributed.batched | INFO] Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:38037 remote=tcp://127.0.0.1:37030>\n",
      "peptide_features_component-1                    | Traceback (most recent call last):\n",
      "peptide_features_component-1                    |   File \"/usr/local/lib/python3.11/site-packages/distributed/batched.py\", line 115, in _background_send\n",
      "peptide_features_component-1                    |     nbytes = yield coro\n",
      "peptide_features_component-1                    |              ^^^^^^^^^^\n",
      "peptide_features_component-1                    |   File \"/usr/local/lib/python3.11/site-packages/tornado/gen.py\", line 767, in run\n",
      "peptide_features_component-1                    |     value = future.result()\n",
      "peptide_features_component-1                    |             ^^^^^^^^^^^^^^^\n",
      "peptide_features_component-1                    |   File \"/usr/local/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 262, in write\n",
      "peptide_features_component-1                    |     raise CommClosedError()\n",
      "peptide_features_component-1                    | distributed.comm.core.CommClosedError\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:06,309 | distributed.batched | INFO] Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:38037 remote=tcp://127.0.0.1:37040>\n",
      "peptide_features_component-1                    | Traceback (most recent call last):\n",
      "peptide_features_component-1                    |   File \"/usr/local/lib/python3.11/site-packages/distributed/batched.py\", line 115, in _background_send\n",
      "peptide_features_component-1                    |     nbytes = yield coro\n",
      "peptide_features_component-1                    |              ^^^^^^^^^^\n",
      "peptide_features_component-1                    |   File \"/usr/local/lib/python3.11/site-packages/tornado/gen.py\", line 767, in run\n",
      "peptide_features_component-1                    |     value = future.result()\n",
      "peptide_features_component-1                    |             ^^^^^^^^^^^^^^^\n",
      "peptide_features_component-1                    |   File \"/usr/local/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 262, in write\n",
      "peptide_features_component-1                    |     raise CommClosedError()\n",
      "peptide_features_component-1                    | distributed.comm.core.CommClosedError\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:06,310 | distributed.batched | INFO] Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:38037 remote=tcp://127.0.0.1:37002>\n",
      "peptide_features_component-1                    | Traceback (most recent call last):\n",
      "peptide_features_component-1                    |   File \"/usr/local/lib/python3.11/site-packages/distributed/batched.py\", line 115, in _background_send\n",
      "peptide_features_component-1                    |     nbytes = yield coro\n",
      "peptide_features_component-1                    |              ^^^^^^^^^^\n",
      "peptide_features_component-1                    |   File \"/usr/local/lib/python3.11/site-packages/tornado/gen.py\", line 767, in run\n",
      "peptide_features_component-1                    |     value = future.result()\n",
      "peptide_features_component-1                    |             ^^^^^^^^^^^^^^^\n",
      "peptide_features_component-1                    |   File \"/usr/local/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 262, in write\n",
      "peptide_features_component-1                    |     raise CommClosedError()\n",
      "peptide_features_component-1                    | distributed.comm.core.CommClosedError\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:06,649 | distributed.scheduler | INFO] Scheduler closing due to unknown reason...\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:06,650 | distributed.scheduler | INFO] Scheduler closing all comms\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:06,654 | fondant.component.executor | INFO] Saving output manifest to /.fondant/feature_extraction_pipeline/feature_extraction_pipeline-20240624171820/peptide_features_component/manifest.json\n",
      "peptide_features_component-1                    | [2024-06-24 15:19:06,654 | fondant.component.executor | INFO] Writing cache key with manifest reference to /.fondant/feature_extraction_pipeline/cache/628e35a1062d2bdd6163bd1f7b6fac25.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[Kpeptide_features_component-1 exited with code 0\n",
      "Finished pipeline run.\n"
     ]
    }
   ],
   "source": [
    "# import shutil\n",
    "\n",
    "# remove the most recent output folder if the manifest file is removed\n",
    "# without a manifest file in the most recent output folder, the pipeline cannot be run\n",
    "# if OUTPUT_FOLDER and REMOVED_MANIFEST:\n",
    "# \tshutil.rmtree(OUTPUT_FOLDER)\n",
    "# \t# remove cache\n",
    "# \tshutil.rmtree(os.path.join(BASE_PATH, PIPELINE_NAME, \"cache\"))\n",
    "\n",
    "# get current full path to the project\n",
    "mounted_data = os.path.join(os.path.abspath(\"data\"), \":/data\")\n",
    "\n",
    "DockerRunner().run(input=pipeline, extra_volumes=mounted_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "The following results have been taken from the output of the pipeline, which is stored in the `.fondant` directory. This directory contains the output of each component, together with the cache of the previous run. Currently, the pipeline doesn't implement the `write_to_file` component, so the results will be taken individually from the output of each component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-24 17:19:07,100 | root | INFO] Last folder: .fondant/feature_extraction_pipeline/feature_extraction_pipeline-20240624171820\n"
     ]
    }
   ],
   "source": [
    "# find the most recent output folder\n",
    "# get the most recent folder in the folder named: BASE_PATH + PIPELINE_NAME + PIPELINE_NAME-<timestamp>\n",
    "matching_folders = glob.glob(f\"{BASE_PATH}/{PIPELINE_NAME}/{PIPELINE_NAME}-*\")\n",
    "\n",
    "if matching_folders:\n",
    "    last_folder = max(matching_folders, key=os.path.getctime)\n",
    "\n",
    "logging.info(f\"Last folder: {last_folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def merge_parquet_folders(folder_path):\n",
    "    df_list = []\n",
    "\n",
    "    for folder in Path(folder_path).iterdir():\n",
    "        if folder.is_dir():\n",
    "            logging.info(f\"Reading parquet partitions from: {folder}\")\n",
    "            parquet_files = list(folder.glob(\"*.parquet\"))\n",
    "            logging.info(f\"Found {len(parquet_files)} parquet files\")\n",
    "            dfs = [pd.read_parquet(file) for file in parquet_files]\n",
    "            dfs = [x for x in dfs if not x.empty]\n",
    "            if len(dfs) == 0:\n",
    "                continue\n",
    "            df = pd.concat(dfs)\n",
    "            df_list.append(df)\n",
    "\n",
    "    return df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-24 17:19:07,128 | root | INFO] Reading parquet partitions from: .fondant/feature_extraction_pipeline/feature_extraction_pipeline-20240624171820/filter_pdb_component\n",
      "[2024-06-24 17:19:07,129 | root | INFO] Found 8 parquet files\n",
      "[2024-06-24 17:19:07,157 | root | INFO] Reading parquet partitions from: .fondant/feature_extraction_pipeline/feature_extraction_pipeline-20240624171820/predict_protein_3d_structure_component\n",
      "[2024-06-24 17:19:07,158 | root | INFO] Found 8 parquet files\n",
      "[2024-06-24 17:19:07,182 | root | INFO] Reading parquet partitions from: .fondant/feature_extraction_pipeline/feature_extraction_pipeline-20240624171820/biopython_component\n",
      "[2024-06-24 17:19:07,183 | root | INFO] Found 8 parquet files\n",
      "[2024-06-24 17:19:07,206 | root | INFO] Reading parquet partitions from: .fondant/feature_extraction_pipeline/feature_extraction_pipeline-20240624171820/load_from_parquet\n",
      "[2024-06-24 17:19:07,207 | root | INFO] Found 0 parquet files\n",
      "[2024-06-24 17:19:07,207 | root | INFO] Reading parquet partitions from: .fondant/feature_extraction_pipeline/feature_extraction_pipeline-20240624171820/ifeatureomega_component\n",
      "[2024-06-24 17:19:07,208 | root | INFO] Found 2 parquet files\n",
      "[2024-06-24 17:19:07,220 | root | INFO] Reading parquet partitions from: .fondant/feature_extraction_pipeline/feature_extraction_pipeline-20240624171820/generate_protein_sequence_checksum_component\n",
      "[2024-06-24 17:19:07,221 | root | INFO] Found 8 parquet files\n",
      "[2024-06-24 17:19:07,235 | root | INFO] Reading parquet partitions from: .fondant/feature_extraction_pipeline/feature_extraction_pipeline-20240624171820/peptide_features_component\n",
      "[2024-06-24 17:19:07,236 | root | INFO] Found 8 parquet files\n",
      "[2024-06-24 17:19:07,255 | root | INFO] Reading parquet partitions from: .fondant/feature_extraction_pipeline/feature_extraction_pipeline-20240624171820/store_pdb_component\n",
      "[2024-06-24 17:19:07,255 | root | INFO] Found 8 parquet files\n",
      "[2024-06-24 17:19:07,271 | root | INFO] Reading parquet partitions from: .fondant/feature_extraction_pipeline/feature_extraction_pipeline-20240624171820/msa_component\n",
      "[2024-06-24 17:19:07,272 | root | INFO] Found 1 parquet files\n"
     ]
    }
   ],
   "source": [
    "dataframe_list = merge_parquet_folders(last_folder)\n",
    "\n",
    "\n",
    "df_final = pd.concat(dataframe_list, axis=1)\n",
    "df_final = df_final.loc[:,~df_final.columns.duplicated()]\n",
    "\n",
    "# filtering out columns that are not properly stored in a csv\n",
    "columns_to_remove = ['pdb_string']\n",
    "df_final = df_final.drop(columns=columns_to_remove)\n",
    "\n",
    "# write to file\n",
    "df_final.to_csv(f\"{last_folder}/final_output.csv\", index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "protein-feature-extraction-NoVdeDG9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
