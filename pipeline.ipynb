{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protein feature extraction pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will contain the pipeline for extracting features from protein sequences. It will be used as a way to show the output without needing to run the `pipeline.py` file locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import logging\n",
    "# from fondant.pipeline import Pipeline\n",
    "from fondant.dataset import Dataset\n",
    "from fondant.dataset.runner import DockerRunner\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "from config import MOCK_DATA_PATH_FONDANT\n",
    "\n",
    "# check if the manifest file is removed.\n",
    "REMOVED_MANIFEST = False\n",
    "\n",
    "# check if the output folder exists\n",
    "OUTPUT_FOLDER = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Mock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python utils/generate_mock_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show content of the mock data\n",
    "import pandas as pd\n",
    "mock_df = pd.read_parquet(\".\" + MOCK_DATA_PATH_FONDANT)  # dot added to make it relative to the current directory\n",
    "mock_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new pipeline\n",
    "\n",
    "BASE_PATH = \".\"\n",
    "DATASET_NAME = \"feature_extraction_pipeline\"\n",
    "\n",
    "dataset = Dataset.create(\n",
    "    \"load_from_parquet\",\n",
    "    arguments={\n",
    "        \"dataset_uri\": MOCK_DATA_PATH_FONDANT,\n",
    "    },\n",
    "    produces={\n",
    "        \"sequence\": pa.string()\n",
    "    },\n",
    "    dataset_name=DATASET_NAME\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components\n",
    "\n",
    "---\n",
    "\n",
    "### generate_protein_sequence_checksum_component\n",
    "\n",
    "This component generates a checksum for the protein sequence.\n",
    "\n",
    "---\n",
    "\n",
    "### biopython_component\n",
    "\n",
    "Extracts features from the protein sequence using Biopython.\n",
    "\n",
    "---\n",
    "\n",
    "### iFeatureOmega_component\n",
    "\n",
    "Extracts features from the protein sequence using the [iFeatureOmega-CLI GitHub repo](https://github.com/Superzchen/iFeatureOmega-CLI). Arguments are used to specify the type of features to extract.\n",
    "\n",
    "---\n",
    "\n",
    "### filter_pdb_component\n",
    "\n",
    "Filters PDB files that are already predicted to avoid redundant predictions. Arguments need to be specified before running the pipeline:\n",
    "```json\n",
    "\"storage_type\": \"local\",\n",
    "\"pdb_path\": \"/data/<your-pdb-folder-path>\",\n",
    "\"bucket_name\": \"your-bucket-name\",\n",
    "\"project_id\": \"your-project-id\",\n",
    "\"google_cloud_credentials_path\": \"/data/<your-credentials>.json\"\n",
    "```\n",
    "\n",
    "If only using local, keep bucket_name, project_id, and google_cloud_credentials_path as empty strings. Using remote requires a Google Cloud Storage bucket with credentials and a project ID.\n",
    "\n",
    "---\n",
    "\n",
    "### predict_protein_3D_structure_component\n",
    "\n",
    "Predicts the 3D structure of the protein using ESMFold. This component requires a `.env` file with the following variables:\n",
    "```env\n",
    "HF_API_KEY=\"\"\n",
    "HF_ENDPOINT_URL=\"\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### store_pdb_component\n",
    "\n",
    "Stores the PDB files in the provided storage_type. Arguments need to be specified before running the pipeline:\n",
    "```json\n",
    "\"storage_type\": \"local\",\n",
    "\"pdb_path\": \"/data/<your-pdb-folder-path>\",\n",
    "\"bucket_name\": \"your-bucket-name\",\n",
    "\"project_id\": \"your-project-id\",\n",
    "\"google_cloud_credentials_path\": \"/data/<your-credentials>.json\"\n",
    "```\n",
    "\n",
    "If only using local, keep bucket_name, project_id, and google_cloud_credentials_path as empty strings. Using remote requires a Google Cloud Storage bucket with credentials and a project ID.\n",
    "\n",
    "---\n",
    "\n",
    "### msa_component\n",
    "\n",
    "Generates the multiple sequence alignment for the protein sequence using [Clustal Omega](http://www.clustal.org/omega/). It's recommended to use a smaller number of sequences or none at all due to potential time consumption.\n",
    "\n",
    "---\n",
    "\n",
    "### unikp_component\n",
    "\n",
    "Uses the UniKP endpoint on HuggingFace to predict the kinetic parameters of a protein sequence and substrate (SMILES) combination. See README for the description of the contents of this file.\n",
    "\n",
    "```yaml\n",
    "\"protein_smiles_path\": \"/data/<path_protein_smiles>\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### peptide_component\n",
    "\n",
    "Calculates the features from the protein sequence using the `peptides` package.\n",
    "\n",
    "---\n",
    "\n",
    "### deepTMpred_component\n",
    "\n",
    "Predicts the transmembrane regions of the protein sequence using the [DeepTMpred GitHub repository](https://github.com/ISYSLAB-HUST/DeepTMpred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.apply(\n",
    "    \"./components/biopython_component\",\n",
    "    cache=False\n",
    ").apply(\n",
    "    \"./components/generate_protein_sequence_checksum_component\",\n",
    "    cache=False\n",
    ").apply(\n",
    "    \"./components/iFeatureOmega_component\",\n",
    "    # currently forcing the number of rows to 5, but there needs to be a better way to do this, see readme for more info\n",
    "    input_partition_rows=5,\n",
    "    arguments={\n",
    "        \"descriptors\": [\"AAC\", \"CTDC\", \"CTDT\"]\n",
    "    }\n",
    ").apply(\n",
    "    \"./components/filter_pdb_component\",\n",
    "    arguments={\n",
    "        \"method\": \"local\",\n",
    "        \"local_pdb_path\": \"/data/pdb_files\",\n",
    "        \"bucket_name\": \"\",\n",
    "        \"project_id\": \"\",\n",
    "        \"google_cloud_credentials_path\": \"\"\n",
    "    }\n",
    ").apply(\n",
    "    \"./components/predict_protein_3D_structure_component\",\n",
    ").apply(\n",
    "    \"./components/store_pdb_component\",\n",
    "    arguments={\n",
    "        \"method\": \"local\",\n",
    "        \"local_pdb_path\": \"/data/pdb_files/\",\n",
    "        \"bucket_name\": \"elated-chassis-400207_dbtl_pipeline_outputs\",\n",
    "        \"project_id\": \"elated-chassis-400207\",\n",
    "        \"google_cloud_credentials_path\": \"/data/google_cloud_credentials.json\"\n",
    "    }\n",
    ").apply(\n",
    "    \"./components/msa_component\",\n",
    "    input_partition_rows='10000'\n",
    "# ).apply(\n",
    "#     \"./components/pdb_features_component\"\n",
    "# ).apply(\n",
    "#     \"./components/unikp_component\",\n",
    "#     arguments={\n",
    "#         \"target_molecule_smiles\": \"/data/target_molecule_smiles.json\",\n",
    "#     },\n",
    "# ).apply(\n",
    "#     \"./components/peptide_features_component\"\n",
    "# ).apply(\n",
    "#     \"./components/DeepTMpred_component\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pipeline.py` file needs to be run using the command line. The following command will run the pipeline:\n",
    "\n",
    "```bash\n",
    "fondant < full_path_to_pipeline.py >\\data:/data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "\n",
    "# remove the most recent output folder if the manifest file is removed\n",
    "# without a manifest file in the most recent output folder, the pipeline cannot be run\n",
    "# if OUTPUT_FOLDER and REMOVED_MANIFEST:\n",
    "# \tshutil.rmtree(OUTPUT_FOLDER)\n",
    "# \t# remove cache\n",
    "# \tshutil.rmtree(os.path.join(BASE_PATH, PIPELINE_NAME, \"cache\"))\n",
    "\n",
    "# get current full path to the project\n",
    "mounted_data = os.path.join(os.path.abspath(\"data\"), \":/data\")\n",
    "\n",
    "DockerRunner().run(dataset=dataset, working_directory=BASE_PATH, extra_volumes=mounted_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "The following results have been taken from the output of the pipeline, which is stored in the `.fondant` directory. This directory contains the output of each component, together with the cache of the previous run. Currently, the pipeline doesn't implement the `write_to_file` component, so the results will be taken individually from the output of each component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the most recent output folder\n",
    "# get the most recent folder in the folder named: BASE_PATH + PIPELINE_NAME + PIPELINE_NAME-<timestamp>\n",
    "matching_folders = glob.glob(f\"{BASE_PATH}/{DATASET_NAME}/{DATASET_NAME}-*\")\n",
    "\n",
    "if matching_folders:\n",
    "    last_folder = max(matching_folders, key=os.path.getctime)\n",
    "\n",
    "logging.info(f\"Last folder: {last_folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def merge_parquet_folders(folder_path):\n",
    "    df_list = []\n",
    "\n",
    "    for folder in Path(folder_path).iterdir():\n",
    "        if folder.is_dir():\n",
    "            logging.info(f\"Reading parquet partitions from: {folder}\")\n",
    "            parquet_files = list(folder.glob(\"*.parquet\"))\n",
    "            logging.info(f\"Found {len(parquet_files)} parquet files\")\n",
    "            dfs = [pd.read_parquet(file) for file in parquet_files]\n",
    "            dfs = [x for x in dfs if not x.empty]\n",
    "            if len(dfs) == 0:\n",
    "                continue\n",
    "            df = pd.concat(dfs)\n",
    "            df_list.append(df)\n",
    "\n",
    "    return df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_list = merge_parquet_folders(last_folder)\n",
    "\n",
    "\n",
    "df_final = pd.concat(dataframe_list, axis=1)\n",
    "df_final = df_final.loc[:,~df_final.columns.duplicated()]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final[[\"sequence\", \"pdb_string\", \"msa_sequence\"]].to_json(\"test_json.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering out columns that are not properly stored in a csv\n",
    "columns_to_remove = ['pdb_string']\n",
    "df_final = df_final.drop(columns=columns_to_remove)\n",
    "\n",
    "# write to file\n",
    "df_final.to_csv(f\"{last_folder}/final_output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "protein-feature-extraction-NoVdeDG9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
