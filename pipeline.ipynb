{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protein feature extraction pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will contain the pipeline for extracting features from protein sequences. It will be used as a way to show the output without needing to run the `pipeline.py` file locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "from fondant.pipeline import Pipeline\n",
    "import os\n",
    "from config import MOCK_DATA_PATH_FONDANT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Mock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['python', 'utils/generate_mock_data.py'], returncode=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "subprocess.run([\"python\", \"utils/generate_mock_data.py\"], shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MNQRGMPIQSLVTNVKINRLEENDCIHTRHRVRPGRTDGKNLHAMM...</td>\n",
       "      <td>Seq1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MAGLKPEVPLHDGINKFGKSDFAGQEGPKIVTTTDKALLVANGALK...</td>\n",
       "      <td>Seq2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MVDLKKELKNFVDSDFPGSPKQEAQGIDVRILLSFNNAAFREALII...</td>\n",
       "      <td>Seq3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MELILAKARLEFECDWGLLMLEPCVPPTKIFADRNYAVGVMFESDK...</td>\n",
       "      <td>Seq4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MRVLCDGSTGYACAKNTRIRFREKVASVLAKIQGYEQTFPHHMPNM...</td>\n",
       "      <td>Seq5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sequence  name\n",
       "0  MNQRGMPIQSLVTNVKINRLEENDCIHTRHRVRPGRTDGKNLHAMM...  Seq1\n",
       "1  MAGLKPEVPLHDGINKFGKSDFAGQEGPKIVTTTDKALLVANGALK...  Seq2\n",
       "2  MVDLKKELKNFVDSDFPGSPKQEAQGIDVRILLSFNNAAFREALII...  Seq3\n",
       "3  MELILAKARLEFECDWGLLMLEPCVPPTKIFADRNYAVGVMFESDK...  Seq4\n",
       "4  MRVLCDGSTGYACAKNTRIRFREKVASVLAKIQGYEQTFPHHMPNM...  Seq5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show content of the mock data\n",
    "import pandas as pd\n",
    "df = pd.read_parquet(\".\" + MOCK_DATA_PATH_FONDANT)  # dot added to make it relative to the current directory\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new pipeline\n",
    "\n",
    "BASE_PATH = \".fondant\"\n",
    "PIPELINE_NAME = \"feature_extraction_pipeline\"\n",
    "\n",
    "pipeline = Pipeline(\n",
    "\tname=PIPELINE_NAME,\n",
    "\tbase_path=BASE_PATH,\n",
    "\tdescription=\"A pipeline to extract features from protein sequences.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "\n",
    "dataset = pipeline.read(\n",
    "\t\"load_from_parquet\",\n",
    "\targuments={\n",
    "\t\t\"dataset_uri\": MOCK_DATA_PATH_FONDANT,\n",
    "\t},\n",
    "\tproduces={\n",
    "\t\t\"sequence\": pa.string()\n",
    "\t}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components\n",
    "\n",
    "This section will contain the components that will be used in the pipeline.\n",
    "\n",
    "These are the components that will be used in the pipeline:\n",
    "\n",
    "- `generate_protein_sequence_checksum_component`: This component will generate a checksum for the protein sequence.\n",
    "\n",
    "- `biopython_component`: This component will extract features from the protein sequence using Biopython.\n",
    "\n",
    "- `iFeatureOmega_component`: This component will extract features from the protein sequence using iFeature Omega. This component uses arguments to specify the type of features to extract.\n",
    "\n",
    "- `filter_pdb_component`: This component will filter the PDB files that are already predicted, so the pipeline doesn't need to predict them again. You'll need to specify the following arguments before running the pipeline:\n",
    "```json\n",
    "\"storage_type\": \"local\",\n",
    "\"pdb_path\": \"/data/pdb_files/\",\n",
    "\"bucket_name\": \"your-bucket-name\",\n",
    "\"project_id\": \"your-project-id\",\n",
    "\"google_cloud_credentials_path\": \"/data/google_cloud_credentials.json\"\n",
    "```\n",
    "\n",
    "If you're only using `local`, then you can keep the `bucket_name`, `project_id` and `google_cloud_credentials_path` as empty strings. Using `remote` will require you to have a Google Cloud Storage bucket with credentials and a project ID.\n",
    "\n",
    "- `predict_protein_3D_structure_component`: This component will predict the 3D structure of the protein using ESMFold.\n",
    "\n",
    "- `store_pdb_component`: This component will store the PDB files in the provided `storage_type`. You'll need to specify the following arguments before running the pipeline:\n",
    "```json\n",
    "\"storage_type\": \"local\",\n",
    "\"pdb_path\": \"/data/pdb_files/\",\n",
    "\"bucket_name\": \"your-bucket-name\",\n",
    "\"project_id\": \"your-project-id\",\n",
    "\"google_cloud_credentials_path\": \"/data/google_cloud_credentials.json\"\n",
    "```\n",
    "\n",
    "If you're only using `local`, then you can keep the `bucket_name`, `project_id` and `google_cloud_credentials_path` as empty strings. Using `remote` will require you to have a Google Cloud Storage bucket with credentials and a project ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-04-16 15:36:50,523 | fondant.pipeline.pipeline | WARNING] Component `Biopython component` has an image tag set to latest. Caching for the component will be disabled to prevent unpredictable behavior due to images updates\n",
      "[2024-04-16 15:36:50,532 | fondant.pipeline.pipeline | WARNING] Component `Generate Protein Sequence Checksum Component` has an image tag set to latest. Caching for the component will be disabled to prevent unpredictable behavior due to images updates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-04-16 15:36:50,584 | fondant.pipeline.pipeline | WARNING] Component `iFeatureOmega component` has an image tag set to latest. Caching for the component will be disabled to prevent unpredictable behavior due to images updates\n"
     ]
    }
   ],
   "source": [
    "# Apply the components to the dataset\n",
    "\n",
    "_ = dataset.apply(\n",
    "\t\"./components/biopython_component\"\n",
    ").apply(\n",
    "\t\"./components/generate_protein_sequence_checksum_component\"\n",
    ").apply(\n",
    "\t\"./components/iFeatureOmega_component\",\n",
    "\t# currently forcing the number of rows to 5, see readme for more info\n",
    "\tinput_partition_rows=5,\n",
    "\targuments={\n",
    "\t\t\"descriptors\": [\"AAC\", \"GAAC\", \"Moran\", \"Geary\", \"NMBroto\", \"APAAC\"]\n",
    "\t}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pipeline.py` file needs to be run using the command line. The following command will run the pipeline:\n",
    "\n",
    "```bash\n",
    "fondant < full_path_to_pipeline.py >\\data:/data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-04-16 15:36:53,062 | root | INFO] Found reference to un-compiled pipeline... compiling\n",
      "[2024-04-16 15:36:53,063 | fondant.pipeline.compiler | INFO] Compiling feature_extraction_pipeline to .fondant/compose.yaml\n",
      "[2024-04-16 15:36:53,063 | fondant.pipeline.compiler | INFO] Base path found on local system, setting up .fondant as mount volume\n",
      "[2024-04-16 15:36:53,065 | fondant.pipeline.pipeline | INFO] Sorting pipeline component graph topologically.\n",
      "[2024-04-16 15:36:53,069 | fondant.pipeline.pipeline | INFO] All pipeline component specifications match.\n",
      "[2024-04-16 15:36:53,070 | fondant.pipeline.compiler | INFO] Compiling service for load_from_parquet\n",
      "[2024-04-16 15:36:53,071 | fondant.pipeline.compiler | INFO] Compiling service for biopython_component\n",
      "[2024-04-16 15:36:53,072 | fondant.pipeline.compiler | INFO] Found Dockerfile for biopython_component, adding build step.\n",
      "[2024-04-16 15:36:53,073 | fondant.pipeline.compiler | INFO] Compiling service for generate_protein_sequence_checksum_component\n",
      "[2024-04-16 15:36:53,074 | fondant.pipeline.compiler | INFO] Found Dockerfile for generate_protein_sequence_checksum_component, adding build step.\n",
      "[2024-04-16 15:36:53,075 | fondant.pipeline.compiler | INFO] Compiling service for ifeatureomega_component\n",
      "[2024-04-16 15:36:53,076 | fondant.pipeline.compiler | INFO] Found Dockerfile for ifeatureomega_component, adding build step.\n",
      "[2024-04-16 15:36:53,097 | fondant.pipeline.compiler | INFO] Successfully compiled to .fondant/compose.yaml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pipeline run...\n",
      "Finished pipeline run.\n"
     ]
    }
   ],
   "source": [
    "from fondant.pipeline.runner import DockerRunner\n",
    "\n",
    "# get current full path to the project\n",
    "mounted_data = os.path.join(os.path.abspath(\"data\"), \":/data\")\n",
    "\n",
    "DockerRunner().run(input=pipeline, extra_volumes=mounted_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "The following results have been taken from the output of the pipeline, which is stored in the `.fondant` directory. This directory contains the output of each component, together with the cache of the previous run. Currently, the pipeline doesn't implement the `write_to_file` component, so the results will be taken individually from the output of each component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# get the most recent folder in the folder named: BASE_PATH + PIPELINE_NAME + PIPELINE_NAME-<timestamp>\n",
    "matching_folders = glob.glob(f\"{BASE_PATH}/{PIPELINE_NAME}/{PIPELINE_NAME}-*\")\n",
    "\n",
    "if matching_folders:\n",
    "    output_folder = max(matching_folders, key=os.path.getctime)\n",
    "else:\n",
    "    print(\"No matching folders found\")\n",
    "    exit()\n",
    "\n",
    "# loop through the folders within that folder and for each folder remove the \"manifest.json\" file\n",
    "for folder in os.listdir(output_folder):\n",
    "\tmanifest_file = os.path.join(output_folder, folder, \"manifest.json\")\n",
    "\tif os.path.exists(manifest_file):\n",
    "\t\tos.remove(manifest_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\denis\\AppData\\Local\\Temp\\ipykernel_18968\\815401560.py:19: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  folder_df = pd.concat([folder_df, df])\n",
      "C:\\Users\\denis\\AppData\\Local\\Temp\\ipykernel_18968\\815401560.py:19: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  folder_df = pd.concat([folder_df, df])\n",
      "C:\\Users\\denis\\AppData\\Local\\Temp\\ipykernel_18968\\815401560.py:19: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  folder_df = pd.concat([folder_df, df])\n",
      "C:\\Users\\denis\\AppData\\Local\\Temp\\ipykernel_18968\\815401560.py:19: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  folder_df = pd.concat([folder_df, df])\n",
      "C:\\Users\\denis\\AppData\\Local\\Temp\\ipykernel_18968\\815401560.py:19: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  folder_df = pd.concat([folder_df, df])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Function to merge the Parquet folders with each other using the sequence column as pivot\n",
    "def merge_parquet_folders(folder_path):\n",
    "\tmerged_df = pd.DataFrame()\n",
    "\t\n",
    "\t# Loop through each folder in the output_folder\n",
    "\tfor folder in os.listdir(folder_path):\n",
    "\t\tparquet_partitions = os.path.join(folder_path, folder)\n",
    "\t\t\n",
    "\t\tfolder_df = pd.DataFrame()\n",
    "\t\t\n",
    "\t\tfor file in os.listdir(parquet_partitions):\n",
    "\t\t\tif file.endswith(\".parquet\"):\n",
    "\t\t\t\tfile_path = os.path.join(parquet_partitions, file)\n",
    "\t\t\t\tdf = pq.read_table(file_path).to_pandas()\t\t\n",
    "\t\t\t\tfolder_df = pd.concat([folder_df, df])\n",
    "\t\t\n",
    "\t\tif not merged_df.empty:\n",
    "\t\t\tmerged_df = pd.merge(merged_df, folder_df, how='outer', on='sequence')\n",
    "\t\telse:\n",
    "\t\t\tmerged_df = folder_df.copy()\n",
    "\t\n",
    "\treturn merged_df\n",
    "\n",
    "merged_df = merge_parquet_folders(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>sequence_length</th>\n",
       "      <th>molecular_weight</th>\n",
       "      <th>aromaticity</th>\n",
       "      <th>isoelectric_point</th>\n",
       "      <th>instability_index</th>\n",
       "      <th>gravy</th>\n",
       "      <th>helix</th>\n",
       "      <th>turn</th>\n",
       "      <th>sheet</th>\n",
       "      <th>...</th>\n",
       "      <th>NMBroto_BEGF750101.lag3</th>\n",
       "      <th>NMBroto_BEGF750102.lag1</th>\n",
       "      <th>NMBroto_BEGF750102.lag2</th>\n",
       "      <th>NMBroto_BEGF750102.lag3</th>\n",
       "      <th>NMBroto_BEGF750103.lag1</th>\n",
       "      <th>NMBroto_BEGF750103.lag2</th>\n",
       "      <th>NMBroto_BEGF750103.lag3</th>\n",
       "      <th>NMBroto_BHAR880101.lag1</th>\n",
       "      <th>NMBroto_BHAR880101.lag2</th>\n",
       "      <th>NMBroto_BHAR880101.lag3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MAGLKPEVPLHDGINKFGKSDFAGQEGPKIVTTTDKALLVANGALK...</td>\n",
       "      <td>400</td>\n",
       "      <td>43254.8112</td>\n",
       "      <td>0.055000</td>\n",
       "      <td>5.964593</td>\n",
       "      <td>38.948025</td>\n",
       "      <td>-0.263750</td>\n",
       "      <td>0.335000</td>\n",
       "      <td>0.322500</td>\n",
       "      <td>0.335000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054463</td>\n",
       "      <td>-0.002630</td>\n",
       "      <td>0.063674</td>\n",
       "      <td>-0.035028</td>\n",
       "      <td>-0.056075</td>\n",
       "      <td>0.069239</td>\n",
       "      <td>-0.028580</td>\n",
       "      <td>0.026617</td>\n",
       "      <td>0.061418</td>\n",
       "      <td>0.047221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MELILAKARLEFECDWGLLMLEPCVPPTKIFADRNYAVGVMFESDK...</td>\n",
       "      <td>350</td>\n",
       "      <td>39615.9422</td>\n",
       "      <td>0.091429</td>\n",
       "      <td>4.825028</td>\n",
       "      <td>40.802857</td>\n",
       "      <td>-0.248000</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.248571</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016027</td>\n",
       "      <td>0.015219</td>\n",
       "      <td>0.038238</td>\n",
       "      <td>-0.181714</td>\n",
       "      <td>0.066448</td>\n",
       "      <td>0.071851</td>\n",
       "      <td>-0.089194</td>\n",
       "      <td>-0.005339</td>\n",
       "      <td>0.032348</td>\n",
       "      <td>-0.036633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MNQRGMPIQSLVTNVKINRLEENDCIHTRHRVRPGRTDGKNLHAMM...</td>\n",
       "      <td>600</td>\n",
       "      <td>66369.0679</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>5.397908</td>\n",
       "      <td>38.074000</td>\n",
       "      <td>-0.334833</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>0.268333</td>\n",
       "      <td>0.328333</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001961</td>\n",
       "      <td>-0.041237</td>\n",
       "      <td>0.100768</td>\n",
       "      <td>-0.049149</td>\n",
       "      <td>0.086146</td>\n",
       "      <td>0.057148</td>\n",
       "      <td>0.028043</td>\n",
       "      <td>-0.021107</td>\n",
       "      <td>0.125624</td>\n",
       "      <td>-0.004387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MRVLCDGSTGYACAKNTRIRFREKVASVLAKIQGYEQTFPHHMPNM...</td>\n",
       "      <td>420</td>\n",
       "      <td>47355.5634</td>\n",
       "      <td>0.088095</td>\n",
       "      <td>5.392736</td>\n",
       "      <td>42.440000</td>\n",
       "      <td>-0.518333</td>\n",
       "      <td>0.319048</td>\n",
       "      <td>0.283333</td>\n",
       "      <td>0.326190</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038768</td>\n",
       "      <td>0.092617</td>\n",
       "      <td>-0.027349</td>\n",
       "      <td>-0.093597</td>\n",
       "      <td>0.083518</td>\n",
       "      <td>0.039504</td>\n",
       "      <td>-0.037585</td>\n",
       "      <td>0.091911</td>\n",
       "      <td>0.019569</td>\n",
       "      <td>0.001394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MVDLKKELKNFVDSDFPGSPKQEAQGIDVRILLSFNNAAFREALII...</td>\n",
       "      <td>550</td>\n",
       "      <td>60158.7220</td>\n",
       "      <td>0.072727</td>\n",
       "      <td>5.349652</td>\n",
       "      <td>38.161636</td>\n",
       "      <td>-0.183273</td>\n",
       "      <td>0.341818</td>\n",
       "      <td>0.296364</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087268</td>\n",
       "      <td>0.032190</td>\n",
       "      <td>0.011671</td>\n",
       "      <td>0.006448</td>\n",
       "      <td>0.069078</td>\n",
       "      <td>0.121472</td>\n",
       "      <td>0.136282</td>\n",
       "      <td>0.044959</td>\n",
       "      <td>0.042260</td>\n",
       "      <td>0.005347</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 139 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sequence  sequence_length  \\\n",
       "0  MAGLKPEVPLHDGINKFGKSDFAGQEGPKIVTTTDKALLVANGALK...              400   \n",
       "1  MELILAKARLEFECDWGLLMLEPCVPPTKIFADRNYAVGVMFESDK...              350   \n",
       "2  MNQRGMPIQSLVTNVKINRLEENDCIHTRHRVRPGRTDGKNLHAMM...              600   \n",
       "3  MRVLCDGSTGYACAKNTRIRFREKVASVLAKIQGYEQTFPHHMPNM...              420   \n",
       "4  MVDLKKELKNFVDSDFPGSPKQEAQGIDVRILLSFNNAAFREALII...              550   \n",
       "\n",
       "   molecular_weight  aromaticity  isoelectric_point  instability_index  \\\n",
       "0        43254.8112     0.055000           5.964593          38.948025   \n",
       "1        39615.9422     0.091429           4.825028          40.802857   \n",
       "2        66369.0679     0.060000           5.397908          38.074000   \n",
       "3        47355.5634     0.088095           5.392736          42.440000   \n",
       "4        60158.7220     0.072727           5.349652          38.161636   \n",
       "\n",
       "      gravy     helix      turn     sheet  ...  NMBroto_BEGF750101.lag3  \\\n",
       "0 -0.263750  0.335000  0.322500  0.335000  ...                -0.054463   \n",
       "1 -0.248000  0.380000  0.248571  0.357143  ...                -0.016027   \n",
       "2 -0.334833  0.385000  0.268333  0.328333  ...                -0.001961   \n",
       "3 -0.518333  0.319048  0.283333  0.326190  ...                 0.038768   \n",
       "4 -0.183273  0.341818  0.296364  0.340000  ...                 0.087268   \n",
       "\n",
       "   NMBroto_BEGF750102.lag1  NMBroto_BEGF750102.lag2  NMBroto_BEGF750102.lag3  \\\n",
       "0                -0.002630                 0.063674                -0.035028   \n",
       "1                 0.015219                 0.038238                -0.181714   \n",
       "2                -0.041237                 0.100768                -0.049149   \n",
       "3                 0.092617                -0.027349                -0.093597   \n",
       "4                 0.032190                 0.011671                 0.006448   \n",
       "\n",
       "  NMBroto_BEGF750103.lag1 NMBroto_BEGF750103.lag2  NMBroto_BEGF750103.lag3  \\\n",
       "0               -0.056075                0.069239                -0.028580   \n",
       "1                0.066448                0.071851                -0.089194   \n",
       "2                0.086146                0.057148                 0.028043   \n",
       "3                0.083518                0.039504                -0.037585   \n",
       "4                0.069078                0.121472                 0.136282   \n",
       "\n",
       "   NMBroto_BHAR880101.lag1  NMBroto_BHAR880101.lag2  NMBroto_BHAR880101.lag3  \n",
       "0                 0.026617                 0.061418                 0.047221  \n",
       "1                -0.005339                 0.032348                -0.036633  \n",
       "2                -0.021107                 0.125624                -0.004387  \n",
       "3                 0.091911                 0.019569                 0.001394  \n",
       "4                 0.044959                 0.042260                 0.005347  \n",
       "\n",
       "[5 rows x 139 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the merged dataframe to a Parquet file\n",
    "\n",
    "if not os.path.exists(os.path.join(os.path.abspath(\"data\"), \"export\")):\n",
    "\tos.makedirs(os.path.join(os.path.abspath(\"data\"), \"export\"))\n",
    "\n",
    "output_path = os.path.join(os.path.abspath(\"data\"), \"export\")\n",
    "\n",
    "merged_df.to_parquet(os.path.join(output_path, \"results.parquet\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "protein-feature-extraction-NoVdeDG9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
