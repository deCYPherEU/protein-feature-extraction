{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protein feature extraction pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will contain the pipeline for extracting features from protein sequences. It will be used as a way to show the output without needing to run the `pipeline.py` file locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import logging\n",
    "# from fondant.pipeline import Pipeline\n",
    "from fondant.dataset import Dataset\n",
    "from fondant.dataset.runner import DockerRunner\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "from config import MOCK_DATA_PATH_FONDANT\n",
    "\n",
    "# check if the manifest file is removed.\n",
    "REMOVED_MANIFEST = False\n",
    "\n",
    "# check if the output folder exists\n",
    "OUTPUT_FOLDER = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Mock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python utils/generate_mock_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MNQRGMPIQSLVTNVKINRLEENDCIHTRHRVRPGRTDGKNLHAMM...</td>\n",
       "      <td>Seq1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MAGLKPEVPLHDGINKFGKSDFAGQEGPKIVTTTDKALLVANGALK...</td>\n",
       "      <td>Seq2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MVDLKKELKNFVDSDFPGSPKQEAQGIDVRILLSFNNAAFREALII...</td>\n",
       "      <td>Seq3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MELILAKARLEFECDWGLLMLEPCVPPTKIFADRNYAVGVMFESDK...</td>\n",
       "      <td>Seq4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MRVLCDGSTGYACAKNTRIRFREKVASVLAKIQGYEQTFPHHMPNM...</td>\n",
       "      <td>Seq5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MRVLCDGSTGYACAKNTRIRFREKVASVLAKIQGYEQTFPHHMPNM...</td>\n",
       "      <td>Seq6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MRVLCDGSTGYACAKNTRIRFREKVASVLAKIQGYEQTFPHHMPNM...</td>\n",
       "      <td>Seq7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MRVLCDGSTGYACAKNTRIRFREKVASVLAKIQGYEQTFPHHMPNM...</td>\n",
       "      <td>Seq8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sequence  name\n",
       "0  MNQRGMPIQSLVTNVKINRLEENDCIHTRHRVRPGRTDGKNLHAMM...  Seq1\n",
       "1  MAGLKPEVPLHDGINKFGKSDFAGQEGPKIVTTTDKALLVANGALK...  Seq2\n",
       "2  MVDLKKELKNFVDSDFPGSPKQEAQGIDVRILLSFNNAAFREALII...  Seq3\n",
       "3  MELILAKARLEFECDWGLLMLEPCVPPTKIFADRNYAVGVMFESDK...  Seq4\n",
       "4  MRVLCDGSTGYACAKNTRIRFREKVASVLAKIQGYEQTFPHHMPNM...  Seq5\n",
       "5  MRVLCDGSTGYACAKNTRIRFREKVASVLAKIQGYEQTFPHHMPNM...  Seq6\n",
       "6  MRVLCDGSTGYACAKNTRIRFREKVASVLAKIQGYEQTFPHHMPNM...  Seq7\n",
       "7  MRVLCDGSTGYACAKNTRIRFREKVASVLAKIQGYEQTFPHHMPNM...  Seq8"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show content of the mock data\n",
    "import pandas as pd\n",
    "mock_df = pd.read_parquet(\".\" + MOCK_DATA_PATH_FONDANT)  # dot added to make it relative to the current directory\n",
    "mock_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-05 11:55:37,310 | fondant.dataset.dataset | INFO] The consumes section of the component spec is not defined. Can not infer consumes of the OperationSpec. Please define a consumes section in the dataset interface. \n"
     ]
    }
   ],
   "source": [
    "# Create a new pipeline\n",
    "\n",
    "BASE_PATH = \".\"\n",
    "DATASET_NAME = \"feature_extraction_pipeline\"\n",
    "\n",
    "dataset = Dataset.create(\n",
    "    \"load_from_parquet\",\n",
    "    arguments={\n",
    "        \"dataset_uri\": MOCK_DATA_PATH_FONDANT,\n",
    "    },\n",
    "    produces={\n",
    "        \"sequence\": pa.string()\n",
    "    },\n",
    "    dataset_name=DATASET_NAME\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components\n",
    "\n",
    "---\n",
    "\n",
    "### generate_protein_sequence_checksum_component\n",
    "\n",
    "This component generates a checksum for the protein sequence.\n",
    "\n",
    "---\n",
    "\n",
    "### biopython_component\n",
    "\n",
    "Extracts features from the protein sequence using Biopython.\n",
    "\n",
    "---\n",
    "\n",
    "### iFeatureOmega_component\n",
    "\n",
    "Extracts features from the protein sequence using the [iFeatureOmega-CLI GitHub repo](https://github.com/Superzchen/iFeatureOmega-CLI). Arguments are used to specify the type of features to extract.\n",
    "\n",
    "---\n",
    "\n",
    "### filter_pdb_component\n",
    "\n",
    "Filters PDB files that are already predicted to avoid redundant predictions. Arguments need to be specified before running the pipeline:\n",
    "```json\n",
    "\"storage_type\": \"local\",\n",
    "\"pdb_path\": \"/data/<your-pdb-folder-path>\",\n",
    "\"bucket_name\": \"your-bucket-name\",\n",
    "\"project_id\": \"your-project-id\",\n",
    "\"google_cloud_credentials_path\": \"/data/<your-credentials>.json\"\n",
    "```\n",
    "\n",
    "If only using local, keep bucket_name, project_id, and google_cloud_credentials_path as empty strings. Using remote requires a Google Cloud Storage bucket with credentials and a project ID.\n",
    "\n",
    "---\n",
    "\n",
    "### predict_protein_3D_structure_component\n",
    "\n",
    "Predicts the 3D structure of the protein using ESMFold. This component requires a `.env` file with the following variables:\n",
    "```env\n",
    "HF_API_KEY=\"\"\n",
    "HF_ENDPOINT_URL=\"\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### store_pdb_component\n",
    "\n",
    "Stores the PDB files in the provided storage_type. Arguments need to be specified before running the pipeline:\n",
    "```json\n",
    "\"storage_type\": \"local\",\n",
    "\"pdb_path\": \"/data/<your-pdb-folder-path>\",\n",
    "\"bucket_name\": \"your-bucket-name\",\n",
    "\"project_id\": \"your-project-id\",\n",
    "\"google_cloud_credentials_path\": \"/data/<your-credentials>.json\"\n",
    "```\n",
    "\n",
    "If only using local, keep bucket_name, project_id, and google_cloud_credentials_path as empty strings. Using remote requires a Google Cloud Storage bucket with credentials and a project ID.\n",
    "\n",
    "---\n",
    "\n",
    "### msa_component\n",
    "\n",
    "Generates the multiple sequence alignment for the protein sequence using [Clustal Omega](http://www.clustal.org/omega/). It's recommended to use a smaller number of sequences or none at all due to potential time consumption.\n",
    "\n",
    "---\n",
    "\n",
    "### unikp_component\n",
    "\n",
    "Uses the UniKP endpoint on HuggingFace to predict the kinetic parameters of a protein sequence and substrate (SMILES) combination. See README for the description of the contents of this file.\n",
    "\n",
    "```yaml\n",
    "\"protein_smiles_path\": \"/data/<path_protein_smiles>\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### peptide_component\n",
    "\n",
    "Calculates the features from the protein sequence using the `peptides` package.\n",
    "\n",
    "---\n",
    "\n",
    "### deepTMpred_component\n",
    "\n",
    "Predicts the transmembrane regions of the protein sequence using the [DeepTMpred GitHub repository](https://github.com/ISYSLAB-HUST/DeepTMpred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-05 11:55:37,360 | fondant.dataset.dataset | WARNING] Component `iFeatureOmega component` has an image tag set to latest. Caching for the component will be disabled to prevent unpredictable behavior due to images updates\n",
      "[2024-09-05 11:55:37,366 | fondant.dataset.dataset | WARNING] Component `Filter PDB Component` has an image tag set to latest. Caching for the component will be disabled to prevent unpredictable behavior due to images updates\n",
      "[2024-09-05 11:55:37,372 | fondant.dataset.dataset | WARNING] Component `Predict Protein 3D Structure Component` has an image tag set to latest. Caching for the component will be disabled to prevent unpredictable behavior due to images updates\n",
      "[2024-09-05 11:55:37,381 | fondant.dataset.dataset | WARNING] Component `Store PDB Component` has an image tag set to latest. Caching for the component will be disabled to prevent unpredictable behavior due to images updates\n",
      "[2024-09-05 11:55:37,386 | fondant.dataset.dataset | WARNING] Component `MSA component` has an image tag set to latest. Caching for the component will be disabled to prevent unpredictable behavior due to images updates\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.apply(\n",
    "    \"./components/biopython_component\",\n",
    "    cache=False\n",
    ").apply(\n",
    "    \"./components/generate_protein_sequence_checksum_component\",\n",
    "    cache=False\n",
    ").apply(\n",
    "    \"./components/iFeatureOmega_component\",\n",
    "    # currently forcing the number of rows to 5, but there needs to be a better way to do this, see readme for more info\n",
    "    input_partition_rows=5,\n",
    "    arguments={\n",
    "        \"descriptors\": [\"AAC\", \"CTDC\", \"CTDT\"]\n",
    "    }\n",
    ").apply(\n",
    "    \"./components/filter_pdb_component\",\n",
    "    arguments={\n",
    "        \"method\": \"local\",\n",
    "        \"local_pdb_path\": \"/data/pdb_files\",\n",
    "        \"bucket_name\": \"\",\n",
    "        \"project_id\": \"\",\n",
    "        \"google_cloud_credentials_path\": \"\"\n",
    "    }\n",
    ").apply(\n",
    "    \"./components/predict_protein_3D_structure_component\",\n",
    ").apply(\n",
    "    \"./components/store_pdb_component\",\n",
    "    arguments={\n",
    "        \"method\": \"local\",\n",
    "        \"local_pdb_path\": \"/data/pdb_files/\",\n",
    "        \"bucket_name\": \"elated-chassis-400207_dbtl_pipeline_outputs\",\n",
    "        \"project_id\": \"elated-chassis-400207\",\n",
    "        \"google_cloud_credentials_path\": \"/data/google_cloud_credentials.json\"\n",
    "    }\n",
    ").apply(\n",
    "    \"./components/msa_component\",\n",
    "    input_partition_rows='10000'\n",
    "# ).apply(\n",
    "#     \"./components/pdb_features_component\"\n",
    "# ).apply(\n",
    "#     \"./components/unikp_component\",\n",
    "#     arguments={\n",
    "#         \"target_molecule_smiles\": \"/data/target_molecule_smiles.json\",\n",
    "#     },\n",
    "# ).apply(\n",
    "#     \"./components/peptide_features_component\"\n",
    "# ).apply(\n",
    "#     \"./components/DeepTMpred_component\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pipeline.py` file needs to be run using the command line. The following command will run the pipeline:\n",
    "\n",
    "```bash\n",
    "fondant < full_path_to_pipeline.py >\\data:/data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-05 11:55:37,458 | root | INFO] Found reference to un-compiled workflow... compiling\n",
      "[2024-09-05 11:55:37,459 | fondant.dataset.compiler | INFO] Base path found on local system, setting up . as mount volume\n",
      "[2024-09-05 11:55:37,459 | fondant.dataset.dataset | INFO] Sorting workflow graph topologically.\n",
      "[2024-09-05 11:55:37,479 | fondant.dataset.dataset | INFO] All workflow component specifications match.\n",
      "[2024-09-05 11:55:37,480 | fondant.dataset.compiler | INFO] Compiling service for load_from_parquet\n",
      "[2024-09-05 11:55:37,481 | fondant.dataset.compiler | INFO] Compiling service for biopython_component\n",
      "[2024-09-05 11:55:37,482 | fondant.dataset.compiler | INFO] Found Dockerfile for biopython_component, adding build step.\n",
      "[2024-09-05 11:55:37,482 | fondant.dataset.compiler | INFO] Compiling service for generate_protein_sequence_checksum_component\n",
      "[2024-09-05 11:55:37,483 | fondant.dataset.compiler | INFO] Found Dockerfile for generate_protein_sequence_checksum_component, adding build step.\n",
      "[2024-09-05 11:55:37,484 | fondant.dataset.compiler | INFO] Compiling service for ifeatureomega_component\n",
      "[2024-09-05 11:55:37,485 | fondant.dataset.compiler | INFO] Found Dockerfile for ifeatureomega_component, adding build step.\n",
      "[2024-09-05 11:55:37,485 | fondant.dataset.compiler | INFO] Compiling service for filter_pdb_component\n",
      "[2024-09-05 11:55:37,486 | fondant.dataset.compiler | INFO] Found Dockerfile for filter_pdb_component, adding build step.\n",
      "[2024-09-05 11:55:37,486 | fondant.dataset.compiler | INFO] Compiling service for predict_protein_3d_structure_component\n",
      "[2024-09-05 11:55:37,487 | fondant.dataset.compiler | INFO] Found Dockerfile for predict_protein_3d_structure_component, adding build step.\n",
      "[2024-09-05 11:55:37,488 | fondant.dataset.compiler | INFO] Compiling service for store_pdb_component\n",
      "[2024-09-05 11:55:37,489 | fondant.dataset.compiler | INFO] Found Dockerfile for store_pdb_component, adding build step.\n",
      "[2024-09-05 11:55:37,490 | fondant.dataset.compiler | INFO] Compiling service for msa_component\n",
      "[2024-09-05 11:55:37,490 | fondant.dataset.compiler | INFO] Found Dockerfile for msa_component, adding build step.\n",
      "[2024-09-05 11:55:37,524 | fondant.dataset.compiler | INFO] Successfully compiled to .fondant/compose.yaml\n",
      "time=\"2024-09-05T11:55:37+02:00\" level=warning msg=\"/home/pietercoussement/Software/deCYPher/protein-feature-extraction/.fondant/compose.yaml: `version` is obsolete\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docker version:\n",
      "(26, 1, 4)\n",
      "Starting workflow run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " load_from_parquet Pulling \n",
      " load_from_parquet Pulled \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0 building with \"desktop-linux\" instance using docker driver\n",
      "\n",
      "#1 [biopython_component internal] load build definition from Dockerfile\n",
      "#1 transferring dockerfile: 466B done\n",
      "#1 DONE 0.0s\n",
      "\n",
      "#2 [biopython_component internal] load metadata for docker.io/library/python:3.10-slim\n",
      "#2 DONE 0.8s\n",
      "\n",
      "#3 [biopython_component internal] load .dockerignore\n",
      "#3 transferring context: 2B done\n",
      "#3 DONE 0.0s\n",
      "\n",
      "#4 [biopython_component 1/6] FROM docker.io/library/python:3.10-slim@sha256:235703999ce36b498d490b534e781664d72cdec9c2f0adb97582467e55fad8e2\n",
      "#4 DONE 0.0s\n",
      "\n",
      "#5 [biopython_component internal] load build context\n",
      "#5 transferring context: 3.63kB done\n",
      "#5 DONE 0.0s\n",
      "\n",
      "#6 [biopython_component 5/6] WORKDIR /component/src\n",
      "#6 CACHED\n",
      "\n",
      "#7 [biopython_component 3/6] COPY requirements.txt ./\n",
      "#7 CACHED\n",
      "\n",
      "#8 [biopython_component 4/6] RUN pip3 install --no-cache-dir -r requirements.txt\n",
      "#8 CACHED\n",
      "\n",
      "#9 [biopython_component 2/6] RUN apt-get update &&     apt-get upgrade -y &&     apt-get install git -y\n",
      "#9 CACHED\n",
      "\n",
      "#10 [biopython_component 6/6] COPY src/ .\n",
      "#10 CACHED\n",
      "\n",
      "#11 [biopython_component] exporting to image\n",
      "#11 exporting layers done\n",
      "#11 writing image sha256:5650498ef87347d36c03f8c76e9bd6b1f89c0032ab4d237363cf04bc1780cdb5 done\n",
      "#11 naming to docker.io/library/feature_extraction_pipeline-biopython_component done\n",
      "#11 DONE 0.0s\n",
      "\n",
      "#12 [generate_protein_sequence_checksum_component internal] load build definition from Dockerfile\n",
      "#12 transferring dockerfile: 466B done\n",
      "#12 DONE 0.0s\n",
      "\n",
      "#2 [generate_protein_sequence_checksum_component internal] load metadata for docker.io/library/python:3.10-slim\n",
      "#2 DONE 1.0s\n",
      "\n",
      "#13 [generate_protein_sequence_checksum_component internal] load .dockerignore\n",
      "#13 transferring context: 2B done\n",
      "#13 DONE 0.0s\n",
      "\n",
      "#4 [generate_protein_sequence_checksum_component 1/6] FROM docker.io/library/python:3.10-slim@sha256:235703999ce36b498d490b534e781664d72cdec9c2f0adb97582467e55fad8e2\n",
      "#4 DONE 0.0s\n",
      "\n",
      "#14 [generate_protein_sequence_checksum_component internal] load build context\n",
      "#14 transferring context: 1.24kB done\n",
      "#14 DONE 0.0s\n",
      "\n",
      "#15 [generate_protein_sequence_checksum_component 4/6] RUN pip3 install --no-cache-dir -r requirements.txt\n",
      "#15 CACHED\n",
      "\n",
      "#9 [generate_protein_sequence_checksum_component 2/6] RUN apt-get update &&     apt-get upgrade -y &&     apt-get install git -y\n",
      "#9 CACHED\n",
      "\n",
      "#16 [generate_protein_sequence_checksum_component 5/6] WORKDIR /component/src\n",
      "#16 CACHED\n",
      "\n",
      "#17 [generate_protein_sequence_checksum_component 3/6] COPY requirements.txt ./\n",
      "#17 CACHED\n",
      "\n",
      "#18 [generate_protein_sequence_checksum_component 6/6] COPY src/ .\n",
      "#18 CACHED\n",
      "\n",
      "#19 [generate_protein_sequence_checksum_component] exporting to image\n",
      "#19 exporting layers done\n",
      "#19 writing image sha256:58b4b9806938c89f5223466d30f4cb68c335ce815b5208b978d3cfb5cb28a2a5 done\n",
      "#19 naming to docker.io/library/feature_extraction_pipeline-generate_protein_sequence_checksum_component done\n",
      "#19 DONE 0.0s\n",
      "\n",
      "#20 [ifeatureomega_component internal] load build definition from Dockerfile\n",
      "#20 transferring dockerfile: 654B done\n",
      "#20 DONE 0.0s\n",
      "\n",
      "#2 [ifeatureomega_component internal] load metadata for docker.io/library/python:3.10-slim\n",
      "#2 DONE 1.2s\n",
      "\n",
      "#21 [ifeatureomega_component internal] load .dockerignore\n",
      "#21 transferring context: 2B done\n",
      "#21 DONE 0.0s\n",
      "\n",
      "#4 [ifeatureomega_component 1/8] FROM docker.io/library/python:3.10-slim@sha256:235703999ce36b498d490b534e781664d72cdec9c2f0adb97582467e55fad8e2\n",
      "#4 DONE 0.0s\n",
      "\n",
      "#22 [ifeatureomega_component internal] load build context\n",
      "#22 transferring context: 3.42kB done\n",
      "#22 DONE 0.0s\n",
      "\n",
      "#9 [ifeatureomega_component 2/8] RUN apt-get update &&     apt-get upgrade -y &&     apt-get install git -y\n",
      "#9 CACHED\n",
      "\n",
      "#23 [ifeatureomega_component 4/8] RUN pip3 install --no-cache-dir -r requirements.txt\n",
      "#23 CACHED\n",
      "\n",
      "#24 [ifeatureomega_component 3/8] COPY requirements.txt ./\n",
      "#24 CACHED\n",
      "\n",
      "#25 [ifeatureomega_component 5/8] WORKDIR /component/src\n",
      "#25 CACHED\n",
      "\n",
      "#26 [ifeatureomega_component 6/8] COPY src/ .\n",
      "#26 CACHED\n",
      "\n",
      "#27 [ifeatureomega_component 7/8] RUN git clone https://github.com/Superzchen/iFeatureOmega-CLI\n",
      "#27 CACHED\n",
      "\n",
      "#28 [ifeatureomega_component 8/8] RUN mv iFeatureOmega-CLI iFeatureOmega_CLI\n",
      "#28 CACHED\n",
      "\n",
      "#29 [ifeatureomega_component] exporting to image\n",
      "#29 exporting layers done\n",
      "#29 writing image sha256:fa1ab7e00fc4aa626081f6b82a18feac1f76ef524620bde2650d27b0c1a296a4 done\n",
      "#29 naming to docker.io/library/feature_extraction_pipeline-ifeatureomega_component done\n",
      "#29 DONE 0.0s\n",
      "\n",
      "#30 [filter_pdb_component internal] load build definition from Dockerfile\n",
      "#30 transferring dockerfile: 466B done\n",
      "#30 DONE 0.0s\n",
      "\n",
      "#2 [filter_pdb_component internal] load metadata for docker.io/library/python:3.10-slim\n",
      "#2 DONE 1.4s\n",
      "\n",
      "#31 [filter_pdb_component internal] load .dockerignore\n",
      "#31 transferring context: 2B done\n",
      "#31 DONE 0.0s\n",
      "\n",
      "#4 [filter_pdb_component 1/6] FROM docker.io/library/python:3.10-slim@sha256:235703999ce36b498d490b534e781664d72cdec9c2f0adb97582467e55fad8e2\n",
      "#4 DONE 0.0s\n",
      "\n",
      "#32 [filter_pdb_component internal] load build context\n",
      "#32 transferring context: 4.41kB done\n",
      "#32 DONE 0.0s\n",
      "\n",
      "#9 [filter_pdb_component 2/6] RUN apt-get update &&     apt-get upgrade -y &&     apt-get install git -y\n",
      "#9 CACHED\n",
      "\n",
      "#33 [filter_pdb_component 3/6] COPY requirements.txt ./\n",
      "#33 CACHED\n",
      "\n",
      "#34 [filter_pdb_component 4/6] RUN pip3 install --no-cache-dir -r requirements.txt\n",
      "#34 CACHED\n",
      "\n",
      "#35 [filter_pdb_component 5/6] WORKDIR /component/src\n",
      "#35 CACHED\n",
      "\n",
      "#36 [filter_pdb_component 6/6] COPY src/ .\n",
      "#36 CACHED\n",
      "\n",
      "#37 [filter_pdb_component] exporting to image\n",
      "#37 exporting layers done\n",
      "#37 writing image sha256:572f458554e8278d40d8d772da5c8b3387b24ba84dadf17e912b1a715943a1ed done\n",
      "#37 naming to docker.io/library/feature_extraction_pipeline-filter_pdb_component done\n",
      "#37 DONE 0.0s\n",
      "\n",
      "#38 [predict_protein_3d_structure_component internal] load build definition from Dockerfile\n",
      "#38 transferring dockerfile: 525B done\n",
      "#38 DONE 0.0s\n",
      "\n",
      "#2 [predict_protein_3d_structure_component internal] load metadata for docker.io/library/python:3.10-slim\n",
      "#2 DONE 1.6s\n",
      "\n",
      "#39 [predict_protein_3d_structure_component internal] load .dockerignore\n",
      "#39 transferring context: 2B done\n",
      "#39 DONE 0.0s\n",
      "\n",
      "#4 [predict_protein_3d_structure_component 1/7] FROM docker.io/library/python:3.10-slim@sha256:235703999ce36b498d490b534e781664d72cdec9c2f0adb97582467e55fad8e2\n",
      "#4 DONE 0.0s\n",
      "\n",
      "#40 [predict_protein_3d_structure_component internal] load build context\n",
      "#40 transferring context: 2.94kB done\n",
      "#40 DONE 0.0s\n",
      "\n",
      "#41 [predict_protein_3d_structure_component 5/7] WORKDIR /component/src\n",
      "#41 CACHED\n",
      "\n",
      "#42 [predict_protein_3d_structure_component 3/7] COPY requirements.txt ./\n",
      "#42 CACHED\n",
      "\n",
      "#43 [predict_protein_3d_structure_component 6/7] COPY .env .\n",
      "#43 CACHED\n",
      "\n",
      "#9 [predict_protein_3d_structure_component 2/7] RUN apt-get update &&     apt-get upgrade -y &&     apt-get install git -y\n",
      "#9 CACHED\n",
      "\n",
      "#44 [predict_protein_3d_structure_component 4/7] RUN pip3 install --no-cache-dir -r requirements.txt\n",
      "#44 CACHED\n",
      "\n",
      "#45 [predict_protein_3d_structure_component 7/7] COPY src/ .\n",
      "#45 CACHED\n",
      "\n",
      "#46 [predict_protein_3d_structure_component] exporting to image\n",
      "#46 exporting layers done\n",
      "#46 writing image sha256:1013993021e3f32240e704ae5f26202199e898ccbd8c4a997821daf7e28a7253 done\n",
      "#46 naming to docker.io/library/feature_extraction_pipeline-predict_protein_3d_structure_component done\n",
      "#46 DONE 0.0s\n",
      "\n",
      "#47 [store_pdb_component internal] load build definition from Dockerfile\n",
      "#47 transferring dockerfile: 466B done\n",
      "#47 DONE 0.0s\n",
      "\n",
      "#2 [store_pdb_component internal] load metadata for docker.io/library/python:3.10-slim\n",
      "#2 DONE 1.9s\n",
      "\n",
      "#48 [store_pdb_component internal] load .dockerignore\n",
      "#48 transferring context: 2B done\n",
      "#48 DONE 0.0s\n",
      "\n",
      "#4 [store_pdb_component 1/6] FROM docker.io/library/python:3.10-slim@sha256:235703999ce36b498d490b534e781664d72cdec9c2f0adb97582467e55fad8e2\n",
      "#4 DONE 0.0s\n",
      "\n",
      "#49 [store_pdb_component internal] load build context\n",
      "#49 transferring context: 3.60kB done\n",
      "#49 DONE 0.0s\n",
      "\n",
      "#50 [store_pdb_component 4/6] RUN pip3 install --no-cache-dir -r requirements.txt\n",
      "#50 CACHED\n",
      "\n",
      "#51 [store_pdb_component 5/6] WORKDIR /component/src\n",
      "#51 CACHED\n",
      "\n",
      "#9 [store_pdb_component 2/6] RUN apt-get update &&     apt-get upgrade -y &&     apt-get install git -y\n",
      "#9 CACHED\n",
      "\n",
      "#52 [store_pdb_component 3/6] COPY requirements.txt ./\n",
      "#52 CACHED\n",
      "\n",
      "#53 [store_pdb_component 6/6] COPY src/ .\n",
      "#53 CACHED\n",
      "\n",
      "#54 [store_pdb_component] exporting to image\n",
      "#54 exporting layers done\n",
      "#54 writing image sha256:60004d8734011c9b3bef5b8bfe621a6861b540ecbb5e67104871bf6a71a2aed6 0.0s done\n",
      "#54 naming to docker.io/library/feature_extraction_pipeline-store_pdb_component 0.0s done\n",
      "#54 DONE 0.0s\n",
      "\n",
      "#55 [msa_component internal] load build definition from Dockerfile\n",
      "#55 transferring dockerfile: 686B done\n",
      "#55 DONE 0.0s\n",
      "\n",
      "#2 [msa_component internal] load metadata for docker.io/library/python:3.10-slim\n",
      "#2 DONE 2.1s\n",
      "\n",
      "#56 [msa_component internal] load .dockerignore\n",
      "#56 transferring context: 2B done\n",
      "#56 DONE 0.0s\n",
      "\n",
      "#4 [msa_component 1/7] FROM docker.io/library/python:3.10-slim@sha256:235703999ce36b498d490b534e781664d72cdec9c2f0adb97582467e55fad8e2\n",
      "#4 DONE 0.0s\n",
      "\n",
      "#57 [msa_component internal] load build context\n",
      "#57 transferring context: 4.34kB done\n",
      "#57 DONE 0.0s\n",
      "\n",
      "#58 [msa_component 5/7] WORKDIR /component/src\n",
      "#58 CACHED\n",
      "\n",
      "#59 [msa_component 2/7] RUN apt-get update &&     apt-get upgrade -y &&     apt-get install git wget -y\n",
      "#59 CACHED\n",
      "\n",
      "#60 [msa_component 3/7] COPY requirements.txt ./\n",
      "#60 CACHED\n",
      "\n",
      "#61 [msa_component 4/7] RUN pip3 install --no-cache-dir -r requirements.txt\n",
      "#61 CACHED\n",
      "\n",
      "#62 [msa_component 6/7] RUN wget http://www.clustal.org/omega/clustalo-1.2.4-Ubuntu-x86_64 &&     mv clustalo-1.2.4-Ubuntu-x86_64 clustalo && \tchmod +x clustalo && \tmv clustalo /usr/local/bin/\n",
      "#62 CACHED\n",
      "\n",
      "#63 [msa_component 7/7] COPY src/ .\n",
      "#63 CACHED\n",
      "\n",
      "#64 [msa_component] exporting to image\n",
      "#64 exporting layers done\n",
      "#64 writing image sha256:b6f0019a41c45cb425cbdc0ebd1196d20bf419e1f317ecfe426303011778c07e done\n",
      "#64 naming to docker.io/library/feature_extraction_pipeline-msa_component done\n",
      "#64 DONE 0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Container feature_extraction_pipeline-load_from_parquet-1  Recreate\n",
      " Container feature_extraction_pipeline-load_from_parquet-1  Recreated\n",
      " Container feature_extraction_pipeline-biopython_component-1  Recreate\n",
      " Container feature_extraction_pipeline-biopython_component-1  Recreated\n",
      " Container feature_extraction_pipeline-generate_protein_sequence_checksum_component-1  Recreate\n",
      " Container feature_extraction_pipeline-generate_protein_sequence_checksum_component-1  Recreated\n",
      " Container feature_extraction_pipeline-ifeatureomega_component-1  Recreate\n",
      " Container feature_extraction_pipeline-ifeatureomega_component-1  Recreated\n",
      " Container feature_extraction_pipeline-filter_pdb_component-1  Recreate\n",
      " Container feature_extraction_pipeline-filter_pdb_component-1  Recreated\n",
      " Container feature_extraction_pipeline-predict_protein_3d_structure_component-1  Recreate\n",
      " Container feature_extraction_pipeline-predict_protein_3d_structure_component-1  Recreated\n",
      " Container feature_extraction_pipeline-store_pdb_component-1  Recreate\n",
      " Container feature_extraction_pipeline-store_pdb_component-1  Recreated\n",
      " Container feature_extraction_pipeline-msa_component-1  Recreate\n",
      " Container feature_extraction_pipeline-msa_component-1  Recreated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attaching to biopython_component-1, filter_pdb_component-1, generate_protein_sequence_checksum_component-1, ifeatureomega_component-1, load_from_parquet-1, msa_component-1, predict_protein_3d_structure_component-1, store_pdb_component-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load_from_parquet-1                             | [2024-09-05 09:55:46,280 | fondant.cli | INFO] Component `LoadFromParquet` found in module main\n",
      "load_from_parquet-1                             | [2024-09-05 09:55:46,284 | fondant.component.executor | INFO] Caching is currently temporarily disabled.\n",
      "load_from_parquet-1                             | [2024-09-05 09:55:46,286 | fondant.component.executor | INFO] Skipping component execution\n",
      "load_from_parquet-1                             | [2024-09-05 09:55:46,287 | fondant.component.executor | INFO] Matching execution detected for component. The last execution of the component originated from `feature_extraction_pipeline-20240905101821`.\n",
      "load_from_parquet-1                             | [2024-09-05 09:55:46,288 | fondant.component.executor | INFO] Saving output manifest to /home/pietercoussement/Software/deCYPher/protein-feature-extraction/feature_extraction_pipeline/feature_extraction_pipeline-20240905115537/load_from_parquet/manifest.json\n",
      "load_from_parquet-1                             | [2024-09-05 09:55:46,288 | fondant.component.executor | INFO] Writing cache key with manifest reference to /home/pietercoussement/Software/deCYPher/protein-feature-extraction/feature_extraction_pipeline/cache/80e8e3d381608751bd0b582a6aa398fa.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[Kload_from_parquet-1 exited with code 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "biopython_component-1                           | [2024-09-05 09:55:48,407 | fondant.cli | INFO] Component `BiopythonComponent` found in module main\n",
      "biopython_component-1                           | [2024-09-05 09:55:48,411 | fondant.component.executor | INFO] Caching disabled for the component\n",
      "biopython_component-1                           | [2024-09-05 09:55:48,412 | root | INFO] Executing component\n",
      "biopython_component-1                           | [2024-09-05 09:55:48,781 | distributed.http.proxy | INFO] To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy\n",
      "biopython_component-1                           | [2024-09-05 09:55:48,809 | distributed.scheduler | INFO] State start\n",
      "biopython_component-1                           | [2024-09-05 09:55:48,813 | distributed.scheduler | INFO]   Scheduler at:     tcp://127.0.0.1:43391\n",
      "biopython_component-1                           | [2024-09-05 09:55:48,814 | distributed.scheduler | INFO]   dashboard at:  http://127.0.0.1:8787/status\n",
      "biopython_component-1                           | [2024-09-05 09:55:48,814 | distributed.scheduler | INFO] Registering Worker plugin shuffle\n",
      "biopython_component-1                           | [2024-09-05 09:55:48,839 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:37345'\n",
      "biopython_component-1                           | [2024-09-05 09:55:48,844 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:43317'\n",
      "biopython_component-1                           | [2024-09-05 09:55:48,846 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:33961'\n",
      "biopython_component-1                           | [2024-09-05 09:55:48,849 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:37099'\n",
      "biopython_component-1                           | [2024-09-05 09:55:48,865 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:43843'\n",
      "biopython_component-1                           | [2024-09-05 09:55:48,870 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:41233'\n",
      "biopython_component-1                           | [2024-09-05 09:55:48,872 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:42115'\n",
      "biopython_component-1                           | [2024-09-05 09:55:48,876 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:40029'\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,238 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:34881\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,239 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:34881\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,239 | distributed.worker | INFO]           Worker name:                          0\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,239 | distributed.worker | INFO]          dashboard at:            127.0.0.1:44953\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,239 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:43391\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,239 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,239 | distributed.worker | INFO]               Threads:                          1\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,239 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,239 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-_ps4st8k\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,239 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,248 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:34881', name: 0, status: init, memory: 0, processing: 0>\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,252 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:39103\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,252 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:39103\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,252 | distributed.worker | INFO]           Worker name:                          5\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,252 | distributed.worker | INFO]          dashboard at:            127.0.0.1:34773\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,252 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:43391\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,252 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,252 | distributed.worker | INFO]               Threads:                          1\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,253 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,253 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-nnzsjmsh\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,253 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,253 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:34881\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,253 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:58582\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,256 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,257 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:43391\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,257 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,258 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:43391\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,265 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:39103', name: 5, status: init, memory: 0, processing: 0>\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,267 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:39103\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,267 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:58588\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,269 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,270 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:43391\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,270 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,270 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:46283\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,270 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:46283\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,270 | distributed.worker | INFO]           Worker name:                          6\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,270 | distributed.worker | INFO]          dashboard at:            127.0.0.1:43247\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,270 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:43391\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,270 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,270 | distributed.worker | INFO]               Threads:                          1\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,270 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,270 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-xje7jhin\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,270 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,272 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:43391\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,277 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:46283', name: 6, status: init, memory: 0, processing: 0>\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,278 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:46283\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,278 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:58590\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,279 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,280 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:43391\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,280 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,281 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:43391\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,318 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:37433\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,318 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:37433\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,318 | distributed.worker | INFO]           Worker name:                          3\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,318 | distributed.worker | INFO]          dashboard at:            127.0.0.1:33539\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,319 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:43391\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,319 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,319 | distributed.worker | INFO]               Threads:                          1\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,319 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,319 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-hqjt8cgf\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,319 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,326 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:37433', name: 3, status: init, memory: 0, processing: 0>\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,327 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:37433\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,327 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:58604\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,328 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,328 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:43391\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,328 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,329 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:43391\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,331 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:35335\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,332 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:35335\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,332 | distributed.worker | INFO]           Worker name:                          4\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,332 | distributed.worker | INFO]          dashboard at:            127.0.0.1:38093\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,332 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:43391\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,332 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,332 | distributed.worker | INFO]               Threads:                          1\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,332 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,332 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-wzq3ulvg\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,332 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,338 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:35335', name: 4, status: init, memory: 0, processing: 0>\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,339 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:35335\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,339 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:58614\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,340 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,340 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:43391\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,340 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,340 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:43391\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,354 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:43189\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,354 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:43189\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,354 | distributed.worker | INFO]           Worker name:                          2\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,354 | distributed.worker | INFO]          dashboard at:            127.0.0.1:45355\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,354 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:43391\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,354 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,354 | distributed.worker | INFO]               Threads:                          1\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,355 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,355 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-6dfxn54n\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,355 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,359 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:43189', name: 2, status: init, memory: 0, processing: 0>\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,359 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:38143\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,360 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:38143\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,359 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:43189\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,360 | distributed.worker | INFO]           Worker name:                          1\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,360 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:58620\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,360 | distributed.worker | INFO]          dashboard at:            127.0.0.1:38495\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,360 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:43391\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,360 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,360 | distributed.worker | INFO]               Threads:                          1\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,360 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,360 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-xhihyx2x\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,360 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,360 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,361 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:43391\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,361 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,361 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:43391\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,364 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:38143', name: 1, status: init, memory: 0, processing: 0>\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,365 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:38143\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,365 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:58632\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,365 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,366 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:43391\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,366 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,366 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:43391\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,395 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:43043\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,395 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:43043\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,395 | distributed.worker | INFO]           Worker name:                          7\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,395 | distributed.worker | INFO]          dashboard at:            127.0.0.1:33709\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,395 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:43391\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,395 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,395 | distributed.worker | INFO]               Threads:                          1\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,395 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,395 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-3wnsvcph\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,395 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,400 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:43043', name: 7, status: init, memory: 0, processing: 0>\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,400 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:43043\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,401 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:58636\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,401 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,401 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:43391\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,402 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,402 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:43391\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,415 | distributed.scheduler | INFO] Receive client connection: Client-0826549f-6b6d-11ef-8001-0242ac120002\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,415 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:58648\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,438 | fondant.component.data_io | INFO] The number of partitions of the input dataframe is 1. The available number of workers is 8.\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,439 | fondant.component.data_io | INFO] Repartitioning the data to 8 partitions before processing to maximize worker usage\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,439 | root | INFO] Columns of dataframe: ['sequence']\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,458 | root | WARNING] Failed to infer dtype of index column, falling back to `string`. Specify the dtype explicitly to prevent this.\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,873 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:37345'. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,873 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,873 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:43317'. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,874 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,874 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:33961'. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,874 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:34881. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,874 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,874 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:37099'. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,874 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,874 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:43843'. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,874 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:38143. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,875 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,875 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:43189. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,875 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:37433. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,875 | distributed.core | INFO] Connection to tcp://127.0.0.1:43391 has been closed.\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,875 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:41233'. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,875 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,876 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:42115'. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,876 | distributed.core | INFO] Connection to tcp://127.0.0.1:43391 has been closed.\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,876 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,876 | distributed.core | INFO] Connection to tcp://127.0.0.1:43391 has been closed.\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,876 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:40029'. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,876 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,876 | distributed.core | INFO] Connection to tcp://127.0.0.1:43391 has been closed.\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,877 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:43043. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,877 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:35335. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,877 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:39103. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,878 | distributed.core | INFO] Connection to tcp://127.0.0.1:43391 has been closed.\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,879 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:46283. Reason: nanny-close\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,879 | distributed.core | INFO] Connection to tcp://127.0.0.1:43391 has been closed.\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,879 | distributed.core | INFO] Connection to tcp://127.0.0.1:43391 has been closed.\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,881 | distributed.core | INFO] Connection to tcp://127.0.0.1:43391 has been closed.\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,887 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:58582; closing.\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,889 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:58632; closing.\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,889 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:58620; closing.\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,889 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:58604; closing.\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,892 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:34881', name: 0, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1725530150.8921993')\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,894 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:38143', name: 1, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1725530150.8941052')\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,896 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:43189', name: 2, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1725530150.8960335')\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,897 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:37433', name: 3, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1725530150.8974836')\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,903 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:58636; closing.\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,905 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:58588; closing.\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,906 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:58614; closing.\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,907 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:58590; closing.\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,909 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:43043', name: 7, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1725530150.9092438')\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,910 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:39103', name: 5, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1725530150.910375')\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,911 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:35335', name: 4, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1725530150.9113772')\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,913 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:46283', name: 6, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1725530150.9134514')\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,914 | distributed.scheduler | INFO] Lost all workers\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,924 | distributed.batched | INFO] Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:43391 remote=tcp://127.0.0.1:58614>\n",
      "biopython_component-1                           | Traceback (most recent call last):\n",
      "biopython_component-1                           |   File \"/usr/local/lib/python3.10/site-packages/distributed/batched.py\", line 115, in _background_send\n",
      "biopython_component-1                           |     nbytes = yield coro\n",
      "biopython_component-1                           |   File \"/usr/local/lib/python3.10/site-packages/tornado/gen.py\", line 766, in run\n",
      "biopython_component-1                           |     value = future.result()\n",
      "biopython_component-1                           |   File \"/usr/local/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 262, in write\n",
      "biopython_component-1                           |     raise CommClosedError()\n",
      "biopython_component-1                           | distributed.comm.core.CommClosedError\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,925 | distributed.batched | INFO] Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:43391 remote=tcp://127.0.0.1:58588>\n",
      "biopython_component-1                           | Traceback (most recent call last):\n",
      "biopython_component-1                           |   File \"/usr/local/lib/python3.10/site-packages/distributed/batched.py\", line 115, in _background_send\n",
      "biopython_component-1                           |     nbytes = yield coro\n",
      "biopython_component-1                           |   File \"/usr/local/lib/python3.10/site-packages/tornado/gen.py\", line 766, in run\n",
      "biopython_component-1                           |     value = future.result()\n",
      "biopython_component-1                           |   File \"/usr/local/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 262, in write\n",
      "biopython_component-1                           |     raise CommClosedError()\n",
      "biopython_component-1                           | distributed.comm.core.CommClosedError\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,926 | distributed.batched | INFO] Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:43391 remote=tcp://127.0.0.1:58636>\n",
      "biopython_component-1                           | Traceback (most recent call last):\n",
      "biopython_component-1                           |   File \"/usr/local/lib/python3.10/site-packages/distributed/batched.py\", line 115, in _background_send\n",
      "biopython_component-1                           |     nbytes = yield coro\n",
      "biopython_component-1                           |   File \"/usr/local/lib/python3.10/site-packages/tornado/gen.py\", line 766, in run\n",
      "biopython_component-1                           |     value = future.result()\n",
      "biopython_component-1                           |   File \"/usr/local/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 262, in write\n",
      "biopython_component-1                           |     raise CommClosedError()\n",
      "biopython_component-1                           | distributed.comm.core.CommClosedError\n",
      "biopython_component-1                           | [2024-09-05 09:55:50,926 | distributed.batched | INFO] Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:43391 remote=tcp://127.0.0.1:58590>\n",
      "biopython_component-1                           | Traceback (most recent call last):\n",
      "biopython_component-1                           |   File \"/usr/local/lib/python3.10/site-packages/distributed/batched.py\", line 115, in _background_send\n",
      "biopython_component-1                           |     nbytes = yield coro\n",
      "biopython_component-1                           |   File \"/usr/local/lib/python3.10/site-packages/tornado/gen.py\", line 766, in run\n",
      "biopython_component-1                           |     value = future.result()\n",
      "biopython_component-1                           |   File \"/usr/local/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 262, in write\n",
      "biopython_component-1                           |     raise CommClosedError()\n",
      "biopython_component-1                           | distributed.comm.core.CommClosedError\n",
      "biopython_component-1                           | [2024-09-05 09:55:51,248 | distributed.scheduler | INFO] Scheduler closing due to unknown reason...\n",
      "biopython_component-1                           | [2024-09-05 09:55:51,249 | distributed.scheduler | INFO] Scheduler closing all comms\n",
      "biopython_component-1                           | [2024-09-05 09:55:51,252 | fondant.component.executor | INFO] Saving output manifest to /home/pietercoussement/Software/deCYPher/protein-feature-extraction/feature_extraction_pipeline/feature_extraction_pipeline-20240905115537/biopython_component/manifest.json\n",
      "biopython_component-1                           | [2024-09-05 09:55:51,252 | fondant.component.executor | INFO] Writing cache key with manifest reference to /home/pietercoussement/Software/deCYPher/protein-feature-extraction/feature_extraction_pipeline/cache/ccc845a963f63fcf3b907dc9fafb991c.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[Kbiopython_component-1 exited with code 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:53,629 | fondant.cli | INFO] Component `GenerateProteinSequenceChecksumComponent` found in module main\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:53,633 | fondant.component.executor | INFO] Caching disabled for the component\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:53,633 | root | INFO] Executing component\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:53,936 | distributed.http.proxy | INFO] To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:53,960 | distributed.scheduler | INFO] State start\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:53,965 | distributed.scheduler | INFO]   Scheduler at:     tcp://127.0.0.1:38917\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:53,965 | distributed.scheduler | INFO]   dashboard at:  http://127.0.0.1:8787/status\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:53,965 | distributed.scheduler | INFO] Registering Worker plugin shuffle\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:53,986 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:37737'\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:53,992 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:40655'\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:53,994 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:46795'\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:53,997 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:34635'\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:54,011 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:38449'\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:54,014 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:35739'\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:54,017 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:40083'\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:54,023 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:36995'\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,352 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:35369\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,352 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:35369\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,352 | distributed.worker | INFO]           Worker name:                          6\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,352 | distributed.worker | INFO]          dashboard at:            127.0.0.1:44553\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,352 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:38917\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,352 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,352 | distributed.worker | INFO]               Threads:                          1\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,352 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,352 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-9gw4cp_t\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,352 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,360 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:35369', name: 6, status: init, memory: 0, processing: 0>\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,366 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:35369\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,366 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:36214\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,368 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,369 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:38917\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,369 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,370 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:38917\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,375 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:45877\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,375 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:45877\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,375 | distributed.worker | INFO]           Worker name:                          1\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,375 | distributed.worker | INFO]          dashboard at:            127.0.0.1:40167\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,375 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:38917\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,375 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,375 | distributed.worker | INFO]               Threads:                          1\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,376 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,376 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-cpl9jw7q\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,376 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,382 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:45877', name: 1, status: init, memory: 0, processing: 0>\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,385 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:45877\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,385 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:36230\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,385 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,386 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:38917\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,386 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,390 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:38917\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,400 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:43349\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,400 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:43349\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,400 | distributed.worker | INFO]           Worker name:                          4\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,400 | distributed.worker | INFO]          dashboard at:            127.0.0.1:33659\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,400 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:38917\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,400 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,400 | distributed.worker | INFO]               Threads:                          1\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,401 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,401 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-ju4sly8p\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,401 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,405 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:43349', name: 4, status: init, memory: 0, processing: 0>\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,406 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:43349\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,406 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:36242\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,407 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,408 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:38917\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,408 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,409 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:38917\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,411 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:35399\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,411 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:35399\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,411 | distributed.worker | INFO]           Worker name:                          7\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,411 | distributed.worker | INFO]          dashboard at:            127.0.0.1:34795\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,411 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:38917\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,411 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,411 | distributed.worker | INFO]               Threads:                          1\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,411 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,411 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-v39232im\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,412 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,419 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:35399', name: 7, status: init, memory: 0, processing: 0>\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,420 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:35399\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,420 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:36258\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,420 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,421 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:38917\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,421 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,422 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:38917\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,448 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:35235\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,448 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:35235\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,448 | distributed.worker | INFO]           Worker name:                          0\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,448 | distributed.worker | INFO]          dashboard at:            127.0.0.1:45777\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,448 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:38917\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,448 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,448 | distributed.worker | INFO]               Threads:                          1\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,449 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,449 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-f27s6a44\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,449 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,454 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:35235', name: 0, status: init, memory: 0, processing: 0>\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,454 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:35235\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,454 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:36268\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,455 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,455 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:38917\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,456 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,456 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:38917\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,475 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:33427\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,475 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:33427\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,475 | distributed.worker | INFO]           Worker name:                          2\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,475 | distributed.worker | INFO]          dashboard at:            127.0.0.1:39985\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,475 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:38917\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,475 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,475 | distributed.worker | INFO]               Threads:                          1\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,475 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,475 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-38xv7ts9\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,475 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,480 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:33427', name: 2, status: init, memory: 0, processing: 0>\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,481 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:33427\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,481 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:36270\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,481 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,482 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:38917\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,482 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,483 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:38917\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,541 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:35939\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,541 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:35939\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,541 | distributed.worker | INFO]           Worker name:                          5\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,541 | distributed.worker | INFO]          dashboard at:            127.0.0.1:41581\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,541 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:38917\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,541 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,541 | distributed.worker | INFO]               Threads:                          1\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,541 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,541 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-hk1qug46\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,541 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,546 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:35939', name: 5, status: init, memory: 0, processing: 0>\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,546 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:35939\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,547 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:36280\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,547 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,548 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:38917\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,548 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,548 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:38917\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,560 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:39803\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,560 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:39803\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,560 | distributed.worker | INFO]           Worker name:                          3\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,561 | distributed.worker | INFO]          dashboard at:            127.0.0.1:32931\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,561 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:38917\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,561 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,561 | distributed.worker | INFO]               Threads:                          1\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,561 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,561 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-jfjbz7pp\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,561 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,565 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:39803', name: 3, status: init, memory: 0, processing: 0>\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,567 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:39803\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,567 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:36292\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,567 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,568 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:38917\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,568 | distributed.worker | INFO] -------------------------------------------------\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,568 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:38917\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,597 | distributed.scheduler | INFO] Receive client connection: Client-0b3cfe4b-6b6d-11ef-8001-0242ac120002\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,598 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:36304\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,626 | root | INFO] Columns of dataframe: ['sequence']\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,636 | root | WARNING] Failed to infer dtype of index column, falling back to `string`. Specify the dtype explicitly to prevent this.\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,934 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:37737'. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,934 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,934 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:40655'. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,935 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,935 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:46795'. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,935 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:35235. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,935 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,935 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:34635'. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,935 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,935 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:45877. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,936 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:38449'. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,936 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,936 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:33427. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,936 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:35739'. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,936 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,936 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:39803. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,936 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:40083'. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,936 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,936 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:36995'. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,936 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:43349. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,936 | distributed.core | INFO] Connection to tcp://127.0.0.1:38917 has been closed.\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,937 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,937 | distributed.core | INFO] Connection to tcp://127.0.0.1:38917 has been closed.\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,937 | distributed.core | INFO] Connection to tcp://127.0.0.1:38917 has been closed.\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,937 | distributed.core | INFO] Connection to tcp://127.0.0.1:38917 has been closed.\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,937 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:35399. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,938 | distributed.core | INFO] Connection to tcp://127.0.0.1:38917 has been closed.\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,939 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:35939. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,939 | distributed.core | INFO] Connection to tcp://127.0.0.1:38917 has been closed.\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,941 | distributed.core | INFO] Connection to tcp://127.0.0.1:38917 has been closed.\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,941 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:36268; closing.\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,941 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:36230; closing.\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,941 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:36270; closing.\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,942 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:36292; closing.\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,942 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:35369. Reason: nanny-close\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,944 | distributed.core | INFO] Connection to tcp://127.0.0.1:38917 has been closed.\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,945 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:35235', name: 0, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1725530155.9451256')\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,947 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:45877', name: 1, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1725530155.9473476')\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,949 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:33427', name: 2, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1725530155.9491668')\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,950 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:39803', name: 3, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1725530155.9501827')\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,951 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:36242; closing.\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,955 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:43349', name: 4, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1725530155.9548135')\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,956 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:36258; closing.\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,958 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:36280; closing.\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,963 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:35399', name: 7, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1725530155.961479')\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,964 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:35939', name: 5, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1725530155.964528')\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,966 | distributed.batched | INFO] Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:38917 remote=tcp://127.0.0.1:36242>\n",
      "generate_protein_sequence_checksum_component-1  | Traceback (most recent call last):\n",
      "generate_protein_sequence_checksum_component-1  |   File \"/usr/local/lib/python3.10/site-packages/distributed/batched.py\", line 115, in _background_send\n",
      "generate_protein_sequence_checksum_component-1  |     nbytes = yield coro\n",
      "generate_protein_sequence_checksum_component-1  |   File \"/usr/local/lib/python3.10/site-packages/tornado/gen.py\", line 766, in run\n",
      "generate_protein_sequence_checksum_component-1  |     value = future.result()\n",
      "generate_protein_sequence_checksum_component-1  |   File \"/usr/local/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 262, in write\n",
      "generate_protein_sequence_checksum_component-1  |     raise CommClosedError()\n",
      "generate_protein_sequence_checksum_component-1  | distributed.comm.core.CommClosedError\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,972 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:36214; closing.\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,980 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:35369', name: 6, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1725530155.980423')\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:55,981 | distributed.scheduler | INFO] Lost all workers\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:56,330 | distributed.scheduler | INFO] Scheduler closing due to unknown reason...\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:56,330 | distributed.scheduler | INFO] Scheduler closing all comms\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:56,333 | fondant.component.executor | INFO] Saving output manifest to /home/pietercoussement/Software/deCYPher/protein-feature-extraction/feature_extraction_pipeline/feature_extraction_pipeline-20240905115537/generate_protein_sequence_checksum_component/manifest.json\n",
      "generate_protein_sequence_checksum_component-1  | [2024-09-05 09:55:56,333 | fondant.component.executor | INFO] Writing cache key with manifest reference to /home/pietercoussement/Software/deCYPher/protein-feature-extraction/feature_extraction_pipeline/cache/1188b2837ea38fae7c2448a8cb64169a.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[Kgenerate_protein_sequence_checksum_component-1 exited with code 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ifeatureomega_component-1                       | [2024-09-05 09:55:59,287 | matplotlib.font_manager | INFO] generated new fontManager\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:55:59,499 | fondant.cli | INFO] Component `IFeatureOmegaComponent` found in module main\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:55:59,510 | fondant.component.executor | INFO] Caching disabled for the component\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:55:59,510 | root | INFO] Executing component\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:55:59,817 | distributed.http.proxy | INFO] To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:55:59,840 | distributed.scheduler | INFO] State start\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:55:59,844 | distributed.scheduler | INFO]   Scheduler at:     tcp://127.0.0.1:42211\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:55:59,844 | distributed.scheduler | INFO]   dashboard at:  http://127.0.0.1:8787/status\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:55:59,844 | distributed.scheduler | INFO] Registering Worker plugin shuffle\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:55:59,863 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:39631'\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:55:59,867 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:41049'\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:55:59,869 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:37911'\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:55:59,870 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:40083'\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:55:59,881 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:34577'\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:55:59,884 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:34147'\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:55:59,887 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:39151'\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:55:59,897 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:41859'\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,304 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:42829\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,305 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:42829\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,305 | distributed.worker | INFO]           Worker name:                          0\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,305 | distributed.worker | INFO]          dashboard at:            127.0.0.1:43849\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,305 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:42211\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,305 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,305 | distributed.worker | INFO]               Threads:                          1\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,305 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,305 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-i47jpjtx\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,305 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,316 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:42829', name: 0, status: init, memory: 0, processing: 0>\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,319 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:36181\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,319 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:36181\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,320 | distributed.worker | INFO]           Worker name:                          1\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,320 | distributed.worker | INFO]          dashboard at:            127.0.0.1:33191\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,320 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:42211\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,320 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,320 | distributed.worker | INFO]               Threads:                          1\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,320 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,320 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-hg9hhnvv\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,321 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,323 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:42829\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,323 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:58736\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,324 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,324 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:42211\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,324 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,326 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:42211\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,333 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:36181', name: 1, status: init, memory: 0, processing: 0>\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,333 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:36181\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,334 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:58746\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,334 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,335 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:42211\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,335 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,336 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:43457\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,336 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:43457\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,336 | distributed.worker | INFO]           Worker name:                          3\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,336 | distributed.worker | INFO]          dashboard at:            127.0.0.1:46873\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,337 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:42211\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,337 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,337 | distributed.worker | INFO]               Threads:                          1\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,337 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,337 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:42211\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,337 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-gisnnpqm\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,338 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,339 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:46551\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,339 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:46551\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,339 | distributed.worker | INFO]           Worker name:                          5\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,339 | distributed.worker | INFO]          dashboard at:            127.0.0.1:33159\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,339 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:42211\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,339 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,339 | distributed.worker | INFO]               Threads:                          1\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,339 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,339 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-arefyxml\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,339 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,345 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:43457', name: 3, status: init, memory: 0, processing: 0>\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,346 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:43457\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,346 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:58762\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,347 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,348 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:42211\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,348 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,348 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:42211\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,349 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:46551', name: 5, status: init, memory: 0, processing: 0>\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,350 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:46551\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,350 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:58770\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,351 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,352 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:42211\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,352 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,353 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:42211\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,362 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:40827\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,362 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:40827\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,362 | distributed.worker | INFO]           Worker name:                          7\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,362 | distributed.worker | INFO]          dashboard at:            127.0.0.1:44993\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,362 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:42211\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,362 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,362 | distributed.worker | INFO]               Threads:                          1\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,362 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,362 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-39lndv10\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,362 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,368 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:40827', name: 7, status: init, memory: 0, processing: 0>\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,368 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:40827\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,369 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:58782\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,369 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,370 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:42211\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,370 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,370 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:42211\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,373 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:43275\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,373 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:43275\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,373 | distributed.worker | INFO]           Worker name:                          4\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,373 | distributed.worker | INFO]          dashboard at:            127.0.0.1:46351\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,373 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:42211\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,373 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,373 | distributed.worker | INFO]               Threads:                          1\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,373 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,373 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-3wnf_66u\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,373 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,377 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:34253\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,377 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:34253\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,377 | distributed.worker | INFO]           Worker name:                          6\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,377 | distributed.worker | INFO]          dashboard at:            127.0.0.1:33185\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,377 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:42211\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,377 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,377 | distributed.worker | INFO]               Threads:                          1\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,377 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,377 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-jtx78yet\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,378 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,380 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:43275', name: 4, status: init, memory: 0, processing: 0>\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,380 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:45639\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,380 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:45639\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,380 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:43275\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,380 | distributed.worker | INFO]           Worker name:                          2\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,380 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:58790\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,380 | distributed.worker | INFO]          dashboard at:            127.0.0.1:36359\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,381 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:42211\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,381 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,381 | distributed.worker | INFO]               Threads:                          1\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,381 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,381 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-bxk8a9_i\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,381 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,381 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,382 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:42211\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,382 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,383 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:42211\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,383 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:34253', name: 6, status: init, memory: 0, processing: 0>\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,383 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:34253\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,384 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:58802\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,384 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,385 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:42211\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,385 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,385 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:42211\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,386 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:45639', name: 2, status: init, memory: 0, processing: 0>\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,387 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:45639\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,387 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:58804\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,387 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,388 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:42211\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,388 | distributed.worker | INFO] -------------------------------------------------\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,389 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:42211\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,412 | distributed.scheduler | INFO] Receive client connection: Client-0eb48411-6b6d-11ef-8001-0242ac120002\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,413 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:58812\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,605 | fondant.component.data_io | INFO] Total number of rows is 8.\n",
      "ifeatureomega_component-1                       | Repartitioning the data from <dask.utils.IndexCallable object at 0x7f165c1cbdf0> partitions to have 2 such that the number of partitions per row is approximately5\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,605 | fondant.component.data_io | WARNING] Setting the `input partition rows` has caused the system to not utilize all available workers 2 out of 8 are used.\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,606 | root | INFO] Columns of dataframe: ['sequence', 'sequence_checksum']\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:01,660 | root | WARNING] Failed to infer dtype of index column, falling back to `string`. Specify the dtype explicitly to prevent this.\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:03,028 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:39631'. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:03,028 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:03,029 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:41049'. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:03,029 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:03,029 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:37911'. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:03,030 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:03,031 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:40083'. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:03,032 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:03,032 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:36181. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:03,032 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:34577'. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:03,032 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:45639. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:03,032 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:42829. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:03,033 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:03,033 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:34147'. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:03,034 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:03,034 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:39151'. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:03,035 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:43457. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:03,035 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:43275. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:03,035 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:03,035 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:41859'. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:03,036 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:03,037 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:46551. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:03,037 | distributed.core | INFO] Connection to tcp://127.0.0.1:42211 has been closed.\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:03,038 | distributed.core | INFO] Connection to tcp://127.0.0.1:42211 has been closed.\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:03,039 | distributed.core | INFO] Connection to tcp://127.0.0.1:42211 has been closed.\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:03,039 | distributed.core | INFO] Connection to tcp://127.0.0.1:42211 has been closed.\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:03,039 | distributed.core | INFO] Connection to tcp://127.0.0.1:42211 has been closed.\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:03,039 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:34253. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:03,041 | distributed.core | INFO] Connection to tcp://127.0.0.1:42211 has been closed.\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:03,041 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:40827. Reason: nanny-close\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:03,042 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:58746; closing.\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:03,043 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:58804; closing.\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:03,043 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:58736; closing.\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:03,044 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:58762; closing.\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:03,045 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:58790; closing.\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:03,046 | distributed.core | INFO] Connection to tcp://127.0.0.1:42211 has been closed.\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:03,048 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:36181', name: 1, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1725530163.0480046')\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:03,048 | distributed.core | INFO] Connection to tcp://127.0.0.1:42211 has been closed.\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:03,049 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:45639', name: 2, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1725530163.049582')\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:03,051 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:42829', name: 0, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1725530163.0509694')\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:03,052 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:43457', name: 3, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1725530163.0520513')\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:03,053 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:43275', name: 4, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1725530163.0530102')\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:03,054 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:58770; closing.\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:03,058 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:46551', name: 5, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1725530163.0578592')\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:03,059 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:58802; closing.\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:03,069 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:34253', name: 6, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1725530163.0695388')\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:03,071 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:58782; closing.\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:03,074 | distributed.batched | INFO] Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:42211 remote=tcp://127.0.0.1:58802>\n",
      "ifeatureomega_component-1                       | Traceback (most recent call last):\n",
      "ifeatureomega_component-1                       |   File \"/usr/local/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 297, in write\n",
      "ifeatureomega_component-1                       |     raise StreamClosedError()\n",
      "ifeatureomega_component-1                       | tornado.iostream.StreamClosedError: Stream is closed\n",
      "ifeatureomega_component-1                       | \n",
      "ifeatureomega_component-1                       | The above exception was the direct cause of the following exception:\n",
      "ifeatureomega_component-1                       | \n",
      "ifeatureomega_component-1                       | Traceback (most recent call last):\n",
      "ifeatureomega_component-1                       |   File \"/usr/local/lib/python3.10/site-packages/distributed/batched.py\", line 115, in _background_send\n",
      "ifeatureomega_component-1                       |     nbytes = yield coro\n",
      "ifeatureomega_component-1                       |   File \"/usr/local/lib/python3.10/site-packages/tornado/gen.py\", line 766, in run\n",
      "ifeatureomega_component-1                       |     value = future.result()\n",
      "ifeatureomega_component-1                       |   File \"/usr/local/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 307, in write\n",
      "ifeatureomega_component-1                       |     convert_stream_closed_error(self, e)\n",
      "ifeatureomega_component-1                       |   File \"/usr/local/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "ifeatureomega_component-1                       |     raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "ifeatureomega_component-1                       | distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:42211 remote=tcp://127.0.0.1:58802>: Stream is closed\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:03,095 | distributed.batched | INFO] Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:42211 remote=tcp://127.0.0.1:58770>\n",
      "ifeatureomega_component-1                       | Traceback (most recent call last):\n",
      "ifeatureomega_component-1                       |   File \"/usr/local/lib/python3.10/site-packages/distributed/batched.py\", line 115, in _background_send\n",
      "ifeatureomega_component-1                       |     nbytes = yield coro\n",
      "ifeatureomega_component-1                       |   File \"/usr/local/lib/python3.10/site-packages/tornado/gen.py\", line 766, in run\n",
      "ifeatureomega_component-1                       |     value = future.result()\n",
      "ifeatureomega_component-1                       |   File \"/usr/local/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 262, in write\n",
      "ifeatureomega_component-1                       |     raise CommClosedError()\n",
      "ifeatureomega_component-1                       | distributed.comm.core.CommClosedError\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:03,101 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:40827', name: 7, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1725530163.0962515')\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:03,101 | distributed.scheduler | INFO] Lost all workers\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:03,559 | distributed.scheduler | INFO] Scheduler closing due to unknown reason...\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:03,559 | distributed.scheduler | INFO] Scheduler closing all comms\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:03,563 | fondant.component.executor | INFO] Saving output manifest to /home/pietercoussement/Software/deCYPher/protein-feature-extraction/feature_extraction_pipeline/feature_extraction_pipeline-20240905115537/ifeatureomega_component/manifest.json\n",
      "ifeatureomega_component-1                       | [2024-09-05 09:56:03,563 | fondant.component.executor | INFO] Writing cache key with manifest reference to /home/pietercoussement/Software/deCYPher/protein-feature-extraction/feature_extraction_pipeline/cache/c2584693a74efa6d45423c693d0f729a.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[Kifeatureomega_component-1 exited with code 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "filter_pdb_component-1                          | [2024-09-05 09:56:06,027 | fondant.cli | INFO] Component `FilterPDBComponent` found in module main\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:06,035 | fondant.component.executor | INFO] Caching disabled for the component\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:06,035 | root | INFO] Executing component\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:06,403 | distributed.http.proxy | INFO] To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:06,428 | distributed.scheduler | INFO] State start\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:06,432 | distributed.scheduler | INFO]   Scheduler at:     tcp://127.0.0.1:40143\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:06,432 | distributed.scheduler | INFO]   dashboard at:  http://127.0.0.1:8787/status\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:06,433 | distributed.scheduler | INFO] Registering Worker plugin shuffle\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:06,505 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:39151'\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:06,512 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:46013'\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:06,515 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:42989'\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:06,517 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:34479'\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:06,530 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:33865'\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:06,534 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:42271'\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:06,537 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:38213'\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:06,547 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:40603'\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,781 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:44499\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,781 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:44499\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,781 | distributed.worker | INFO]           Worker name:                          0\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,781 | distributed.worker | INFO]          dashboard at:            127.0.0.1:37779\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,781 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:40143\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,781 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,781 | distributed.worker | INFO]               Threads:                          1\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,781 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,781 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-vvw6iyao\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,781 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,790 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:44499', name: 0, status: init, memory: 0, processing: 0>\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,793 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:44499\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,794 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:35694\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,794 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,795 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:40143\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,795 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,798 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:37229\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,798 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:37229\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,798 | distributed.worker | INFO]           Worker name:                          1\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,798 | distributed.worker | INFO]          dashboard at:            127.0.0.1:40371\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,798 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:40143\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,798 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,798 | distributed.worker | INFO]               Threads:                          1\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,798 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,798 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-hkzn9l2h\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,798 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,799 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:40143\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,803 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:37229', name: 1, status: init, memory: 0, processing: 0>\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,804 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:37229\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,804 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:35700\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,805 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,805 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:40143\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,805 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,806 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:40143\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,824 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:42495\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,825 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:42495\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,825 | distributed.worker | INFO]           Worker name:                          6\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,825 | distributed.worker | INFO]          dashboard at:            127.0.0.1:42847\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,825 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:40143\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,825 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,825 | distributed.worker | INFO]               Threads:                          1\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,825 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,825 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-libtfq8h\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,826 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,831 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:42495', name: 6, status: init, memory: 0, processing: 0>\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,832 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:42495\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,832 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:35702\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,832 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:45269\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,832 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:45269\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,832 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:37537\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,832 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,832 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:37537\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,833 | distributed.worker | INFO]           Worker name:                          3\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,833 | distributed.worker | INFO]          dashboard at:            127.0.0.1:46459\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,833 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:40143\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,833 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,833 | distributed.worker | INFO]           Worker name:                          5\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,833 | distributed.worker | INFO]               Threads:                          1\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,833 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,833 | distributed.worker | INFO]          dashboard at:            127.0.0.1:42711\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,833 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-xmllaubk\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,833 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:40143\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,833 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,833 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,833 | distributed.worker | INFO]               Threads:                          1\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,833 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,833 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-34jbqe91\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,833 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,833 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:40143\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,833 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,834 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:40143\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,834 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:46605\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,834 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:46605\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,834 | distributed.worker | INFO]           Worker name:                          4\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,835 | distributed.worker | INFO]          dashboard at:            127.0.0.1:40757\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,835 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:40143\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,835 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,835 | distributed.worker | INFO]               Threads:                          1\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,835 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,835 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-hv3fd25p\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,835 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,838 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:45269', name: 5, status: init, memory: 0, processing: 0>\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,839 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:45269\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,839 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:35708\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,840 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,840 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:40143\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,840 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,840 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:37537', name: 3, status: init, memory: 0, processing: 0>\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,841 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:37537\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,841 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:35706\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,841 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:40143\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,841 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,842 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:40143\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,842 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,842 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:46605', name: 4, status: init, memory: 0, processing: 0>\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,842 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:40143\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,842 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:46605\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,842 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:35710\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,843 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,844 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:40143\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,844 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,844 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:40143\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,869 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:44663\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,869 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:44663\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,869 | distributed.worker | INFO]           Worker name:                          7\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,869 | distributed.worker | INFO]          dashboard at:            127.0.0.1:39853\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,869 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:40143\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,869 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,869 | distributed.worker | INFO]               Threads:                          1\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,869 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,869 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-5h3igv7_\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,869 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,873 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:44663', name: 7, status: init, memory: 0, processing: 0>\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,874 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:44663\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,874 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:35714\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,874 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,874 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:40143\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,875 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,875 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:40143\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,881 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:46233\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,881 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:46233\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,881 | distributed.worker | INFO]           Worker name:                          2\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,881 | distributed.worker | INFO]          dashboard at:            127.0.0.1:42903\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,881 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:40143\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,882 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,882 | distributed.worker | INFO]               Threads:                          1\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,882 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,882 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-7od6bbwa\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,882 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,885 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:46233', name: 2, status: init, memory: 0, processing: 0>\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,885 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:46233\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,886 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:35726\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,886 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,886 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:40143\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,886 | distributed.worker | INFO] -------------------------------------------------\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,887 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:40143\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,900 | distributed.scheduler | INFO] Receive client connection: Client-12928ace-6b6d-11ef-8001-0242ac120002\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,900 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:35738\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,949 | fondant.component.data_io | INFO] The number of partitions of the input dataframe is 2. The available number of workers is 8.\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,950 | fondant.component.data_io | INFO] Repartitioning the data to 8 partitions before processing to maximize worker usage\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,950 | root | INFO] Columns of dataframe: ['sequence', 'sequence_checksum']\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:07,962 | root | WARNING] Failed to infer dtype of index column, falling back to `string`. Specify the dtype explicitly to prevent this.\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:08,417 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:39151'. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:08,417 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:08,417 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:46013'. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:08,418 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:08,418 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:42989'. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:08,419 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:44499. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:08,419 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:08,419 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:34479'. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:08,420 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:37229. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:08,420 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:08,421 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:33865'. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:08,421 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:46233. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:08,421 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:08,422 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:42271'. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:08,422 | distributed.core | INFO] Connection to tcp://127.0.0.1:40143 has been closed.\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:08,422 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:37537. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:08,422 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:08,423 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:38213'. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:08,423 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:08,423 | distributed.core | INFO] Connection to tcp://127.0.0.1:40143 has been closed.\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:08,423 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:40603'. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:08,424 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:08,424 | distributed.core | INFO] Connection to tcp://127.0.0.1:40143 has been closed.\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:08,425 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:46605. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:08,425 | distributed.core | INFO] Connection to tcp://127.0.0.1:40143 has been closed.\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:08,425 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:42495. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:08,426 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:45269. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:08,428 | distributed.core | INFO] Connection to tcp://127.0.0.1:40143 has been closed.\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:08,428 | distributed.core | INFO] Connection to tcp://127.0.0.1:40143 has been closed.\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:08,429 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:35694; closing.\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:08,429 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:35700; closing.\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:08,430 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:35726; closing.\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:08,430 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:35706; closing.\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:08,430 | distributed.core | INFO] Connection to tcp://127.0.0.1:40143 has been closed.\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:08,432 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:44499', name: 0, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1725530168.4320893')\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:08,433 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:37229', name: 1, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1725530168.4331353')\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:08,434 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:44663. Reason: nanny-close\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:08,434 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:46233', name: 2, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1725530168.4345095')\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:08,435 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:37537', name: 3, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1725530168.4355977')\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:08,436 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:35710; closing.\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:08,436 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:35702; closing.\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:08,439 | distributed.core | INFO] Connection to tcp://127.0.0.1:40143 has been closed.\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:08,439 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:46605', name: 4, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1725530168.4391093')\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:08,440 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:42495', name: 6, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1725530168.440173')\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:08,442 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:35708; closing.\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:08,445 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:45269', name: 5, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1725530168.4451888')\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:08,450 | distributed.batched | INFO] Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:40143 remote=tcp://127.0.0.1:35702>\n",
      "filter_pdb_component-1                          | Traceback (most recent call last):\n",
      "filter_pdb_component-1                          |   File \"/usr/local/lib/python3.10/site-packages/distributed/batched.py\", line 115, in _background_send\n",
      "filter_pdb_component-1                          |     nbytes = yield coro\n",
      "filter_pdb_component-1                          |   File \"/usr/local/lib/python3.10/site-packages/tornado/gen.py\", line 766, in run\n",
      "filter_pdb_component-1                          |     value = future.result()\n",
      "filter_pdb_component-1                          |   File \"/usr/local/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 262, in write\n",
      "filter_pdb_component-1                          |     raise CommClosedError()\n",
      "filter_pdb_component-1                          | distributed.comm.core.CommClosedError\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:08,464 | distributed.batched | INFO] Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:40143 remote=tcp://127.0.0.1:35710>\n",
      "filter_pdb_component-1                          | Traceback (most recent call last):\n",
      "filter_pdb_component-1                          |   File \"/usr/local/lib/python3.10/site-packages/distributed/batched.py\", line 115, in _background_send\n",
      "filter_pdb_component-1                          |     nbytes = yield coro\n",
      "filter_pdb_component-1                          |   File \"/usr/local/lib/python3.10/site-packages/tornado/gen.py\", line 766, in run\n",
      "filter_pdb_component-1                          |     value = future.result()\n",
      "filter_pdb_component-1                          |   File \"/usr/local/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 262, in write\n",
      "filter_pdb_component-1                          |     raise CommClosedError()\n",
      "filter_pdb_component-1                          | distributed.comm.core.CommClosedError\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:08,465 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:35714; closing.\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:08,465 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:44663', name: 7, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1725530168.4658291')\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:08,466 | distributed.scheduler | INFO] Lost all workers\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:08,763 | distributed.scheduler | INFO] Scheduler closing due to unknown reason...\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:08,764 | distributed.scheduler | INFO] Scheduler closing all comms\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:08,768 | fondant.component.executor | INFO] Saving output manifest to /home/pietercoussement/Software/deCYPher/protein-feature-extraction/feature_extraction_pipeline/feature_extraction_pipeline-20240905115537/filter_pdb_component/manifest.json\n",
      "filter_pdb_component-1                          | [2024-09-05 09:56:08,768 | fondant.component.executor | INFO] Writing cache key with manifest reference to /home/pietercoussement/Software/deCYPher/protein-feature-extraction/feature_extraction_pipeline/cache/79428601b17f58d1c2c55573167d0600.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[Kfilter_pdb_component-1 exited with code 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:11,087 | fondant.cli | INFO] Component `PredictProtein3DStructureComponent` found in module main\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:11,094 | fondant.component.executor | INFO] Caching disabled for the component\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:11,094 | root | INFO] Executing component\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:11,466 | distributed.http.proxy | INFO] To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:11,494 | distributed.scheduler | INFO] State start\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:11,498 | distributed.scheduler | INFO]   Scheduler at:     tcp://127.0.0.1:37667\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:11,499 | distributed.scheduler | INFO]   dashboard at:  http://127.0.0.1:8787/status\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:11,499 | distributed.scheduler | INFO] Registering Worker plugin shuffle\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:11,520 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:45387'\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:11,526 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:41357'\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:11,528 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:42405'\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:11,530 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:45207'\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:11,541 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:41853'\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:11,543 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:44957'\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:11,546 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:35459'\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:11,551 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:35563'\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,087 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:45469\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,087 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:45469\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,087 | distributed.worker | INFO]           Worker name:                          3\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,087 | distributed.worker | INFO]          dashboard at:            127.0.0.1:37133\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,087 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:37667\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,087 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,087 | distributed.worker | INFO]               Threads:                          1\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,087 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,087 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-afvbtn1n\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,087 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,095 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:45469', name: 3, status: init, memory: 0, processing: 0>\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,100 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:45469\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,101 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:59980\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,102 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,103 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:37667\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,104 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,106 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:45653\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,106 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:37667\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,106 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:45653\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,106 | distributed.worker | INFO]           Worker name:                          5\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,107 | distributed.worker | INFO]          dashboard at:            127.0.0.1:33457\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,107 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:37667\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,107 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,107 | distributed.worker | INFO]               Threads:                          1\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,107 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,107 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-tkjbz4se\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,107 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,110 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:41539\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,110 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:41539\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,110 | distributed.worker | INFO]           Worker name:                          4\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,110 | distributed.worker | INFO]          dashboard at:            127.0.0.1:33327\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,110 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:37667\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,110 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,110 | distributed.worker | INFO]               Threads:                          1\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,110 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,110 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-dhu387i6\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,111 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,114 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:39773\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,115 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:39773\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,115 | distributed.worker | INFO]           Worker name:                          2\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,115 | distributed.worker | INFO]          dashboard at:            127.0.0.1:40555\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,115 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:37667\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,115 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,115 | distributed.worker | INFO]               Threads:                          1\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,115 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,115 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-y1ieip5w\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,115 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,115 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:45653', name: 5, status: init, memory: 0, processing: 0>\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,115 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:41261\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,115 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:41261\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,116 | distributed.worker | INFO]           Worker name:                          0\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,116 | distributed.worker | INFO]          dashboard at:            127.0.0.1:43375\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,116 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:37667\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,116 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,116 | distributed.worker | INFO]               Threads:                          1\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,116 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,116 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-rwq8fi7u\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,116 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:45653\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,116 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,116 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:59988\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,117 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,117 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:37667\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,117 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,118 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:37667\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,119 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:41539', name: 4, status: init, memory: 0, processing: 0>\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,120 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:41539\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,120 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:59992\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,120 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,121 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:37667\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,121 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,122 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:37667\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,123 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:39773', name: 2, status: init, memory: 0, processing: 0>\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,124 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:39773\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,124 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:60000\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,125 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:41261', name: 0, status: init, memory: 0, processing: 0>\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,125 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,125 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:41261\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,125 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:60016\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,125 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:37667\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,125 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,126 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,126 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:37667\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,127 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:37667\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,127 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,127 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:40437\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,127 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:40437\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,127 | distributed.worker | INFO]           Worker name:                          7\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,127 | distributed.worker | INFO]          dashboard at:            127.0.0.1:45365\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,127 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:37667\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,127 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,127 | distributed.worker | INFO]               Threads:                          1\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,127 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,128 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:37667\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,128 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-5p0jv4wp\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,128 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,131 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:37225\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,131 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:37225\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,131 | distributed.worker | INFO]           Worker name:                          1\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,131 | distributed.worker | INFO]          dashboard at:            127.0.0.1:41105\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,132 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:37667\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,132 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,132 | distributed.worker | INFO]               Threads:                          1\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,132 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,132 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-jcoche0z\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,132 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,134 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:40437', name: 7, status: init, memory: 0, processing: 0>\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,135 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:40437\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,135 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:60020\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,135 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,136 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:37667\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,136 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,136 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:37299\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,136 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:37299\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,136 | distributed.worker | INFO]           Worker name:                          6\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,136 | distributed.worker | INFO]          dashboard at:            127.0.0.1:41355\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,136 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:37667\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,136 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:37667\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,137 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,137 | distributed.worker | INFO]               Threads:                          1\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,137 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,137 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-z2p6wwlt\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,137 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,138 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:37225', name: 1, status: init, memory: 0, processing: 0>\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,138 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:37225\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,138 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:60036\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,139 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,140 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:37667\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,140 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,140 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:37667\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,143 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:37299', name: 6, status: init, memory: 0, processing: 0>\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,144 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:37299\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,144 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:60038\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,144 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,145 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:37667\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,145 | distributed.worker | INFO] -------------------------------------------------\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,145 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:37667\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,165 | distributed.scheduler | INFO] Receive client connection: Client-15b5c1f0-6b6d-11ef-8001-0242ac120002\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,166 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:60050\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,205 | root | INFO] Columns of dataframe: ['sequence', 'sequence_checksum', 'pdb_string']\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,222 | root | WARNING] Failed to infer dtype of index column, falling back to `string`. Specify the dtype explicitly to prevent this.\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,541 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:45387'. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,542 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,542 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:41357'. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,542 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,543 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:42405'. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,544 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:41261. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,544 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:37225. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,544 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,544 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:45207'. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,545 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,545 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:41853'. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,546 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:39773. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,547 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,547 | distributed.core | INFO] Connection to tcp://127.0.0.1:37667 has been closed.\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,548 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:44957'. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,548 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,548 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:35459'. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,550 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:45469. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,550 | distributed.core | INFO] Connection to tcp://127.0.0.1:37667 has been closed.\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,550 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,550 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:45653. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,551 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:35563'. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,551 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:41539. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,551 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,552 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:37299. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,553 | distributed.core | INFO] Connection to tcp://127.0.0.1:37667 has been closed.\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,554 | distributed.core | INFO] Connection to tcp://127.0.0.1:37667 has been closed.\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,554 | distributed.core | INFO] Connection to tcp://127.0.0.1:37667 has been closed.\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,554 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:40437. Reason: nanny-close\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,555 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:60016; closing.\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,556 | distributed.core | INFO] Connection to tcp://127.0.0.1:37667 has been closed.\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,556 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:60036; closing.\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,557 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:60000; closing.\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,558 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:59980; closing.\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,558 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:59988; closing.\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,558 | distributed.core | INFO] Connection to tcp://127.0.0.1:37667 has been closed.\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,560 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:41261', name: 0, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1725530173.5603774')\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,562 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:37225', name: 1, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1725530173.5618775')\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,563 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:39773', name: 2, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1725530173.5631237')\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,563 | distributed.core | INFO] Connection to tcp://127.0.0.1:37667 has been closed.\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,564 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:45469', name: 3, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1725530173.5641897')\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,565 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:45653', name: 5, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1725530173.5657837')\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,567 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:59992; closing.\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,572 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:41539', name: 4, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1725530173.5718346')\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,573 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:60038; closing.\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,578 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:37299', name: 6, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1725530173.57849')\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,580 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:60020; closing.\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,583 | distributed.batched | INFO] Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:37667 remote=tcp://127.0.0.1:59992>\n",
      "predict_protein_3d_structure_component-1        | Traceback (most recent call last):\n",
      "predict_protein_3d_structure_component-1        |   File \"/usr/local/lib/python3.10/site-packages/distributed/batched.py\", line 115, in _background_send\n",
      "predict_protein_3d_structure_component-1        |     nbytes = yield coro\n",
      "predict_protein_3d_structure_component-1        |   File \"/usr/local/lib/python3.10/site-packages/tornado/gen.py\", line 766, in run\n",
      "predict_protein_3d_structure_component-1        |     value = future.result()\n",
      "predict_protein_3d_structure_component-1        |   File \"/usr/local/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 262, in write\n",
      "predict_protein_3d_structure_component-1        |     raise CommClosedError()\n",
      "predict_protein_3d_structure_component-1        | distributed.comm.core.CommClosedError\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,596 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:40437', name: 7, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1725530173.5967155')\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,597 | distributed.scheduler | INFO] Lost all workers\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,916 | distributed.scheduler | INFO] Scheduler closing due to unknown reason...\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,916 | distributed.scheduler | INFO] Scheduler closing all comms\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,921 | fondant.component.executor | INFO] Saving output manifest to /home/pietercoussement/Software/deCYPher/protein-feature-extraction/feature_extraction_pipeline/feature_extraction_pipeline-20240905115537/predict_protein_3d_structure_component/manifest.json\n",
      "predict_protein_3d_structure_component-1        | [2024-09-05 09:56:13,921 | fondant.component.executor | INFO] Writing cache key with manifest reference to /home/pietercoussement/Software/deCYPher/protein-feature-extraction/feature_extraction_pipeline/cache/61108703c7168370a88934206e8b6d3f.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[Kpredict_protein_3d_structure_component-1 exited with code 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "store_pdb_component-1                           | [2024-09-05 09:56:16,392 | fondant.cli | INFO] Component `StorePDBComponent` found in module main\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:16,400 | fondant.component.executor | INFO] Caching disabled for the component\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:16,401 | root | INFO] Executing component\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:16,712 | distributed.http.proxy | INFO] To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:16,739 | distributed.scheduler | INFO] State start\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:16,742 | distributed.scheduler | INFO]   Scheduler at:     tcp://127.0.0.1:35799\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:16,743 | distributed.scheduler | INFO]   dashboard at:  http://127.0.0.1:8787/status\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:16,743 | distributed.scheduler | INFO] Registering Worker plugin shuffle\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:16,820 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:35305'\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:16,826 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:36517'\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:16,827 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:43519'\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:16,830 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:34063'\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:16,840 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:45401'\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:16,841 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:38745'\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:16,844 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:34669'\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:16,853 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:38465'\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,345 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:36291\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,345 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:36291\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,345 | distributed.worker | INFO]           Worker name:                          0\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,345 | distributed.worker | INFO]          dashboard at:            127.0.0.1:35957\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,345 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:35799\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,345 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,345 | distributed.worker | INFO]               Threads:                          1\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,345 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,345 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-362ozjii\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,345 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,354 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:36291', name: 0, status: init, memory: 0, processing: 0>\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,359 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:42139\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,359 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:36291\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,359 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:41106\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,359 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:42139\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,359 | distributed.worker | INFO]           Worker name:                          6\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,360 | distributed.worker | INFO]          dashboard at:            127.0.0.1:37475\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,360 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:35799\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,360 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,360 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,360 | distributed.worker | INFO]               Threads:                          1\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,360 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,360 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-e16kyi2z\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,360 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,363 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:35799\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,364 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,365 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:35799\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,369 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:42139', name: 6, status: init, memory: 0, processing: 0>\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,371 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:42139\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,371 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:41112\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,372 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,372 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:35799\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,372 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,372 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:33821\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,373 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:33821\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,373 | distributed.worker | INFO]           Worker name:                          1\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,373 | distributed.worker | INFO]          dashboard at:            127.0.0.1:43845\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,373 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:35799\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,373 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,373 | distributed.worker | INFO]               Threads:                          1\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,373 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,373 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-sbmv_2c_\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,373 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,373 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:45573\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,373 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:45573\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,373 | distributed.worker | INFO]           Worker name:                          3\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,373 | distributed.worker | INFO]          dashboard at:            127.0.0.1:40443\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,373 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:35799\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,373 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,373 | distributed.worker | INFO]               Threads:                          1\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,373 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:35799\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,374 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,374 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-22dang_9\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,374 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,380 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:33821', name: 1, status: init, memory: 0, processing: 0>\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,381 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:33821\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,382 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:41124\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,382 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,382 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:35799\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,383 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,383 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:45573', name: 3, status: init, memory: 0, processing: 0>\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,383 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:35799\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,384 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:45573\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,384 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:41132\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,385 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,386 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:35799\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,386 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,387 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:35799\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,414 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:39503\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,414 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:39503\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,414 | distributed.worker | INFO]           Worker name:                          2\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,415 | distributed.worker | INFO]          dashboard at:            127.0.0.1:33727\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,415 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:35799\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,415 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,415 | distributed.worker | INFO]               Threads:                          1\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,415 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,415 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-8xnrm8jj\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,415 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,422 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:39503', name: 2, status: init, memory: 0, processing: 0>\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,422 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:39503\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,422 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:41142\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,423 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,424 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:35799\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,424 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,425 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:35799\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,429 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:45899\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,429 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:45899\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,429 | distributed.worker | INFO]           Worker name:                          4\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,429 | distributed.worker | INFO]          dashboard at:            127.0.0.1:33655\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,429 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:35799\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,429 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,429 | distributed.worker | INFO]               Threads:                          1\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,429 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,429 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-6byln5oc\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,429 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,432 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:45343\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,433 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:45343\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,433 | distributed.worker | INFO]           Worker name:                          7\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,433 | distributed.worker | INFO]          dashboard at:            127.0.0.1:42335\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,433 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:35799\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,433 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,433 | distributed.worker | INFO]               Threads:                          1\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,433 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,433 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-97brfeim\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,433 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,434 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:33251\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,434 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:33251\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,434 | distributed.worker | INFO]           Worker name:                          5\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,434 | distributed.worker | INFO]          dashboard at:            127.0.0.1:41001\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,434 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:35799\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,434 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,434 | distributed.worker | INFO]               Threads:                          1\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,434 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,435 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-juuhfda_\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,435 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,435 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:45899', name: 4, status: init, memory: 0, processing: 0>\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,436 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:45899\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,436 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:41158\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,437 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,437 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:35799\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,437 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,438 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:35799\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,438 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:45343', name: 7, status: init, memory: 0, processing: 0>\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,439 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:45343\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,439 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:41164\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,440 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,440 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:35799\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,440 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,441 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:35799\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,442 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:33251', name: 5, status: init, memory: 0, processing: 0>\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,443 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:33251\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,443 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:41178\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,443 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,444 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:35799\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,444 | distributed.worker | INFO] -------------------------------------------------\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,444 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:35799\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,471 | distributed.scheduler | INFO] Receive client connection: Client-18df21bd-6b6d-11ef-8001-0242ac120002\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,472 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:41182\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,508 | root | INFO] Columns of dataframe: ['sequence', 'sequence_checksum', 'pdb_string']\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:18,525 | root | WARNING] Failed to infer dtype of index column, falling back to `string`. Specify the dtype explicitly to prevent this.\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:19,046 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:35305'. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:19,047 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:19,047 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:36517'. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:19,048 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:19,048 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:43519'. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:19,048 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:19,049 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:34063'. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:19,050 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:33821. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:19,050 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:39503. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:19,050 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:36291. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:19,051 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:19,051 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:45401'. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:19,051 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:19,051 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:38745'. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:19,052 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:45899. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:19,052 | distributed.core | INFO] Connection to tcp://127.0.0.1:35799 has been closed.\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:19,052 | distributed.core | INFO] Connection to tcp://127.0.0.1:35799 has been closed.\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:19,053 | distributed.core | INFO] Connection to tcp://127.0.0.1:35799 has been closed.\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:19,054 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:45573. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:19,055 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:19,055 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:34669'. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:19,056 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:19,056 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:38465'. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:19,056 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:19,056 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:33251. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:19,056 | distributed.core | INFO] Connection to tcp://127.0.0.1:35799 has been closed.\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:19,057 | distributed.core | INFO] Connection to tcp://127.0.0.1:35799 has been closed.\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:19,058 | distributed.core | INFO] Connection to tcp://127.0.0.1:35799 has been closed.\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:19,060 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:41124; closing.\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:19,065 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:41142; closing.\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:19,066 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:41106; closing.\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:19,066 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:41158; closing.\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:19,066 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:41132; closing.\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:19,067 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:33821', name: 1, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1725530179.0672586')\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:19,068 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:39503', name: 2, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1725530179.0679188')\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:19,068 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:36291', name: 0, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1725530179.06841')\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:19,068 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:45899', name: 4, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1725530179.0688288')\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:19,075 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:45573', name: 3, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1725530179.0749075')\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:19,075 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:41178; closing.\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:19,076 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:45343. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:19,077 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:42139. Reason: nanny-close\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:19,079 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:33251', name: 5, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1725530179.0795374')\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:19,079 | distributed.core | INFO] Connection to tcp://127.0.0.1:35799 has been closed.\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:19,081 | distributed.core | INFO] Connection to tcp://127.0.0.1:35799 has been closed.\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:19,091 | distributed.batched | INFO] Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:35799 remote=tcp://127.0.0.1:41178>\n",
      "store_pdb_component-1                           | Traceback (most recent call last):\n",
      "store_pdb_component-1                           |   File \"/usr/local/lib/python3.10/site-packages/distributed/batched.py\", line 115, in _background_send\n",
      "store_pdb_component-1                           |     nbytes = yield coro\n",
      "store_pdb_component-1                           |   File \"/usr/local/lib/python3.10/site-packages/tornado/gen.py\", line 766, in run\n",
      "store_pdb_component-1                           |     value = future.result()\n",
      "store_pdb_component-1                           |   File \"/usr/local/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 262, in write\n",
      "store_pdb_component-1                           |     raise CommClosedError()\n",
      "store_pdb_component-1                           | distributed.comm.core.CommClosedError\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:19,098 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:41112; closing.\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:19,098 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:41164; closing.\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:19,100 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:42139', name: 6, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1725530179.100254')\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:19,101 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:45343', name: 7, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1725530179.1010065')\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:19,101 | distributed.scheduler | INFO] Lost all workers\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:19,417 | distributed.scheduler | INFO] Scheduler closing due to unknown reason...\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:19,418 | distributed.scheduler | INFO] Scheduler closing all comms\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:19,423 | fondant.component.executor | INFO] Saving output manifest to /home/pietercoussement/Software/deCYPher/protein-feature-extraction/feature_extraction_pipeline/feature_extraction_pipeline-20240905115537/store_pdb_component/manifest.json\n",
      "store_pdb_component-1                           | [2024-09-05 09:56:19,423 | fondant.component.executor | INFO] Writing cache key with manifest reference to /home/pietercoussement/Software/deCYPher/protein-feature-extraction/feature_extraction_pipeline/cache/b99ba9b4124de2d232d9eecf4b633615.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[Kstore_pdb_component-1 exited with code 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "msa_component-1                                 | [2024-09-05 09:56:22,091 | fondant.cli | INFO] Component `MSAComponent` found in module main\n",
      "msa_component-1                                 | [2024-09-05 09:56:22,100 | fondant.component.executor | INFO] Caching disabled for the component\n",
      "msa_component-1                                 | [2024-09-05 09:56:22,100 | root | INFO] Executing component\n",
      "msa_component-1                                 | [2024-09-05 09:56:22,472 | distributed.http.proxy | INFO] To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy\n",
      "msa_component-1                                 | [2024-09-05 09:56:22,497 | distributed.scheduler | INFO] State start\n",
      "msa_component-1                                 | [2024-09-05 09:56:22,501 | distributed.scheduler | INFO]   Scheduler at:     tcp://127.0.0.1:39887\n",
      "msa_component-1                                 | [2024-09-05 09:56:22,501 | distributed.scheduler | INFO]   dashboard at:  http://127.0.0.1:8787/status\n",
      "msa_component-1                                 | [2024-09-05 09:56:22,502 | distributed.scheduler | INFO] Registering Worker plugin shuffle\n",
      "msa_component-1                                 | [2024-09-05 09:56:22,522 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:41439'\n",
      "msa_component-1                                 | [2024-09-05 09:56:22,527 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:45529'\n",
      "msa_component-1                                 | [2024-09-05 09:56:22,529 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:37289'\n",
      "msa_component-1                                 | [2024-09-05 09:56:22,531 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:40987'\n",
      "msa_component-1                                 | [2024-09-05 09:56:22,548 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:46403'\n",
      "msa_component-1                                 | [2024-09-05 09:56:22,550 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:39665'\n",
      "msa_component-1                                 | [2024-09-05 09:56:22,557 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:34451'\n",
      "msa_component-1                                 | [2024-09-05 09:56:22,559 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:39359'\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,046 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:44035\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,046 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:44035\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,046 | distributed.worker | INFO]           Worker name:                          0\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,047 | distributed.worker | INFO]          dashboard at:            127.0.0.1:34941\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,047 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:39887\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,047 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,047 | distributed.worker | INFO]               Threads:                          1\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,047 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,047 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-n86fyr6w\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,047 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,057 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:44035', name: 0, status: init, memory: 0, processing: 0>\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,070 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:44035\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,070 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:58010\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,072 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,073 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:39887\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,073 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,077 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:39887\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,083 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:44395\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,083 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:44395\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,084 | distributed.worker | INFO]           Worker name:                          3\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,084 | distributed.worker | INFO]          dashboard at:            127.0.0.1:33427\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,084 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:39887\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,084 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,084 | distributed.worker | INFO]               Threads:                          1\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,084 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,084 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-wd_q6dm2\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,084 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,089 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:35925\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,089 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:35925\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,090 | distributed.worker | INFO]           Worker name:                          1\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,090 | distributed.worker | INFO]          dashboard at:            127.0.0.1:35733\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,090 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:39887\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,090 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,090 | distributed.worker | INFO]               Threads:                          1\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,090 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,090 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-0l5bdibv\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,090 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,093 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:44395', name: 3, status: init, memory: 0, processing: 0>\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,094 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:44395\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,094 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:58012\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,094 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,095 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:39887\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,095 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,096 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:39887\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,099 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:35925', name: 1, status: init, memory: 0, processing: 0>\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,100 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:35925\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,100 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:58024\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,100 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,101 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:39887\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,102 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,102 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:39887\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,105 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:45179\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,105 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:45179\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,105 | distributed.worker | INFO]           Worker name:                          4\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,105 | distributed.worker | INFO]          dashboard at:            127.0.0.1:38433\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,105 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:39887\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,105 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,105 | distributed.worker | INFO]               Threads:                          1\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,105 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,105 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-cwrsliks\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,106 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,106 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:44507\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,106 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:44507\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,106 | distributed.worker | INFO]           Worker name:                          2\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,106 | distributed.worker | INFO]          dashboard at:            127.0.0.1:46679\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,106 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:39887\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,107 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,107 | distributed.worker | INFO]               Threads:                          1\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,107 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,107 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-ojntk41z\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,107 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,113 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:45179', name: 4, status: init, memory: 0, processing: 0>\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,113 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:36339\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,113 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:36339\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,113 | distributed.worker | INFO]           Worker name:                          5\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,113 | distributed.worker | INFO]          dashboard at:            127.0.0.1:40303\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,113 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:39887\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,113 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,113 | distributed.worker | INFO]               Threads:                          1\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,113 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,113 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-lzmwz2xd\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,113 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,114 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:45179\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,114 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:58036\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,114 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,115 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:44507', name: 2, status: init, memory: 0, processing: 0>\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,115 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:39887\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,115 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,115 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:44507\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,115 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:58042\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,116 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:39887\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,116 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,117 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:39887\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,117 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,118 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:39887\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,121 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:36339', name: 5, status: init, memory: 0, processing: 0>\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,121 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:36339\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,122 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:58054\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,122 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,123 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:39887\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,123 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,123 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:39887\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,145 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:46301\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,145 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:46301\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,145 | distributed.worker | INFO]           Worker name:                          6\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,145 | distributed.worker | INFO]          dashboard at:            127.0.0.1:40095\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,145 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:39887\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,145 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,145 | distributed.worker | INFO]               Threads:                          1\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,145 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,145 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-a1geipty\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,145 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,170 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:46301', name: 6, status: init, memory: 0, processing: 0>\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,171 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:46301\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,171 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:58058\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,171 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,172 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:36947\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,172 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:36947\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,172 | distributed.worker | INFO]           Worker name:                          7\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,172 | distributed.worker | INFO]          dashboard at:            127.0.0.1:34809\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,172 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:39887\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,172 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:39887\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,172 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,172 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,172 | distributed.worker | INFO]               Threads:                          1\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,172 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,172 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-mz1pzo2u\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,173 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,173 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:39887\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,179 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:36947', name: 7, status: init, memory: 0, processing: 0>\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,180 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:36947\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,180 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:58066\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,181 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,182 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:39887\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,182 | distributed.worker | INFO] -------------------------------------------------\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,183 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:39887\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,254 | distributed.scheduler | INFO] Receive client connection: Client-1c4f675c-6b6d-11ef-8001-0242ac120002\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,255 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:58080\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,388 | fondant.component.data_io | INFO] Total number of rows is 8.\n",
      "msa_component-1                                 | Repartitioning the data from <dask.utils.IndexCallable object at 0x7f7881a58430> partitions to have 1 such that the number of partitions per row is approximately10000\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,388 | fondant.component.data_io | WARNING] Setting the `input partition rows` has caused the system to not utilize all available workers 1 out of 8 are used.\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,389 | root | INFO] Columns of dataframe: ['sequence', 'sequence_checksum']\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,410 | root | WARNING] Failed to infer dtype of index column, falling back to `string`. Specify the dtype explicitly to prevent this.\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,528 | distributed.worker | WARNING] Compute Failed\n",
      "msa_component-1                                 | Key:       ('getitem-9446e44af462e944bc81a44c72deefb3', 0)\n",
      "msa_component-1                                 | Function:  subgraph_callable-ba219433d8728d4e8cf0b89990e249c5\n",
      "msa_component-1                                 | args:      (['sequence', 'sequence_checksum', 'msa_sequence'], Index([], dtype='object', name='id'), 'index',                                               sequence     sequence_checksum\n",
      "msa_component-1                                 | id                                                                          \n",
      "msa_component-1                                 | 0_1  MNQRGMPIQSLVTNVKINRLEENDCIHTRHRVRPGRTDGKNLHAMM...  CRC-94CF2EE011C80480\n",
      "msa_component-1                                 | 0_2  MAGLKPEVPLHDGINKFGKSDFAGQEGPKIVTTTDKALLVANGALK...  CRC-68D748EC385E9BEC\n",
      "msa_component-1                                 | 0_3  MVDLKKELKNFVDSDFPGSPKQEAQGIDVRILLSFNNAAFREALII...  CRC-3B9E0764E7D3C737\n",
      "msa_component-1                                 | 0_4  MELILAKARLEFECDWGLLMLEPCVPPTKIFADRNYAVGVMFESDK...  CRC-B08C4E4E86E87F17\n",
      "msa_component-1                                 | 0_5  MRVLCDGSTGYACAKNTRIRFREKVASVLAKIQGYEQTFPHHMPNM...  CRC-747F108552578E1D\n",
      "msa_component-1                                 | 0_6  MRVLCDGSTGYACAKNTRIRFREKVASVLAKIQGYEQTFPHHMPNM...  CRC-747F108552578E1D\n",
      "msa_component-1                                 | 0_7  MRVLCDGSTGYACAKNTRIRFREKVASVLAKIQGYEQTFPHHMPNM...  CRC-747F108552578E1D\n",
      "msa_component-1                                 | 0_8  MRVLCDGSTGYACAKNTRIRFREKVASVLAKIQGYEQTFPHHMPNM...  CRC-747F108552578E1D, 'repartition-1-b9eb7fe1014e9fbe13007753e3b6674a')\n",
      "msa_component-1                                 | kwargs:    {}\n",
      "msa_component-1                                 | Exception: \"FileNotFoundError(2, 'No such file or directory')\"\n",
      "msa_component-1                                 | \n",
      "msa_component-1                                 | Traceback (most recent call last):\n",
      "msa_component-1                                 |   File \"/usr/local/bin/fondant\", line 8, in <module>\n",
      "msa_component-1                                 |     sys.exit(entrypoint())\n",
      "msa_component-1                                 |   File \"/usr/local/lib/python3.10/site-packages/fondant/cli.py\", line 90, in entrypoint\n",
      "msa_component-1                                 |     args.func(args)\n",
      "msa_component-1                                 |   File \"/usr/local/lib/python3.10/site-packages/fondant/cli.py\", line 749, in execute\n",
      "msa_component-1                                 |     executor.execute(component)\n",
      "msa_component-1                                 |   File \"/usr/local/lib/python3.10/site-packages/fondant/component/executor.py\", line 349, in execute\n",
      "msa_component-1                                 |     output_manifest = self._run_execution(component_cls, input_manifest)\n",
      "msa_component-1                                 |   File \"/usr/local/lib/python3.10/site-packages/fondant/component/executor.py\", line 312, in _run_execution\n",
      "msa_component-1                                 |     self._write_data(dataframe=output_df, manifest=output_manifest)\n",
      "msa_component-1                                 |   File \"/usr/local/lib/python3.10/site-packages/fondant/component/executor.py\", line 229, in _write_data\n",
      "msa_component-1                                 |     data_writer.write_dataframe(dataframe)\n",
      "msa_component-1                                 |   File \"/usr/local/lib/python3.10/site-packages/fondant/component/data_io.py\", line 178, in write_dataframe\n",
      "msa_component-1                                 |     self._write_dataframe(dataframe)\n",
      "msa_component-1                                 |   File \"/usr/local/lib/python3.10/site-packages/fondant/component/data_io.py\", line 252, in _write_dataframe\n",
      "msa_component-1                                 |     future.result()\n",
      "msa_component-1                                 |   File \"/usr/local/lib/python3.10/site-packages/distributed/client.py\", line 323, in result\n",
      "msa_component-1                                 |     return self.client.sync(self._result, callback_timeout=timeout)\n",
      "msa_component-1                                 |   File \"/usr/local/lib/python3.10/site-packages/fondant/component/executor.py\", line 510, in wrapped_transform\n",
      "msa_component-1                                 |     dataframe = transform(dataframe)\n",
      "msa_component-1                                 |   File \"/component/src/./main.py\", line 31, in transform\n",
      "msa_component-1                                 |     msa_file_content = self.execute_clustalo_cmd(dataframe)\n",
      "msa_component-1                                 |   File \"/component/src/./main.py\", line 66, in execute_clustalo_cmd\n",
      "msa_component-1                                 |     with open(output_file, \"w\") as f:\n",
      "msa_component-1                                 | FileNotFoundError: [Errno 2] No such file or directory: '/.fondant/msa.fasta'\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,537 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:41439'. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,537 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,537 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:45529'. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,537 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,538 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:37289'. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,538 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:44035. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,538 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,538 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:40987'. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,538 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,538 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:35925. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,539 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:46403'. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,539 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,539 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:39665'. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,539 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:44507. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,539 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,539 | distributed.core | INFO] Connection to tcp://127.0.0.1:39887 has been closed.\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,539 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:34451'. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,539 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,540 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:39359'. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,540 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:45179. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,540 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,540 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:36339. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,541 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:58010; closing.\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,541 | distributed.core | INFO] Connection to tcp://127.0.0.1:39887 has been closed.\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,541 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:58024; closing.\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,541 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:44035', name: 0, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1725530184.5416746')\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,542 | distributed.core | INFO] Connection to tcp://127.0.0.1:39887 has been closed.\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,542 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:44395. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,542 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:35925', name: 1, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1725530184.542237')\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,542 | distributed.core | INFO] Connection to tcp://127.0.0.1:39887 has been closed.\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,543 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:58036; closing.\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,543 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:46301. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,543 | distributed.core | INFO] Connection to tcp://127.0.0.1:39887 has been closed.\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,544 | distributed.core | INFO] Connection to tcp://127.0.0.1:39887 has been closed.\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,544 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:45179', name: 4, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1725530184.5440412')\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,544 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:58042; closing.\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,545 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:36947. Reason: nanny-close\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,545 | distributed.core | INFO] Connection to tcp://127.0.0.1:39887 has been closed.\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,547 | distributed.core | INFO] Connection to tcp://127.0.0.1:39887 has been closed.\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,545 | distributed.batched | INFO] Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:39887 remote=tcp://127.0.0.1:58036>\n",
      "msa_component-1                                 | Traceback (most recent call last):\n",
      "msa_component-1                                 |   File \"/usr/local/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 297, in write\n",
      "msa_component-1                                 |     raise StreamClosedError()\n",
      "msa_component-1                                 | tornado.iostream.StreamClosedError: Stream is closed\n",
      "msa_component-1                                 | \n",
      "msa_component-1                                 | The above exception was the direct cause of the following exception:\n",
      "msa_component-1                                 | \n",
      "msa_component-1                                 | Traceback (most recent call last):\n",
      "msa_component-1                                 |   File \"/usr/local/lib/python3.10/site-packages/distributed/batched.py\", line 115, in _background_send\n",
      "msa_component-1                                 |     nbytes = yield coro\n",
      "msa_component-1                                 |   File \"/usr/local/lib/python3.10/site-packages/tornado/gen.py\", line 766, in run\n",
      "msa_component-1                                 |     value = future.result()\n",
      "msa_component-1                                 |   File \"/usr/local/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 307, in write\n",
      "msa_component-1                                 |     convert_stream_closed_error(self, e)\n",
      "msa_component-1                                 |   File \"/usr/local/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "msa_component-1                                 |     raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "msa_component-1                                 | distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:39887 remote=tcp://127.0.0.1:58036>: Stream is closed\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,551 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:44507', name: 2, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1725530184.5508604')\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,552 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:58012; closing.\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,554 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:58054; closing.\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,555 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:44395', name: 3, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1725530184.5554175')\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,556 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:36339', name: 5, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1725530184.5561173')\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,556 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:58058; closing.\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,558 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:46301', name: 6, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1725530184.5585723')\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,559 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:58066; closing.\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,560 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:36947', name: 7, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1725530184.560381')\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,561 | distributed.scheduler | INFO] Lost all workers\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,562 | distributed.batched | INFO] Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:39887 remote=tcp://127.0.0.1:58058>\n",
      "msa_component-1                                 | Traceback (most recent call last):\n",
      "msa_component-1                                 |   File \"/usr/local/lib/python3.10/site-packages/distributed/batched.py\", line 115, in _background_send\n",
      "msa_component-1                                 |     nbytes = yield coro\n",
      "msa_component-1                                 |   File \"/usr/local/lib/python3.10/site-packages/tornado/gen.py\", line 766, in run\n",
      "msa_component-1                                 |     value = future.result()\n",
      "msa_component-1                                 |   File \"/usr/local/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 262, in write\n",
      "msa_component-1                                 |     raise CommClosedError()\n",
      "msa_component-1                                 | distributed.comm.core.CommClosedError\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,860 | distributed.scheduler | INFO] Scheduler closing due to unknown reason...\n",
      "msa_component-1                                 | [2024-09-05 09:56:24,861 | distributed.scheduler | INFO] Scheduler closing all comms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[Kmsa_component-1 exited with code 1\n",
      "Finished workflow run.\n"
     ]
    }
   ],
   "source": [
    "# import shutil\n",
    "\n",
    "# remove the most recent output folder if the manifest file is removed\n",
    "# without a manifest file in the most recent output folder, the pipeline cannot be run\n",
    "# if OUTPUT_FOLDER and REMOVED_MANIFEST:\n",
    "# \tshutil.rmtree(OUTPUT_FOLDER)\n",
    "# \t# remove cache\n",
    "# \tshutil.rmtree(os.path.join(BASE_PATH, PIPELINE_NAME, \"cache\"))\n",
    "\n",
    "# get current full path to the project\n",
    "mounted_data = os.path.join(os.path.abspath(\"data\"), \":/data\")\n",
    "\n",
    "DockerRunner().run(dataset=dataset, working_directory=BASE_PATH, extra_volumes=mounted_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "The following results have been taken from the output of the pipeline, which is stored in the `.fondant` directory. This directory contains the output of each component, together with the cache of the previous run. Currently, the pipeline doesn't implement the `write_to_file` component, so the results will be taken individually from the output of each component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-05 11:56:25,308 | root | INFO] Last folder: ./feature_extraction_pipeline/feature_extraction_pipeline-20240905115537\n"
     ]
    }
   ],
   "source": [
    "# find the most recent output folder\n",
    "# get the most recent folder in the folder named: BASE_PATH + PIPELINE_NAME + PIPELINE_NAME-<timestamp>\n",
    "matching_folders = glob.glob(f\"{BASE_PATH}/{DATASET_NAME}/{DATASET_NAME}-*\")\n",
    "\n",
    "if matching_folders:\n",
    "    last_folder = max(matching_folders, key=os.path.getctime)\n",
    "\n",
    "logging.info(f\"Last folder: {last_folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def merge_parquet_folders(folder_path):\n",
    "    df_list = []\n",
    "\n",
    "    for folder in Path(folder_path).iterdir():\n",
    "        if folder.is_dir():\n",
    "            logging.info(f\"Reading parquet partitions from: {folder}\")\n",
    "            parquet_files = list(folder.glob(\"*.parquet\"))\n",
    "            logging.info(f\"Found {len(parquet_files)} parquet files\")\n",
    "            dfs = [pd.read_parquet(file) for file in parquet_files]\n",
    "            dfs = [x for x in dfs if not x.empty]\n",
    "            if len(dfs) == 0:\n",
    "                continue\n",
    "            df = pd.concat(dfs)\n",
    "            df_list.append(df)\n",
    "\n",
    "    return df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-05 11:56:25,333 | root | INFO] Reading parquet partitions from: feature_extraction_pipeline/feature_extraction_pipeline-20240905115537/filter_pdb_component\n",
      "[2024-09-05 11:56:25,335 | root | INFO] Found 8 parquet files\n",
      "[2024-09-05 11:56:25,374 | root | INFO] Reading parquet partitions from: feature_extraction_pipeline/feature_extraction_pipeline-20240905115537/predict_protein_3d_structure_component\n",
      "[2024-09-05 11:56:25,375 | root | INFO] Found 8 parquet files\n",
      "[2024-09-05 11:56:25,408 | root | INFO] Reading parquet partitions from: feature_extraction_pipeline/feature_extraction_pipeline-20240905115537/biopython_component\n",
      "[2024-09-05 11:56:25,409 | root | INFO] Found 8 parquet files\n",
      "[2024-09-05 11:56:25,440 | root | INFO] Reading parquet partitions from: feature_extraction_pipeline/feature_extraction_pipeline-20240905115537/load_from_parquet\n",
      "[2024-09-05 11:56:25,441 | root | INFO] Found 0 parquet files\n",
      "[2024-09-05 11:56:25,441 | root | INFO] Reading parquet partitions from: feature_extraction_pipeline/feature_extraction_pipeline-20240905115537/ifeatureomega_component\n",
      "[2024-09-05 11:56:25,442 | root | INFO] Found 2 parquet files\n",
      "[2024-09-05 11:56:25,463 | root | INFO] Reading parquet partitions from: feature_extraction_pipeline/feature_extraction_pipeline-20240905115537/generate_protein_sequence_checksum_component\n",
      "[2024-09-05 11:56:25,464 | root | INFO] Found 8 parquet files\n",
      "[2024-09-05 11:56:25,493 | root | INFO] Reading parquet partitions from: feature_extraction_pipeline/feature_extraction_pipeline-20240905115537/store_pdb_component\n",
      "[2024-09-05 11:56:25,494 | root | INFO] Found 8 parquet files\n",
      "[2024-09-05 11:56:25,519 | root | INFO] Reading parquet partitions from: feature_extraction_pipeline/feature_extraction_pipeline-20240905115537/msa_component\n",
      "[2024-09-05 11:56:25,520 | root | INFO] Found 0 parquet files\n"
     ]
    }
   ],
   "source": [
    "dataframe_list = merge_parquet_folders(last_folder)\n",
    "\n",
    "\n",
    "df_final = pd.concat(dataframe_list, axis=1)\n",
    "df_final = df_final.loc[:,~df_final.columns.duplicated()]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['msa_sequence'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf_final\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msequence\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpdb_string\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmsa_sequence\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mto_json(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_json.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecords\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Software/deCYPher/protein-feature-extraction/.venv/lib/python3.10/site-packages/pandas/core/frame.py:4096\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4094\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4095\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4096\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4098\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/Software/deCYPher/protein-feature-extraction/.venv/lib/python3.10/site-packages/pandas/core/indexes/base.py:6199\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6196\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6197\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6199\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6201\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6203\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/Software/deCYPher/protein-feature-extraction/.venv/lib/python3.10/site-packages/pandas/core/indexes/base.py:6251\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6250\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6251\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['msa_sequence'] not in index\""
     ]
    }
   ],
   "source": [
    "df_final[[\"sequence\", \"pdb_string\", \"msa_sequence\"]].to_json(\"test_json.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering out columns that are not properly stored in a csv\n",
    "columns_to_remove = ['pdb_string']\n",
    "df_final = df_final.drop(columns=columns_to_remove)\n",
    "\n",
    "# write to file\n",
    "df_final.to_csv(f\"{last_folder}/final_output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "protein-feature-extraction-NoVdeDG9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
