{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protein feature extraction pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will contain the pipeline for extracting features from protein sequences. It will be used as a way to show the output without needing to run the `pipeline.py` file locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "# import pandas as pd\n",
    "from fondant.dataset import Dataset\n",
    "import os\n",
    "from config import MOCK_DATA_PATH_FONDANT\n",
    "\n",
    "# check if the manifest file is removed.\n",
    "REMOVED_MANIFEST = False\n",
    "\n",
    "# check if the output folder exists\n",
    "OUTPUT_FOLDER = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Mock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python utils/generate_mock_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MNQRGMPIQSLVTNVKINRLEENDCIHTRHRVRPGRTDGKNLHAMM...</td>\n",
       "      <td>Seq1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MAGLKPEVPLHDGINKFGKSDFAGQEGPKIVTTTDKALLVANGALK...</td>\n",
       "      <td>Seq2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MVDLKKELKNFVDSDFPGSPKQEAQGIDVRILLSFNNAAFREALII...</td>\n",
       "      <td>Seq3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MELILAKARLEFECDWGLLMLEPCVPPTKIFADRNYAVGVMFESDK...</td>\n",
       "      <td>Seq4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MRVLCDGSTGYACAKNTRIRFREKVASVLAKIQGYEQTFPHHMPNM...</td>\n",
       "      <td>Seq5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sequence  name\n",
       "0  MNQRGMPIQSLVTNVKINRLEENDCIHTRHRVRPGRTDGKNLHAMM...  Seq1\n",
       "1  MAGLKPEVPLHDGINKFGKSDFAGQEGPKIVTTTDKALLVANGALK...  Seq2\n",
       "2  MVDLKKELKNFVDSDFPGSPKQEAQGIDVRILLSFNNAAFREALII...  Seq3\n",
       "3  MELILAKARLEFECDWGLLMLEPCVPPTKIFADRNYAVGVMFESDK...  Seq4\n",
       "4  MRVLCDGSTGYACAKNTRIRFREKVASVLAKIQGYEQTFPHHMPNM...  Seq5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show content of the mock data\n",
    "import pandas as pd\n",
    "mock_df = pd.read_parquet(\".\" + MOCK_DATA_PATH_FONDANT)  # dot added to make it relative to the current directory\n",
    "mock_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a new pipeline\n",
    "\n",
    "BASE_PATH = \".fondant\"\n",
    "PIPELINE_NAME = \"feature_extraction_pipeline\"\n",
    "\n",
    "# dataset = Dataset(\n",
    "# \tname=PIPELINE_NAME,\n",
    "# \tbase_path=BASE_PATH,\n",
    "# \tdescription=\"A pipeline to extract features from protein sequences.\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-12 13:15:22,274 | fondant.dataset.dataset | INFO] The consumes section of the component spec is not defined. Can not infer consumes of the OperationSpec. Please define a consumes section in the dataset interface. \n"
     ]
    }
   ],
   "source": [
    "# Read the dataset\n",
    "\n",
    "raw_data = Dataset.create(\n",
    "\t\"load_from_parquet\",\n",
    "\targuments={\n",
    "\t\t\"dataset_uri\": MOCK_DATA_PATH_FONDANT,\n",
    "\t},\n",
    "\tproduces={\n",
    "\t\t\"sequence\": pa.string()\n",
    "\t}\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components\n",
    "\n",
    "---\n",
    "\n",
    "### generate_protein_sequence_checksum_component\n",
    "\n",
    "This component generates a checksum for the protein sequence.\n",
    "\n",
    "---\n",
    "\n",
    "### biopython_component\n",
    "\n",
    "Extracts features from the protein sequence using Biopython.\n",
    "\n",
    "---\n",
    "\n",
    "### iFeatureOmega_component\n",
    "\n",
    "Extracts features from the protein sequence using the [iFeatureOmega-CLI GitHub repo](https://github.com/Superzchen/iFeatureOmega-CLI). Arguments are used to specify the type of features to extract.\n",
    "\n",
    "---\n",
    "\n",
    "### filter_pdb_component\n",
    "\n",
    "Filters PDB files that are already predicted to avoid redundant predictions. Arguments need to be specified before running the pipeline:\n",
    "```json\n",
    "\"storage_type\": \"local\",\n",
    "\"pdb_path\": \"/data/<your-pdb-folder-path>\",\n",
    "\"bucket_name\": \"your-bucket-name\",\n",
    "\"project_id\": \"your-project-id\",\n",
    "\"google_cloud_credentials_path\": \"/data/<your-credentials>.json\"\n",
    "```\n",
    "\n",
    "If only using local, keep bucket_name, project_id, and google_cloud_credentials_path as empty strings. Using remote requires a Google Cloud Storage bucket with credentials and a project ID.\n",
    "\n",
    "---\n",
    "\n",
    "### predict_protein_3D_structure_component\n",
    "\n",
    "Predicts the 3D structure of the protein using ESMFold. This component requires a `.env` file with the following variables:\n",
    "```env\n",
    "HF_API_KEY=\"\"\n",
    "HF_ENDPOINT_URL=\"\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### store_pdb_component\n",
    "\n",
    "Stores the PDB files in the provided storage_type. Arguments need to be specified before running the pipeline:\n",
    "```json\n",
    "\"storage_type\": \"local\",\n",
    "\"pdb_path\": \"/data/<your-pdb-folder-path>\",\n",
    "\"bucket_name\": \"your-bucket-name\",\n",
    "\"project_id\": \"your-project-id\",\n",
    "\"google_cloud_credentials_path\": \"/data/<your-credentials>.json\"\n",
    "```\n",
    "\n",
    "If only using local, keep bucket_name, project_id, and google_cloud_credentials_path as empty strings. Using remote requires a Google Cloud Storage bucket with credentials and a project ID.\n",
    "\n",
    "---\n",
    "\n",
    "### msa_component\n",
    "\n",
    "Generates the multiple sequence alignment for the protein sequence using [Clustal Omega](http://www.clustal.org/omega/). It's recommended to use a smaller number of sequences or none at all due to potential time consumption.\n",
    "\n",
    "---\n",
    "\n",
    "### unikp_component\n",
    "\n",
    "Uses the UniKP endpoint on HuggingFace to predict the kinetic parameters of a protein sequence and substrate (SMILES) combination. See README for the description of the contents of this file.\n",
    "\n",
    "```yaml\n",
    "\"protein_smiles_path\": \"/data/<path_protein_smiles>\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### peptide_component\n",
    "\n",
    "Calculates the features from the protein sequence using the `peptides` package.\n",
    "\n",
    "---\n",
    "\n",
    "### deepTMpred_component\n",
    "\n",
    "Predicts the transmembrane regions of the protein sequence using the [DeepTMpred GitHub repository](https://github.com/ISYSLAB-HUST/DeepTMpred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-12 13:15:22,290 | fondant.dataset.dataset | WARNING] Component `Biopython component` has an image tag set to latest. Caching for the component will be disabled to prevent unpredictable behavior due to images updates\n"
     ]
    }
   ],
   "source": [
    "final_dataset = raw_data.apply(\n",
    "\t\"./components/biopython_component\"\n",
    "# ).apply(\n",
    "# \t\"./components/generate_protein_sequence_checksum_component\"\n",
    "# ).apply(\n",
    "# \t\"./components/iFeatureOmega_component\",\n",
    "# \t# currently forcing the number of rows to 5, but there needs to be a better way to do this, see readme for more info\n",
    "# \tinput_partition_rows=5,\n",
    "# \targuments={\n",
    "# \t\t\"descriptors\": [\"AAC\", \"CTDC\", \"CTDT\"]\n",
    "# \t}\n",
    "# ).apply(\n",
    "# \t\"./components/filter_pdb_component\",\n",
    "# \targuments={\n",
    "# \t\t\"method\": \"local\",\n",
    "# \t\t\"local_pdb_path\": \"/data/pdb_files\",\n",
    "# \t\t\"bucket_name\": \"\",\n",
    "# \t\t\"project_id\": \"\",\n",
    "# \t\t\"google_cloud_credentials_path\": \"\"\n",
    "# \t}\n",
    "# ).apply(\n",
    "# \t\"./components/predict_protein_3D_structure_component\",\n",
    "# ).apply(\n",
    "# \t\"./components/store_pdb_component\",\n",
    "# \targuments={\n",
    "# \t\t\"method\": \"local\",\n",
    "# \t\t\"local_pdb_path\": \"/data/pdb_files/\",\n",
    "# \t\t\"bucket_name\": \"\",\n",
    "# \t\t\"project_id\": \"\",\n",
    "# \t\t\"google_cloud_credentials_path\": \"\"\n",
    "# \t}\n",
    "# ).apply(\n",
    "# \t\"./components/msa_component\",\n",
    "# ).apply(\n",
    "# \t\"./components/pdb_features_component\"\n",
    "# ).apply(\n",
    "# \t\"./components/unikp_component\",\n",
    "# \targuments={\n",
    "# \t\t\"protein_smiles_path\": \"/data/protein_smiles.json\",\n",
    "# \t},\n",
    "# ).apply(\n",
    "# \t\"./components/peptide_features_component\"\n",
    "# ).apply(\n",
    "# \t\"./components/DeepTMpred_component\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pipeline.py` file needs to be run using the command line. The following command will run the pipeline:\n",
    "\n",
    "```bash\n",
    "fondant < full_path_to_pipeline.py >\\data:/data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-12 13:15:22,363 | root | INFO] Found reference to un-compiled workflow... compiling\n",
      "[2024-06-12 13:15:22,365 | fondant.dataset.compiler | INFO] Base path not found on local system, created base path and setting up /home/pietercoussement/Software/Sandbox/protein-feature-extraction/data/:/data as mount volume\n",
      "[2024-06-12 13:15:22,365 | fondant.dataset.dataset | INFO] Sorting workflow graph topologically.\n",
      "[2024-06-12 13:15:22,367 | fondant.dataset.dataset | INFO] All workflow component specifications match.\n",
      "[2024-06-12 13:15:22,368 | fondant.dataset.compiler | INFO] Compiling service for load_from_parquet\n",
      "[2024-06-12 13:15:22,368 | fondant.dataset.compiler | INFO] Compiling service for biopython_component\n",
      "[2024-06-12 13:15:22,369 | fondant.dataset.compiler | INFO] Found Dockerfile for biopython_component, adding build step.\n",
      "[2024-06-12 13:15:22,376 | fondant.dataset.compiler | INFO] Successfully compiled to .fondant/compose.yaml\n",
      "time=\"2024-06-12T13:15:22+02:00\" level=warning msg=\"/home/pietercoussement/Software/Sandbox/protein-feature-extraction/.fondant/compose.yaml: `version` is obsolete\"\n",
      " load_from_parquet Pulling \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docker version:\n",
      "(26, 1, 4)\n",
      "Starting workflow run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13808c22b207 Pulling fs layer \n",
      " 6c9a484475c1 Pulling fs layer \n",
      " fb408522af25 Pulling fs layer \n",
      " 54ac57f98245 Pulling fs layer \n",
      " de5947f22207 Pulling fs layer \n",
      " ce6504e44327 Pulling fs layer \n",
      " 967763eca59b Pulling fs layer \n",
      " c36f755488a2 Pulling fs layer \n",
      " 61055d3d12d4 Pulling fs layer \n",
      " 6b82aacabbe1 Pulling fs layer \n",
      " 8e6d0a0acda7 Pulling fs layer \n",
      " 967763eca59b Waiting \n",
      " c36f755488a2 Waiting \n",
      " 61055d3d12d4 Waiting \n",
      " 6b82aacabbe1 Waiting \n",
      " 54ac57f98245 Waiting \n",
      " de5947f22207 Waiting \n",
      " 8e6d0a0acda7 Waiting \n",
      " ce6504e44327 Waiting \n",
      " 6c9a484475c1 Downloading [>                                                  ]  36.13kB/3.508MB\n",
      " fb408522af25 Downloading [>                                                  ]  127.2kB/12.38MB\n",
      " 6c9a484475c1 Downloading [=================>                                 ]  1.198MB/3.508MB\n",
      " fb408522af25 Downloading [===>                                               ]  920.8kB/12.38MB\n",
      " 13808c22b207 Downloading [>                                                  ]  294.9kB/29.13MB\n",
      " 6c9a484475c1 Downloading [=============================>                     ]  2.075MB/3.508MB\n",
      " fb408522af25 Downloading [=======>                                           ]  1.841MB/12.38MB\n",
      " 13808c22b207 Downloading [=>                                                 ]  601.3kB/29.13MB\n",
      " 6c9a484475c1 Downloading [==================================================>]  3.508MB/3.508MB\n",
      " 6c9a484475c1 Verifying Checksum \n",
      " 6c9a484475c1 Download complete \n",
      " fb408522af25 Downloading [===========>                                       ]  2.898MB/12.38MB\n",
      " 13808c22b207 Downloading [===>                                               ]  2.091MB/29.13MB\n",
      " 13808c22b207 Downloading [=======>                                           ]  4.168MB/29.13MB\n",
      " 13808c22b207 Downloading [=========>                                         ]  5.352MB/29.13MB\n",
      " fb408522af25 Downloading [===============>                                   ]  3.824MB/12.38MB\n",
      " 13808c22b207 Downloading [===========>                                       ]  6.531MB/29.13MB\n",
      " fb408522af25 Downloading [========================>                          ]  6.069MB/12.38MB\n",
      " 13808c22b207 Downloading [============>                                      ]  7.416MB/29.13MB\n",
      " fb408522af25 Downloading [=============================>                     ]  7.252MB/12.38MB\n",
      " 13808c22b207 Downloading [==============>                                    ]  8.301MB/29.13MB\n",
      " fb408522af25 Downloading [==================================>                ]  8.567MB/12.38MB\n",
      " 13808c22b207 Downloading [================>                                  ]  9.796MB/29.13MB\n",
      " fb408522af25 Downloading [======================================>            ]  9.628MB/12.38MB\n",
      " 13808c22b207 Downloading [==================>                                ]  10.68MB/29.13MB\n",
      " fb408522af25 Downloading [===========================================>       ]  10.82MB/12.38MB\n",
      " 13808c22b207 Downloading [====================>                              ]  11.86MB/29.13MB\n",
      " 13808c22b207 Downloading [=====================>                             ]  12.76MB/29.13MB\n",
      " fb408522af25 Downloading [===============================================>   ]  11.87MB/12.38MB\n",
      " fb408522af25 Verifying Checksum \n",
      " fb408522af25 Download complete \n",
      " 13808c22b207 Downloading [========================>                          ]  14.25MB/29.13MB\n",
      " 13808c22b207 Downloading [===========================>                       ]  16.02MB/29.13MB\n",
      " 13808c22b207 Downloading [===============================>                   ]  18.11MB/29.13MB\n",
      " 13808c22b207 Downloading [==================================>                ]  20.19MB/29.13MB\n",
      " 13808c22b207 Downloading [=======================================>           ]  23.15MB/29.13MB\n",
      " 13808c22b207 Downloading [============================================>      ]  26.13MB/29.13MB\n",
      " de5947f22207 Downloading [>                                                  ]  34.76kB/3.364MB\n",
      " 13808c22b207 Downloading [===============================================>   ]  27.92MB/29.13MB\n",
      " de5947f22207 Downloading [===>                                               ]  261.4kB/3.364MB\n",
      " 13808c22b207 Verifying Checksum \n",
      " 13808c22b207 Download complete \n",
      " 13808c22b207 Extracting [>                                                  ]  294.9kB/29.13MB\n",
      " de5947f22207 Downloading [=========================>                         ]  1.727MB/3.364MB\n",
      " de5947f22207 Download complete \n",
      " 13808c22b207 Extracting [==============>                                    ]  8.258MB/29.13MB\n",
      " 13808c22b207 Extracting [=========================>                         ]  14.75MB/29.13MB\n",
      " 13808c22b207 Extracting [=========================================>         ]  24.18MB/29.13MB\n",
      " ce6504e44327 Downloading [>                                                  ]  491.5kB/47.9MB\n",
      " 13808c22b207 Extracting [==============================================>    ]  26.84MB/29.13MB\n",
      " ce6504e44327 Downloading [===>                                               ]  2.915MB/47.9MB\n",
      " 54ac57f98245 Downloading [==================================================>]     244B/244B\n",
      " 54ac57f98245 Verifying Checksum \n",
      " 54ac57f98245 Download complete \n",
      " 967763eca59b Downloading [==================================================>]     104B/104B\n",
      " 967763eca59b Verifying Checksum \n",
      " 967763eca59b Download complete \n",
      " ce6504e44327 Downloading [=====>                                             ]  5.348MB/47.9MB\n",
      " 13808c22b207 Extracting [================================================>  ]  28.31MB/29.13MB\n",
      " 13808c22b207 Extracting [==================================================>]  29.13MB/29.13MB\n",
      " ce6504e44327 Downloading [=======>                                           ]  7.297MB/47.9MB\n",
      " 13808c22b207 Pull complete \n",
      " 6c9a484475c1 Extracting [>                                                  ]  65.54kB/3.508MB\n",
      " 6c9a484475c1 Extracting [==================================================>]  3.508MB/3.508MB\n",
      " ce6504e44327 Downloading [==========>                                        ]  9.718MB/47.9MB\n",
      " 6c9a484475c1 Pull complete \n",
      " fb408522af25 Extracting [>                                                  ]  131.1kB/12.38MB\n",
      " ce6504e44327 Downloading [=============>                                     ]  12.61MB/47.9MB\n",
      " fb408522af25 Extracting [===================>                               ]   4.85MB/12.38MB\n",
      " c36f755488a2 Downloading [>                                                  ]  27.92kB/2.673MB\n",
      " fb408522af25 Extracting [================================================>  ]  11.93MB/12.38MB\n",
      " ce6504e44327 Downloading [===============>                                   ]  15.06MB/47.9MB\n",
      " fb408522af25 Extracting [==================================================>]  12.38MB/12.38MB\n",
      " fb408522af25 Pull complete \n",
      " 54ac57f98245 Extracting [==================================================>]     244B/244B\n",
      " 54ac57f98245 Extracting [==================================================>]     244B/244B\n",
      " c36f755488a2 Downloading [===>                                               ]  199.9kB/2.673MB\n",
      " 54ac57f98245 Pull complete \n",
      " de5947f22207 Extracting [>                                                  ]  65.54kB/3.364MB\n",
      " ce6504e44327 Downloading [=================>                                 ]     17MB/47.9MB\n",
      " c36f755488a2 Downloading [==================>                                ]  989.4kB/2.673MB\n",
      " de5947f22207 Extracting [=====================================>             ]  2.556MB/3.364MB\n",
      " de5947f22207 Extracting [==================================================>]  3.364MB/3.364MB\n",
      " de5947f22207 Pull complete \n",
      " ce6504e44327 Downloading [===================>                               ]  18.48MB/47.9MB\n",
      " c36f755488a2 Downloading [===========================>                       ]  1.481MB/2.673MB\n",
      " c36f755488a2 Downloading [======================================>            ]  2.071MB/2.673MB\n",
      " ce6504e44327 Downloading [======================>                            ]  21.39MB/47.9MB\n",
      " c36f755488a2 Verifying Checksum \n",
      " c36f755488a2 Download complete \n",
      " ce6504e44327 Downloading [=========================>                         ]  24.33MB/47.9MB\n",
      " ce6504e44327 Downloading [===========================>                       ]  26.27MB/47.9MB\n",
      " ce6504e44327 Downloading [=============================>                     ]  28.71MB/47.9MB\n",
      " 61055d3d12d4 Downloading [>                                                  ]  540.7kB/249.8MB\n",
      " ce6504e44327 Downloading [===============================>                   ]  30.65MB/47.9MB\n",
      " ce6504e44327 Downloading [===================================>               ]  33.56MB/47.9MB\n",
      " ce6504e44327 Downloading [======================================>            ]  36.98MB/47.9MB\n",
      " 6b82aacabbe1 Downloading [==================================================>]     118B/118B\n",
      " 6b82aacabbe1 Verifying Checksum \n",
      " 6b82aacabbe1 Download complete \n",
      " 61055d3d12d4 Downloading [>                                                  ]   1.62MB/249.8MB\n",
      " ce6504e44327 Downloading [========================================>          ]  38.93MB/47.9MB\n",
      " ce6504e44327 Downloading [==========================================>        ]   40.9MB/47.9MB\n",
      " 61055d3d12d4 Downloading [>                                                  ]  2.161MB/249.8MB\n",
      " ce6504e44327 Downloading [============================================>      ]  42.35MB/47.9MB\n",
      " 61055d3d12d4 Downloading [>                                                  ]   3.23MB/249.8MB\n",
      " ce6504e44327 Downloading [==============================================>    ]   44.8MB/47.9MB\n",
      " 61055d3d12d4 Downloading [>                                                  ]  3.762MB/249.8MB\n",
      " 61055d3d12d4 Downloading [>                                                  ]  4.303MB/249.8MB\n",
      " ce6504e44327 Downloading [=================================================> ]  47.24MB/47.9MB\n",
      " ce6504e44327 Verifying Checksum \n",
      " ce6504e44327 Download complete \n",
      " ce6504e44327 Extracting [>                                                  ]  491.5kB/47.9MB\n",
      " 61055d3d12d4 Downloading [=>                                                 ]  5.913MB/249.8MB\n",
      " ce6504e44327 Extracting [===========>                                       ]  10.81MB/47.9MB\n",
      " ce6504e44327 Extracting [======================>                            ]  21.14MB/47.9MB\n",
      " 61055d3d12d4 Downloading [=>                                                 ]  7.535MB/249.8MB\n",
      " ce6504e44327 Extracting [============================>                      ]  27.53MB/47.9MB\n",
      " 61055d3d12d4 Downloading [=>                                                 ]  9.153MB/249.8MB\n",
      " ce6504e44327 Extracting [===============================>                   ]  29.98MB/47.9MB\n",
      " 61055d3d12d4 Downloading [==>                                                ]  11.32MB/249.8MB\n",
      " 61055d3d12d4 Downloading [==>                                                ]  13.45MB/249.8MB\n",
      " ce6504e44327 Extracting [==================================>                ]  32.93MB/47.9MB\n",
      " 61055d3d12d4 Downloading [===>                                               ]  16.12MB/249.8MB\n",
      " ce6504e44327 Extracting [==============================================>    ]  44.73MB/47.9MB\n",
      " ce6504e44327 Extracting [==================================================>]   47.9MB/47.9MB\n",
      " 61055d3d12d4 Downloading [===>                                               ]  18.26MB/249.8MB\n",
      " ce6504e44327 Pull complete \n",
      " 967763eca59b Extracting [==================================================>]     104B/104B\n",
      " 967763eca59b Extracting [==================================================>]     104B/104B\n",
      " 967763eca59b Pull complete \n",
      " c36f755488a2 Extracting [>                                                  ]  32.77kB/2.673MB\n",
      " 61055d3d12d4 Downloading [====>                                              ]   20.4MB/249.8MB\n",
      " c36f755488a2 Extracting [==================================================>]  2.673MB/2.673MB\n",
      " c36f755488a2 Pull complete \n",
      " 61055d3d12d4 Downloading [====>                                              ]  22.55MB/249.8MB\n",
      " 61055d3d12d4 Downloading [=====>                                             ]  25.76MB/249.8MB\n",
      " 61055d3d12d4 Downloading [=====>                                             ]  27.88MB/249.8MB\n",
      " 8e6d0a0acda7 Downloading [=====================>                             ]     722B/1.669kB\n",
      " 8e6d0a0acda7 Downloading [==================================================>]  1.669kB/1.669kB\n",
      " 8e6d0a0acda7 Verifying Checksum \n",
      " 8e6d0a0acda7 Download complete \n",
      " 61055d3d12d4 Downloading [=====>                                             ]  28.96MB/249.8MB\n",
      " 61055d3d12d4 Downloading [======>                                            ]  32.17MB/249.8MB\n",
      " 61055d3d12d4 Downloading [======>                                            ]  34.86MB/249.8MB\n",
      " 61055d3d12d4 Downloading [=======>                                           ]  38.06MB/249.8MB\n",
      " 61055d3d12d4 Downloading [========>                                          ]  41.27MB/249.8MB\n",
      " 61055d3d12d4 Downloading [========>                                          ]   43.4MB/249.8MB\n",
      " 61055d3d12d4 Downloading [=========>                                         ]  46.07MB/249.8MB\n",
      " 61055d3d12d4 Downloading [=========>                                         ]  48.75MB/249.8MB\n",
      " 61055d3d12d4 Downloading [==========>                                        ]  50.89MB/249.8MB\n",
      " 61055d3d12d4 Downloading [==========>                                        ]     53MB/249.8MB\n",
      " 61055d3d12d4 Downloading [===========>                                       ]  55.68MB/249.8MB\n",
      " 61055d3d12d4 Downloading [===========>                                       ]  58.36MB/249.8MB\n",
      " 61055d3d12d4 Downloading [============>                                      ]  60.49MB/249.8MB\n",
      " 61055d3d12d4 Downloading [============>                                      ]  63.17MB/249.8MB\n",
      " 61055d3d12d4 Downloading [=============>                                     ]  65.87MB/249.8MB\n",
      " 61055d3d12d4 Downloading [=============>                                     ]  68.54MB/249.8MB\n",
      " 61055d3d12d4 Downloading [==============>                                    ]  70.16MB/249.8MB\n",
      " 61055d3d12d4 Downloading [==============>                                    ]  72.85MB/249.8MB\n",
      " 61055d3d12d4 Downloading [==============>                                    ]  73.92MB/249.8MB\n",
      " 61055d3d12d4 Downloading [===============>                                   ]  77.14MB/249.8MB\n",
      " 61055d3d12d4 Downloading [===============>                                   ]  79.29MB/249.8MB\n",
      " 61055d3d12d4 Downloading [================>                                  ]  81.97MB/249.8MB\n",
      " 61055d3d12d4 Downloading [=================>                                 ]  85.19MB/249.8MB\n",
      " 61055d3d12d4 Downloading [=================>                                 ]  87.87MB/249.8MB\n",
      " 61055d3d12d4 Downloading [==================>                                ]  90.02MB/249.8MB\n",
      " 61055d3d12d4 Downloading [==================>                                ]   92.7MB/249.8MB\n",
      " 61055d3d12d4 Downloading [===================>                               ]  95.38MB/249.8MB\n",
      " 61055d3d12d4 Downloading [===================>                               ]   98.6MB/249.8MB\n",
      " 61055d3d12d4 Downloading [====================>                              ]  100.8MB/249.8MB\n",
      " 61055d3d12d4 Downloading [====================>                              ]  103.4MB/249.8MB\n",
      " 61055d3d12d4 Downloading [=====================>                             ]  106.1MB/249.8MB\n",
      " 61055d3d12d4 Downloading [=====================>                             ]  108.3MB/249.8MB\n",
      " 61055d3d12d4 Downloading [======================>                            ]    112MB/249.8MB\n",
      " 61055d3d12d4 Downloading [=======================>                           ]  115.7MB/249.8MB\n",
      " 61055d3d12d4 Downloading [=======================>                           ]  118.9MB/249.8MB\n",
      " 61055d3d12d4 Downloading [========================>                          ]  121.1MB/249.8MB\n",
      " 61055d3d12d4 Downloading [========================>                          ]  124.3MB/249.8MB\n",
      " 61055d3d12d4 Downloading [=========================>                         ]  126.4MB/249.8MB\n",
      " 61055d3d12d4 Downloading [=========================>                         ]  128.6MB/249.8MB\n",
      " 61055d3d12d4 Downloading [==========================>                        ]  130.7MB/249.8MB\n",
      " 61055d3d12d4 Downloading [==========================>                        ]  133.4MB/249.8MB\n",
      " 61055d3d12d4 Downloading [===========================>                       ]    136MB/249.8MB\n",
      " 61055d3d12d4 Downloading [===========================>                       ]  138.7MB/249.8MB\n",
      " 61055d3d12d4 Downloading [============================>                      ]  140.9MB/249.8MB\n",
      " 61055d3d12d4 Downloading [============================>                      ]  143.6MB/249.8MB\n",
      " 61055d3d12d4 Downloading [=============================>                     ]  146.2MB/249.8MB\n",
      " 61055d3d12d4 Downloading [=============================>                     ]  148.9MB/249.8MB\n",
      " 61055d3d12d4 Downloading [==============================>                    ]  151.6MB/249.8MB\n",
      " 61055d3d12d4 Downloading [==============================>                    ]  154.8MB/249.8MB\n",
      " 61055d3d12d4 Downloading [===============================>                   ]  156.9MB/249.8MB\n",
      " 61055d3d12d4 Downloading [===============================>                   ]  159.6MB/249.8MB\n",
      " 61055d3d12d4 Downloading [================================>                  ]  162.3MB/249.8MB\n",
      " 61055d3d12d4 Downloading [================================>                  ]  164.4MB/249.8MB\n",
      " 61055d3d12d4 Downloading [=================================>                 ]  167.1MB/249.8MB\n",
      " 61055d3d12d4 Downloading [=================================>                 ]  169.7MB/249.8MB\n",
      " 61055d3d12d4 Downloading [==================================>                ]  172.4MB/249.8MB\n",
      " 61055d3d12d4 Downloading [==================================>                ]  174.6MB/249.8MB\n",
      " 61055d3d12d4 Downloading [===================================>               ]  175.6MB/249.8MB\n",
      " 61055d3d12d4 Downloading [===================================>               ]  178.3MB/249.8MB\n",
      " 61055d3d12d4 Downloading [====================================>              ]    181MB/249.8MB\n",
      " 61055d3d12d4 Downloading [====================================>              ]  183.7MB/249.8MB\n",
      " 61055d3d12d4 Downloading [=====================================>             ]  186.9MB/249.8MB\n",
      " 61055d3d12d4 Downloading [=====================================>             ]  189.1MB/249.8MB\n",
      " 61055d3d12d4 Downloading [======================================>            ]  191.7MB/249.8MB\n",
      " 61055d3d12d4 Downloading [======================================>            ]  194.4MB/249.8MB\n",
      " 61055d3d12d4 Downloading [=======================================>           ]  197.1MB/249.8MB\n",
      " 61055d3d12d4 Downloading [========================================>          ]  200.3MB/249.8MB\n",
      " 61055d3d12d4 Downloading [========================================>          ]  202.5MB/249.8MB\n",
      " 61055d3d12d4 Downloading [========================================>          ]  204.6MB/249.8MB\n",
      " 61055d3d12d4 Downloading [=========================================>         ]  206.8MB/249.8MB\n",
      " 61055d3d12d4 Downloading [=========================================>         ]  209.4MB/249.8MB\n",
      " 61055d3d12d4 Downloading [==========================================>        ]  210.5MB/249.8MB\n",
      " 61055d3d12d4 Downloading [==========================================>        ]  213.2MB/249.8MB\n",
      " 61055d3d12d4 Downloading [===========================================>       ]  216.4MB/249.8MB\n",
      " 61055d3d12d4 Downloading [===========================================>       ]  218.5MB/249.8MB\n",
      " 61055d3d12d4 Downloading [============================================>      ]  221.8MB/249.8MB\n",
      " 61055d3d12d4 Downloading [============================================>      ]  224.5MB/249.8MB\n",
      " 61055d3d12d4 Downloading [=============================================>     ]  226.6MB/249.8MB\n",
      " 61055d3d12d4 Downloading [=============================================>     ]  229.3MB/249.8MB\n",
      " 61055d3d12d4 Downloading [==============================================>    ]    232MB/249.8MB\n",
      " 61055d3d12d4 Downloading [==============================================>    ]  234.6MB/249.8MB\n",
      " 61055d3d12d4 Downloading [===============================================>   ]  237.9MB/249.8MB\n",
      " 61055d3d12d4 Downloading [================================================>  ]  240.5MB/249.8MB\n",
      " 61055d3d12d4 Downloading [================================================>  ]  243.2MB/249.8MB\n",
      " 61055d3d12d4 Downloading [=================================================> ]  246.4MB/249.8MB\n",
      " 61055d3d12d4 Downloading [=================================================> ]  248.6MB/249.8MB\n",
      " 61055d3d12d4 Verifying Checksum \n",
      " 61055d3d12d4 Download complete \n",
      " 61055d3d12d4 Extracting [>                                                  ]  557.1kB/249.8MB\n",
      " 61055d3d12d4 Extracting [=====>                                             ]  25.62MB/249.8MB\n",
      " 61055d3d12d4 Extracting [===========>                                       ]  56.82MB/249.8MB\n",
      " 61055d3d12d4 Extracting [=================>                                 ]  89.69MB/249.8MB\n",
      " 61055d3d12d4 Extracting [=======================>                           ]  118.7MB/249.8MB\n",
      " 61055d3d12d4 Extracting [========================>                          ]  122.6MB/249.8MB\n",
      " 61055d3d12d4 Extracting [========================>                          ]  124.8MB/249.8MB\n",
      " 61055d3d12d4 Extracting [==========================>                        ]  130.4MB/249.8MB\n",
      " 61055d3d12d4 Extracting [==========================>                        ]  134.3MB/249.8MB\n",
      " 61055d3d12d4 Extracting [===========================>                       ]  139.8MB/249.8MB\n",
      " 61055d3d12d4 Extracting [============================>                      ]  144.3MB/249.8MB\n",
      " 61055d3d12d4 Extracting [==============================>                    ]    151MB/249.8MB\n",
      " 61055d3d12d4 Extracting [==============================>                    ]  154.3MB/249.8MB\n",
      " 61055d3d12d4 Extracting [===============================>                   ]  157.1MB/249.8MB\n",
      " 61055d3d12d4 Extracting [===============================>                   ]  159.9MB/249.8MB\n",
      " 61055d3d12d4 Extracting [=================================>                 ]  164.9MB/249.8MB\n",
      " 61055d3d12d4 Extracting [=================================>                 ]  168.8MB/249.8MB\n",
      " 61055d3d12d4 Extracting [==================================>                ]  173.8MB/249.8MB\n",
      " 61055d3d12d4 Extracting [====================================>              ]  180.5MB/249.8MB\n",
      " 61055d3d12d4 Extracting [=====================================>             ]  187.7MB/249.8MB\n",
      " 61055d3d12d4 Extracting [======================================>            ]  191.6MB/249.8MB\n",
      " 61055d3d12d4 Extracting [======================================>            ]  193.9MB/249.8MB\n",
      " 61055d3d12d4 Extracting [=======================================>           ]  195.5MB/249.8MB\n",
      " 61055d3d12d4 Extracting [=======================================>           ]  197.8MB/249.8MB\n",
      " 61055d3d12d4 Extracting [========================================>          ]    200MB/249.8MB\n",
      " 61055d3d12d4 Extracting [=========================================>         ]    205MB/249.8MB\n",
      " 61055d3d12d4 Extracting [=========================================>         ]  206.7MB/249.8MB\n",
      " 61055d3d12d4 Extracting [==========================================>        ]  212.8MB/249.8MB\n",
      " 61055d3d12d4 Extracting [============================================>      ]  220.6MB/249.8MB\n",
      " 61055d3d12d4 Extracting [=============================================>     ]  226.7MB/249.8MB\n",
      " 61055d3d12d4 Extracting [==============================================>    ]  230.1MB/249.8MB\n",
      " 61055d3d12d4 Extracting [===============================================>   ]  237.3MB/249.8MB\n",
      " 61055d3d12d4 Extracting [================================================>  ]  243.4MB/249.8MB\n",
      " 61055d3d12d4 Extracting [=================================================> ]  245.1MB/249.8MB\n",
      " 61055d3d12d4 Extracting [=================================================> ]  246.2MB/249.8MB\n",
      " 61055d3d12d4 Extracting [=================================================> ]  248.4MB/249.8MB\n",
      " 61055d3d12d4 Extracting [==================================================>]  249.8MB/249.8MB\n",
      " 61055d3d12d4 Pull complete \n",
      " 6b82aacabbe1 Extracting [==================================================>]     118B/118B\n",
      " 6b82aacabbe1 Extracting [==================================================>]     118B/118B\n",
      " 6b82aacabbe1 Pull complete \n",
      " 8e6d0a0acda7 Extracting [==================================================>]  1.669kB/1.669kB\n",
      " 8e6d0a0acda7 Extracting [==================================================>]  1.669kB/1.669kB\n",
      " 8e6d0a0acda7 Pull complete \n",
      " load_from_parquet Pulled \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0 building with \"desktop-linux\" instance using docker driver\n",
      "\n",
      "#1 [biopython_component internal] load build definition from Dockerfile\n",
      "#1 transferring dockerfile: 476B done\n",
      "#1 DONE 0.1s\n",
      "\n",
      "#2 [biopython_component internal] load metadata for docker.io/fndnt/fondant:0.12.1-py3.9\n",
      "#2 DONE 1.5s\n",
      "\n",
      "#3 [biopython_component internal] load .dockerignore\n",
      "#3 transferring context: 2B done\n",
      "#3 DONE 0.2s\n",
      "\n",
      "#4 [biopython_component internal] load build context\n",
      "#4 transferring context: 123B done\n",
      "#4 DONE 0.0s\n",
      "\n",
      "#5 [biopython_component 1/6] FROM docker.io/fndnt/fondant:0.12.1-py3.9@sha256:138b4b8ddef694c5256ec9c0e67f37460b1258a7aed66f1225c0e6c8b4764950\n",
      "#5 resolve docker.io/fndnt/fondant:0.12.1-py3.9@sha256:138b4b8ddef694c5256ec9c0e67f37460b1258a7aed66f1225c0e6c8b4764950 0.0s done\n",
      "#5 sha256:e8690706172b28215b62640a817fd89cc50c6a011c0877e20be0956b6312bc70 0B / 244B 0.1s\n",
      "#5 sha256:4a81626d2c6be5f7d7ee1da92369e83a013dd773f83958f463314daa30b7e2dd 0B / 11.89MB 0.1s\n",
      "#5 sha256:8c60f620628dd111479032e759bd057e2c05944ad339ae62dc218bbff01f9bed 0B / 3.13MB 0.1s\n",
      "#5 sha256:138b4b8ddef694c5256ec9c0e67f37460b1258a7aed66f1225c0e6c8b4764950 856B / 856B done\n",
      "#5 sha256:738dd22d574a6eaaccd7a0cad87cd777123b8dc74026f3377db85d4f8c23c0a4 1.63kB / 1.63kB done\n",
      "#5 sha256:8bcc1f6e38e492e77f6c7da9c8642082afa51ad4bb110bd6b73764cd681578d1 7.65kB / 7.65kB done\n",
      "#5 sha256:4a81626d2c6be5f7d7ee1da92369e83a013dd773f83958f463314daa30b7e2dd 4.19MB / 11.89MB 0.4s\n",
      "#5 sha256:e8690706172b28215b62640a817fd89cc50c6a011c0877e20be0956b6312bc70 244B / 244B 0.5s done\n",
      "#5 sha256:4a81626d2c6be5f7d7ee1da92369e83a013dd773f83958f463314daa30b7e2dd 6.29MB / 11.89MB 0.5s\n",
      "#5 sha256:4a81626d2c6be5f7d7ee1da92369e83a013dd773f83958f463314daa30b7e2dd 9.44MB / 11.89MB 0.6s\n",
      "#5 sha256:e5aa2f0cb56dbe8636128e30698153d778e7c8c331a117c8c88e411ccbc44e7b 0B / 47.90MB 0.7s\n",
      "#5 sha256:4a81626d2c6be5f7d7ee1da92369e83a013dd773f83958f463314daa30b7e2dd 11.89MB / 11.89MB 0.7s done\n",
      "#5 extracting sha256:4a81626d2c6be5f7d7ee1da92369e83a013dd773f83958f463314daa30b7e2dd 0.1s\n",
      "#5 sha256:d0c00b7a3a2bb461ca332bcfaca79313280eacaec850b01ec5bbb6112265265b 0B / 252.40MB 0.8s\n",
      "#5 sha256:8c60f620628dd111479032e759bd057e2c05944ad339ae62dc218bbff01f9bed 1.05MB / 3.13MB 1.0s\n",
      "#5 sha256:8c60f620628dd111479032e759bd057e2c05944ad339ae62dc218bbff01f9bed 2.10MB / 3.13MB 1.1s\n",
      "#5 sha256:8c60f620628dd111479032e759bd057e2c05944ad339ae62dc218bbff01f9bed 3.13MB / 3.13MB 1.2s\n",
      "#5 sha256:8c60f620628dd111479032e759bd057e2c05944ad339ae62dc218bbff01f9bed 3.13MB / 3.13MB 1.2s done\n",
      "#5 sha256:e5aa2f0cb56dbe8636128e30698153d778e7c8c331a117c8c88e411ccbc44e7b 4.19MB / 47.90MB 1.3s\n",
      "#5 extracting sha256:4a81626d2c6be5f7d7ee1da92369e83a013dd773f83958f463314daa30b7e2dd 0.5s done\n",
      "#5 extracting sha256:e8690706172b28215b62640a817fd89cc50c6a011c0877e20be0956b6312bc70 done\n",
      "#5 extracting sha256:8c60f620628dd111479032e759bd057e2c05944ad339ae62dc218bbff01f9bed\n",
      "#5 sha256:e5aa2f0cb56dbe8636128e30698153d778e7c8c331a117c8c88e411ccbc44e7b 9.44MB / 47.90MB 1.5s\n",
      "#5 extracting sha256:8c60f620628dd111479032e759bd057e2c05944ad339ae62dc218bbff01f9bed 0.2s done\n",
      "#5 sha256:e5aa2f0cb56dbe8636128e30698153d778e7c8c331a117c8c88e411ccbc44e7b 14.68MB / 47.90MB 1.8s\n",
      "#5 sha256:e5aa2f0cb56dbe8636128e30698153d778e7c8c331a117c8c88e411ccbc44e7b 18.87MB / 47.90MB 2.0s\n",
      "#5 sha256:e5aa2f0cb56dbe8636128e30698153d778e7c8c331a117c8c88e411ccbc44e7b 23.07MB / 47.90MB 2.2s\n",
      "#5 sha256:e5aa2f0cb56dbe8636128e30698153d778e7c8c331a117c8c88e411ccbc44e7b 30.41MB / 47.90MB 2.7s\n",
      "#5 sha256:e5aa2f0cb56dbe8636128e30698153d778e7c8c331a117c8c88e411ccbc44e7b 33.55MB / 47.90MB 2.9s\n",
      "#5 sha256:e5aa2f0cb56dbe8636128e30698153d778e7c8c331a117c8c88e411ccbc44e7b 36.70MB / 47.90MB 3.1s\n",
      "#5 sha256:e5aa2f0cb56dbe8636128e30698153d778e7c8c331a117c8c88e411ccbc44e7b 39.85MB / 47.90MB 3.4s\n",
      "#5 sha256:e5aa2f0cb56dbe8636128e30698153d778e7c8c331a117c8c88e411ccbc44e7b 42.99MB / 47.90MB 3.7s\n",
      "#5 sha256:e5aa2f0cb56dbe8636128e30698153d778e7c8c331a117c8c88e411ccbc44e7b 47.19MB / 47.90MB 4.0s\n",
      "#5 extracting sha256:e5aa2f0cb56dbe8636128e30698153d778e7c8c331a117c8c88e411ccbc44e7b\n",
      "#5 sha256:e5aa2f0cb56dbe8636128e30698153d778e7c8c331a117c8c88e411ccbc44e7b 47.90MB / 47.90MB 4.1s done\n",
      "#5 sha256:d0c00b7a3a2bb461ca332bcfaca79313280eacaec850b01ec5bbb6112265265b 13.63MB / 252.40MB 4.4s\n",
      "#5 sha256:d0c00b7a3a2bb461ca332bcfaca79313280eacaec850b01ec5bbb6112265265b 27.26MB / 252.40MB 5.0s\n",
      "#5 extracting sha256:e5aa2f0cb56dbe8636128e30698153d778e7c8c331a117c8c88e411ccbc44e7b 1.4s done\n",
      "#5 sha256:d0c00b7a3a2bb461ca332bcfaca79313280eacaec850b01ec5bbb6112265265b 44.04MB / 252.40MB 5.8s\n",
      "#5 sha256:d0c00b7a3a2bb461ca332bcfaca79313280eacaec850b01ec5bbb6112265265b 58.72MB / 252.40MB 6.5s\n",
      "#5 sha256:d0c00b7a3a2bb461ca332bcfaca79313280eacaec850b01ec5bbb6112265265b 73.40MB / 252.40MB 7.2s\n",
      "#5 sha256:d0c00b7a3a2bb461ca332bcfaca79313280eacaec850b01ec5bbb6112265265b 90.18MB / 252.40MB 8.0s\n",
      "#5 sha256:d0c00b7a3a2bb461ca332bcfaca79313280eacaec850b01ec5bbb6112265265b 103.81MB / 252.40MB 8.7s\n",
      "#5 sha256:d0c00b7a3a2bb461ca332bcfaca79313280eacaec850b01ec5bbb6112265265b 117.44MB / 252.40MB 9.3s\n",
      "#5 sha256:d0c00b7a3a2bb461ca332bcfaca79313280eacaec850b01ec5bbb6112265265b 131.07MB / 252.40MB 9.9s\n",
      "#5 sha256:d0c00b7a3a2bb461ca332bcfaca79313280eacaec850b01ec5bbb6112265265b 146.80MB / 252.40MB 10.6s\n",
      "#5 sha256:d0c00b7a3a2bb461ca332bcfaca79313280eacaec850b01ec5bbb6112265265b 160.43MB / 252.40MB 11.2s\n",
      "#5 sha256:d0c00b7a3a2bb461ca332bcfaca79313280eacaec850b01ec5bbb6112265265b 175.11MB / 252.40MB 11.9s\n",
      "#5 sha256:d0c00b7a3a2bb461ca332bcfaca79313280eacaec850b01ec5bbb6112265265b 188.74MB / 252.40MB 12.6s\n",
      "#5 sha256:d0c00b7a3a2bb461ca332bcfaca79313280eacaec850b01ec5bbb6112265265b 203.42MB / 252.40MB 13.3s\n",
      "#5 sha256:d0c00b7a3a2bb461ca332bcfaca79313280eacaec850b01ec5bbb6112265265b 217.06MB / 252.40MB 14.0s\n",
      "#5 sha256:d0c00b7a3a2bb461ca332bcfaca79313280eacaec850b01ec5bbb6112265265b 232.78MB / 252.40MB 14.9s\n",
      "#5 sha256:d0c00b7a3a2bb461ca332bcfaca79313280eacaec850b01ec5bbb6112265265b 247.46MB / 252.40MB 15.6s\n",
      "#5 sha256:d0c00b7a3a2bb461ca332bcfaca79313280eacaec850b01ec5bbb6112265265b 252.40MB / 252.40MB 15.9s done\n",
      "#5 extracting sha256:d0c00b7a3a2bb461ca332bcfaca79313280eacaec850b01ec5bbb6112265265b 0.1s\n",
      "#5 extracting sha256:d0c00b7a3a2bb461ca332bcfaca79313280eacaec850b01ec5bbb6112265265b 4.8s done\n",
      "#5 DONE 21.2s\n",
      "\n",
      "#6 [biopython_component 2/6] RUN apt-get update &&     apt-get upgrade -y &&     apt-get install git -y\n",
      "#6 0.426 Hit:1 http://deb.debian.org/debian bookworm InRelease\n",
      "#6 0.430 Get:2 http://deb.debian.org/debian bookworm-updates InRelease [55.4 kB]\n",
      "#6 0.455 Get:3 http://deb.debian.org/debian-security bookworm-security InRelease [47.9 kB]\n",
      "#6 0.525 Get:4 http://deb.debian.org/debian bookworm-updates/main amd64 Packages.diff/Index [10.6 kB]\n",
      "#6 0.539 Get:5 http://deb.debian.org/debian bookworm-updates/main amd64 Packages T-2024-04-23-2036.10-F-2024-04-23-2036.10.pdiff [1595 B]\n",
      "#6 0.546 Get:5 http://deb.debian.org/debian bookworm-updates/main amd64 Packages T-2024-04-23-2036.10-F-2024-04-23-2036.10.pdiff [1595 B]\n",
      "#6 0.563 Get:6 http://deb.debian.org/debian-security bookworm-security/main amd64 Packages [160 kB]\n",
      "#6 0.600 Fetched 275 kB in 0s (1075 kB/s)\n",
      "#6 0.600 Reading package lists...\n",
      "#6 1.055 Reading package lists...\n",
      "#6 1.508 Building dependency tree...\n",
      "#6 1.622 Reading state information...\n",
      "#6 1.650 Calculating upgrade...\n",
      "#6 1.817 The following packages will be upgraded:\n",
      "#6 1.817   less libc-bin libc6\n",
      "#6 1.874 3 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\n",
      "#6 1.874 Need to get 3497 kB of archives.\n",
      "#6 1.874 After this operation, 4096 B of additional disk space will be used.\n",
      "#6 1.874 Get:1 http://deb.debian.org/debian-security bookworm-security/main amd64 libc6 amd64 2.36-9+deb12u7 [2758 kB]\n",
      "#6 2.512 Get:2 http://deb.debian.org/debian-security bookworm-security/main amd64 libc-bin amd64 2.36-9+deb12u7 [607 kB]\n",
      "#6 2.610 Get:3 http://deb.debian.org/debian-security bookworm-security/main amd64 less amd64 590-2.1~deb12u2 [132 kB]\n",
      "#6 2.773 debconf: delaying package configuration, since apt-utils is not installed\n",
      "#6 2.804 Fetched 3497 kB in 1s (4323 kB/s)\n",
      "(Reading database ... 10323 files and directories currently installed.)\n",
      "#6 2.882 Preparing to unpack .../libc6_2.36-9+deb12u7_amd64.deb ...\n",
      "#6 2.980 debconf: unable to initialize frontend: Dialog\n",
      "#6 2.980 debconf: (TERM is not set, so the dialog frontend is not usable.)\n",
      "#6 2.980 debconf: falling back to frontend: Readline\n",
      "#6 2.988 debconf: unable to initialize frontend: Readline\n",
      "#6 2.988 debconf: (This frontend requires a controlling tty.)\n",
      "#6 2.988 debconf: falling back to frontend: Teletype\n",
      "#6 3.041 Unpacking libc6:amd64 (2.36-9+deb12u7) over (2.36-9+deb12u4) ...\n",
      "#6 4.205 Setting up libc6:amd64 (2.36-9+deb12u7) ...\n",
      "#6 4.285 debconf: unable to initialize frontend: Dialog\n",
      "#6 4.285 debconf: (TERM is not set, so the dialog frontend is not usable.)\n",
      "#6 4.285 debconf: falling back to frontend: Readline\n",
      "#6 4.292 debconf: unable to initialize frontend: Readline\n",
      "#6 4.292 debconf: (This frontend requires a controlling tty.)\n",
      "#6 4.292 debconf: falling back to frontend: Teletype\n",
      "(Reading database ... 10323 files and directories currently installed.)\n",
      "#6 5.373 Preparing to unpack .../libc-bin_2.36-9+deb12u7_amd64.deb ...\n",
      "#6 5.392 Unpacking libc-bin (2.36-9+deb12u7) over (2.36-9+deb12u4) ...\n",
      "#6 5.570 Setting up libc-bin (2.36-9+deb12u7) ...\n",
      "(Reading database ... 10323 files and directories currently installed.)\n",
      "#6 5.682 Preparing to unpack .../less_590-2.1~deb12u2_amd64.deb ...\n",
      "#6 5.705 Unpacking less (590-2.1~deb12u2) over (590-2) ...\n",
      "#6 5.779 Setting up less (590-2.1~deb12u2) ...\n",
      "#6 5.829 Reading package lists...\n",
      "#6 6.222 Building dependency tree...\n",
      "#6 6.339 Reading state information...\n",
      "#6 6.467 git is already the newest version (1:2.39.2-1.1).\n",
      "#6 6.467 0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\n",
      "#6 DONE 6.7s\n",
      "\n",
      "#7 [biopython_component 3/6] COPY requirements.txt ./\n",
      "#7 DONE 0.1s\n",
      "\n",
      "#8 [biopython_component 4/6] RUN pip3 install --no-cache-dir -r requirements.txt\n",
      "#8 0.491 Requirement already satisfied: fondant[component] in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (0.1.dev0)\n",
      "#8 0.702 Collecting biopython==1.83\n",
      "#8 0.844   Downloading biopython-1.83-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "#8 1.128      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/3.1 MB 11.0 MB/s eta 0:00:00\n",
      "#8 1.270 Collecting pyarrow==15.0.0\n",
      "#8 1.288   Downloading pyarrow-15.0.0-cp39-cp39-manylinux_2_28_x86_64.whl (38.3 MB)\n",
      "#8 6.482      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 38.3/38.3 MB 6.8 MB/s eta 0:00:00\n",
      "#8 6.520 Requirement already satisfied: numpy in /usr/local/lib/python3.9/site-packages (from biopython==1.83->-r requirements.txt (line 2)) (1.26.4)\n",
      "#8 6.533 WARNING: fondant 0.1.dev0 does not provide the extra 'component'\n",
      "#8 6.536 Requirement already satisfied: docker>=6.1.3 in /usr/local/lib/python3.9/site-packages (from fondant[component]->-r requirements.txt (line 1)) (7.0.0)\n",
      "#8 6.537 Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.9/site-packages (from fondant[component]->-r requirements.txt (line 1)) (6.0.1)\n",
      "#8 6.538 Requirement already satisfied: fsspec>=2023.4.0 in /usr/local/lib/python3.9/site-packages (from fondant[component]->-r requirements.txt (line 1)) (2024.3.1)\n",
      "#8 6.539 Requirement already satisfied: dask[dataframe,diagnostics,distributed]<2024.3.0,>=2023.4.1 in /usr/local/lib/python3.9/site-packages (from fondant[component]->-r requirements.txt (line 1)) (2024.2.1)\n",
      "#8 6.543 Requirement already satisfied: jsonschema>=4.18 in /usr/local/lib/python3.9/site-packages (from fondant[component]->-r requirements.txt (line 1)) (4.21.1)\n",
      "#8 6.561 Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.9/site-packages (from dask[dataframe,diagnostics,distributed]<2024.3.0,>=2023.4.1->fondant[component]->-r requirements.txt (line 1)) (7.1.0)\n",
      "#8 6.562 Requirement already satisfied: partd>=1.2.0 in /usr/local/lib/python3.9/site-packages (from dask[dataframe,diagnostics,distributed]<2024.3.0,>=2023.4.1->fondant[component]->-r requirements.txt (line 1)) (1.4.1)\n",
      "#8 6.563 Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/site-packages (from dask[dataframe,diagnostics,distributed]<2024.3.0,>=2023.4.1->fondant[component]->-r requirements.txt (line 1)) (24.0)\n",
      "#8 6.564 Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.9/site-packages (from dask[dataframe,diagnostics,distributed]<2024.3.0,>=2023.4.1->fondant[component]->-r requirements.txt (line 1)) (0.12.1)\n",
      "#8 6.564 Requirement already satisfied: cloudpickle>=1.5.0 in /usr/local/lib/python3.9/site-packages (from dask[dataframe,diagnostics,distributed]<2024.3.0,>=2023.4.1->fondant[component]->-r requirements.txt (line 1)) (3.0.0)\n",
      "#8 6.565 Requirement already satisfied: click>=8.1 in /usr/local/lib/python3.9/site-packages (from dask[dataframe,diagnostics,distributed]<2024.3.0,>=2023.4.1->fondant[component]->-r requirements.txt (line 1)) (8.1.7)\n",
      "#8 6.566 Requirement already satisfied: distributed==2024.2.1 in /usr/local/lib/python3.9/site-packages (from dask[dataframe,diagnostics,distributed]<2024.3.0,>=2023.4.1->fondant[component]->-r requirements.txt (line 1)) (2024.2.1)\n",
      "#8 6.567 Requirement already satisfied: jinja2>=2.10.3 in /usr/local/lib/python3.9/site-packages (from dask[dataframe,diagnostics,distributed]<2024.3.0,>=2023.4.1->fondant[component]->-r requirements.txt (line 1)) (3.1.3)\n",
      "#8 6.568 Requirement already satisfied: bokeh>=2.4.2 in /usr/local/lib/python3.9/site-packages (from dask[dataframe,diagnostics,distributed]<2024.3.0,>=2023.4.1->fondant[component]->-r requirements.txt (line 1)) (3.4.1)\n",
      "#8 6.570 Requirement already satisfied: pandas>=1.3 in /usr/local/lib/python3.9/site-packages (from dask[dataframe,diagnostics,distributed]<2024.3.0,>=2023.4.1->fondant[component]->-r requirements.txt (line 1)) (2.2.2)\n",
      "#8 6.578 Requirement already satisfied: tornado>=6.0.4 in /usr/local/lib/python3.9/site-packages (from distributed==2024.2.1->dask[dataframe,diagnostics,distributed]<2024.3.0,>=2023.4.1->fondant[component]->-r requirements.txt (line 1)) (6.4)\n",
      "#8 6.578 Requirement already satisfied: zict>=3.0.0 in /usr/local/lib/python3.9/site-packages (from distributed==2024.2.1->dask[dataframe,diagnostics,distributed]<2024.3.0,>=2023.4.1->fondant[component]->-r requirements.txt (line 1)) (3.0.0)\n",
      "#8 6.579 Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.9/site-packages (from distributed==2024.2.1->dask[dataframe,diagnostics,distributed]<2024.3.0,>=2023.4.1->fondant[component]->-r requirements.txt (line 1)) (3.0.0)\n",
      "#8 6.580 Requirement already satisfied: psutil>=5.7.2 in /usr/local/lib/python3.9/site-packages (from distributed==2024.2.1->dask[dataframe,diagnostics,distributed]<2024.3.0,>=2023.4.1->fondant[component]->-r requirements.txt (line 1)) (5.9.8)\n",
      "#8 6.581 Requirement already satisfied: msgpack>=1.0.0 in /usr/local/lib/python3.9/site-packages (from distributed==2024.2.1->dask[dataframe,diagnostics,distributed]<2024.3.0,>=2023.4.1->fondant[component]->-r requirements.txt (line 1)) (1.0.8)\n",
      "#8 6.582 Requirement already satisfied: sortedcontainers>=2.0.5 in /usr/local/lib/python3.9/site-packages (from distributed==2024.2.1->dask[dataframe,diagnostics,distributed]<2024.3.0,>=2023.4.1->fondant[component]->-r requirements.txt (line 1)) (2.4.0)\n",
      "#8 6.583 Requirement already satisfied: urllib3>=1.24.3 in /usr/local/lib/python3.9/site-packages (from distributed==2024.2.1->dask[dataframe,diagnostics,distributed]<2024.3.0,>=2023.4.1->fondant[component]->-r requirements.txt (line 1)) (1.26.18)\n",
      "#8 6.584 Requirement already satisfied: locket>=1.0.0 in /usr/local/lib/python3.9/site-packages (from distributed==2024.2.1->dask[dataframe,diagnostics,distributed]<2024.3.0,>=2023.4.1->fondant[component]->-r requirements.txt (line 1)) (1.0.0)\n",
      "#8 6.590 Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.9/site-packages (from docker>=6.1.3->fondant[component]->-r requirements.txt (line 1)) (2.31.0)\n",
      "#8 6.631 Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.9/site-packages (from jsonschema>=4.18->fondant[component]->-r requirements.txt (line 1)) (23.2.0)\n",
      "#8 6.632 Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.9/site-packages (from jsonschema>=4.18->fondant[component]->-r requirements.txt (line 1)) (0.34.0)\n",
      "#8 6.633 Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.9/site-packages (from jsonschema>=4.18->fondant[component]->-r requirements.txt (line 1)) (2023.12.1)\n",
      "#8 6.633 Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.9/site-packages (from jsonschema>=4.18->fondant[component]->-r requirements.txt (line 1)) (0.18.0)\n",
      "#8 6.659 Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.9/site-packages (from bokeh>=2.4.2->dask[dataframe,diagnostics,distributed]<2024.3.0,>=2023.4.1->fondant[component]->-r requirements.txt (line 1)) (10.3.0)\n",
      "#8 6.660 Requirement already satisfied: xyzservices>=2021.09.1 in /usr/local/lib/python3.9/site-packages (from bokeh>=2.4.2->dask[dataframe,diagnostics,distributed]<2024.3.0,>=2023.4.1->fondant[component]->-r requirements.txt (line 1)) (2024.4.0)\n",
      "#8 6.661 Requirement already satisfied: contourpy>=1.2 in /usr/local/lib/python3.9/site-packages (from bokeh>=2.4.2->dask[dataframe,diagnostics,distributed]<2024.3.0,>=2023.4.1->fondant[component]->-r requirements.txt (line 1)) (1.2.1)\n",
      "#8 6.680 Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/site-packages (from importlib-metadata>=4.13.0->dask[dataframe,diagnostics,distributed]<2024.3.0,>=2023.4.1->fondant[component]->-r requirements.txt (line 1)) (3.18.1)\n",
      "#8 6.701 Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/site-packages (from jinja2>=2.10.3->dask[dataframe,diagnostics,distributed]<2024.3.0,>=2023.4.1->fondant[component]->-r requirements.txt (line 1)) (2.1.5)\n",
      "#8 6.768 Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/site-packages (from pandas>=1.3->dask[dataframe,diagnostics,distributed]<2024.3.0,>=2023.4.1->fondant[component]->-r requirements.txt (line 1)) (2024.1)\n",
      "#8 6.769 Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.9/site-packages (from pandas>=1.3->dask[dataframe,diagnostics,distributed]<2024.3.0,>=2023.4.1->fondant[component]->-r requirements.txt (line 1)) (2024.1)\n",
      "#8 6.770 Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.9/site-packages (from pandas>=1.3->dask[dataframe,diagnostics,distributed]<2024.3.0,>=2023.4.1->fondant[component]->-r requirements.txt (line 1)) (2.9.0.post0)\n",
      "#8 6.785 Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/site-packages (from requests>=2.26.0->docker>=6.1.3->fondant[component]->-r requirements.txt (line 1)) (2024.2.2)\n",
      "#8 6.785 Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/site-packages (from requests>=2.26.0->docker>=6.1.3->fondant[component]->-r requirements.txt (line 1)) (3.3.2)\n",
      "#8 6.786 Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/site-packages (from requests>=2.26.0->docker>=6.1.3->fondant[component]->-r requirements.txt (line 1)) (3.7)\n",
      "#8 6.857 Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas>=1.3->dask[dataframe,diagnostics,distributed]<2024.3.0,>=2023.4.1->fondant[component]->-r requirements.txt (line 1)) (1.16.0)\n",
      "#8 7.030 Installing collected packages: pyarrow, biopython\n",
      "#8 7.030   Attempting uninstall: pyarrow\n",
      "#8 7.031     Found existing installation: pyarrow 16.0.0\n",
      "#8 7.081     Uninstalling pyarrow-16.0.0:\n",
      "#8 7.201       Successfully uninstalled pyarrow-16.0.0\n",
      "#8 8.896 Successfully installed biopython-1.83 pyarrow-15.0.0\n",
      "#8 8.896 WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "#8 9.007 \n",
      "#8 9.007 [notice] A new release of pip is available: 23.0.1 -> 24.0\n",
      "#8 9.007 [notice] To update, run: pip install --upgrade pip\n",
      "#8 DONE 9.1s\n",
      "\n",
      "#9 [biopython_component 5/6] WORKDIR /component/src\n",
      "#9 DONE 0.1s\n",
      "\n",
      "#10 [biopython_component 6/6] COPY src/ .\n",
      "#10 DONE 0.1s\n",
      "\n",
      "#11 [biopython_component] exporting to image\n",
      "#11 exporting layers\n",
      "#11 exporting layers 0.3s done\n",
      "#11 writing image sha256:929d5e2da4d5e0e368be9fd8dc0e33f0bf7e3bfcadd0f65742b10365b1fc12e2 done\n",
      "#11 naming to docker.io/library/dataset-17-biopython_component done\n",
      "#11 DONE 0.4s\n",
      "Attaching to biopython_component-1, load_from_parquet-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Network dataset-17_default  Creating\n",
      " Network dataset-17_default  Created\n",
      " Container dataset-17-load_from_parquet-1  Creating\n",
      " Container dataset-17-load_from_parquet-1  Created\n",
      " Container dataset-17-biopython_component-1  Creating\n",
      " Container dataset-17-biopython_component-1  Created\n",
      "load_from_parquet-1    | [2024-06-12 11:16:27,624 | fondant.cli | INFO] Component `LoadFromParquet` found in module main\n",
      "load_from_parquet-1    | [2024-06-12 11:16:27,628 | fondant.component.executor | INFO] Caching is currently temporarily disabled.\n",
      "load_from_parquet-1    | [2024-06-12 11:16:27,628 | fondant.component.executor | INFO] No matching execution for component detected\n",
      "load_from_parquet-1    | [2024-06-12 11:16:27,628 | root | INFO] Executing component\n",
      "load_from_parquet-1    | [2024-06-12 11:16:27,968 | distributed.http.proxy | INFO] To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy\n",
      "load_from_parquet-1    | [2024-06-12 11:16:27,998 | distributed.scheduler | INFO] State start\n",
      "load_from_parquet-1    | [2024-06-12 11:16:28,013 | distributed.scheduler | INFO]   Scheduler at:     tcp://127.0.0.1:40579\n",
      "load_from_parquet-1    | [2024-06-12 11:16:28,014 | distributed.scheduler | INFO]   dashboard at:  http://127.0.0.1:8787/status\n",
      "load_from_parquet-1    | [2024-06-12 11:16:28,014 | distributed.scheduler | INFO] Registering Worker plugin shuffle\n",
      "load_from_parquet-1    | [2024-06-12 11:16:28,039 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:34875'\n",
      "load_from_parquet-1    | [2024-06-12 11:16:28,045 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:35853'\n",
      "load_from_parquet-1    | [2024-06-12 11:16:28,048 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:37923'\n",
      "load_from_parquet-1    | [2024-06-12 11:16:28,052 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:42349'\n",
      "load_from_parquet-1    | [2024-06-12 11:16:28,065 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:39775'\n",
      "load_from_parquet-1    | [2024-06-12 11:16:28,068 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:37233'\n",
      "load_from_parquet-1    | [2024-06-12 11:16:28,071 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:44545'\n",
      "load_from_parquet-1    | [2024-06-12 11:16:28,075 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:35857'\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,929 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:44419\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,930 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:44419\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,930 | distributed.worker | INFO]           Worker name:                          7\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,930 | distributed.worker | INFO]          dashboard at:            127.0.0.1:34701\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,930 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:40579\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,930 | distributed.worker | INFO] -------------------------------------------------\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,930 | distributed.worker | INFO]               Threads:                          1\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,930 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,930 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-iu29rewi\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,930 | distributed.worker | INFO] -------------------------------------------------\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,950 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:44419', name: 7, status: init, memory: 0, processing: 0>\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,956 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:44419\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,957 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:60834\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,960 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,962 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:40579\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,962 | distributed.worker | INFO] -------------------------------------------------\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,964 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:42531\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,965 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:42531\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,966 | distributed.worker | INFO]           Worker name:                          2\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,966 | distributed.worker | INFO]          dashboard at:            127.0.0.1:36933\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,966 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:40579\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,966 | distributed.worker | INFO] -------------------------------------------------\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,966 | distributed.worker | INFO]               Threads:                          1\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,966 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,966 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-u7txhtiz\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,967 | distributed.worker | INFO] -------------------------------------------------\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,971 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:40579\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,976 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:39219\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,976 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:39219\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,976 | distributed.worker | INFO]           Worker name:                          4\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,976 | distributed.worker | INFO]          dashboard at:            127.0.0.1:36941\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,976 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:40579\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,976 | distributed.worker | INFO] -------------------------------------------------\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,976 | distributed.worker | INFO]               Threads:                          1\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,977 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,977 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-t5gpi43a\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,977 | distributed.worker | INFO] -------------------------------------------------\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,980 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:42531', name: 2, status: init, memory: 0, processing: 0>\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,982 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:42531\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,982 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:60842\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,983 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,983 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:40579\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,984 | distributed.worker | INFO] -------------------------------------------------\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,985 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:40579\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,990 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:39219', name: 4, status: init, memory: 0, processing: 0>\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,992 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:38963\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,993 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:38963\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,993 | distributed.worker | INFO]           Worker name:                          1\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,993 | distributed.worker | INFO]          dashboard at:            127.0.0.1:40147\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,993 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:40579\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,993 | distributed.worker | INFO] -------------------------------------------------\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,994 | distributed.worker | INFO]               Threads:                          1\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,994 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,994 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-qflsia9_\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,994 | distributed.worker | INFO] -------------------------------------------------\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,994 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:39219\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,995 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:60850\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,995 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,995 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:34457\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,995 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:34457\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,995 | distributed.worker | INFO]           Worker name:                          5\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,995 | distributed.worker | INFO]          dashboard at:            127.0.0.1:36075\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,995 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:40579\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,996 | distributed.worker | INFO] -------------------------------------------------\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,996 | distributed.worker | INFO]               Threads:                          1\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,996 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:40579\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,996 | distributed.worker | INFO] -------------------------------------------------\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,996 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,996 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-ki5b91li\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,996 | distributed.worker | INFO] -------------------------------------------------\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,996 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:41955\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,996 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:41955\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,996 | distributed.worker | INFO]           Worker name:                          0\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,996 | distributed.worker | INFO]          dashboard at:            127.0.0.1:44459\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,996 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:40579\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,996 | distributed.worker | INFO] -------------------------------------------------\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,996 | distributed.worker | INFO]               Threads:                          1\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,997 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,997 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-x8sqj73g\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,997 | distributed.worker | INFO] -------------------------------------------------\n",
      "load_from_parquet-1    | [2024-06-12 11:16:29,999 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:40579\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,003 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:34457', name: 5, status: init, memory: 0, processing: 0>\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,004 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:34457\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,004 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:60870\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,005 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,006 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:41955', name: 0, status: init, memory: 0, processing: 0>\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,006 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:40579\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,006 | distributed.worker | INFO] -------------------------------------------------\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,006 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:40579\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,006 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:41955\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,006 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:60874\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,007 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,008 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:38963', name: 1, status: init, memory: 0, processing: 0>\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,008 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:40579\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,008 | distributed.worker | INFO] -------------------------------------------------\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,009 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:38963\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,009 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:60864\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,009 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:40579\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,009 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,010 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:40579\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,010 | distributed.worker | INFO] -------------------------------------------------\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,011 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:40579\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,096 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:46563\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,097 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:46563\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,097 | distributed.worker | INFO]           Worker name:                          6\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,097 | distributed.worker | INFO]          dashboard at:            127.0.0.1:46393\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,097 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:40579\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,097 | distributed.worker | INFO] -------------------------------------------------\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,097 | distributed.worker | INFO]               Threads:                          1\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,097 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,097 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-lzjrnb55\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,097 | distributed.worker | INFO] -------------------------------------------------\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,103 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:46563', name: 6, status: init, memory: 0, processing: 0>\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,104 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:46563\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,104 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:60884\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,105 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,106 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:40579\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,106 | distributed.worker | INFO] -------------------------------------------------\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,107 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:40579\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,136 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:43835\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,136 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:43835\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,136 | distributed.worker | INFO]           Worker name:                          3\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,136 | distributed.worker | INFO]          dashboard at:            127.0.0.1:33289\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,136 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:40579\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,136 | distributed.worker | INFO] -------------------------------------------------\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,136 | distributed.worker | INFO]               Threads:                          1\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,136 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,136 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-mh66jma3\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,136 | distributed.worker | INFO] -------------------------------------------------\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,141 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:43835', name: 3, status: init, memory: 0, processing: 0>\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,142 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:43835\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,142 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:60896\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,142 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,143 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:40579\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,143 | distributed.worker | INFO] -------------------------------------------------\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,144 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:40579\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,190 | distributed.scheduler | INFO] Receive client connection: Client-37c310fb-28ad-11ef-8001-0242ac190002\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,191 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:60906\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,192 | main | INFO] Loading dataset from the hub...\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,217 | main | INFO] Index column not specified, setting a globally unique index\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,231 | root | WARNING] Failed to infer dtype of index column, falling back to `string`. Specify the dtype explicitly to prevent this.\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,402 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:34875'. Reason: nanny-close\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,402 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,402 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:35853'. Reason: nanny-close\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,402 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,403 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:37923'. Reason: nanny-close\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,403 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,403 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:42349'. Reason: nanny-close\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,403 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:41955. Reason: nanny-close\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,403 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:38963. Reason: nanny-close\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,403 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,403 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:39775'. Reason: nanny-close\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,404 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,404 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:37233'. Reason: nanny-close\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,404 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:43835. Reason: nanny-close\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,405 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:42531. Reason: nanny-close\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,405 | distributed.core | INFO] Connection to tcp://127.0.0.1:40579 has been closed.\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,405 | distributed.core | INFO] Connection to tcp://127.0.0.1:40579 has been closed.\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,406 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,406 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:44545'. Reason: nanny-close\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,406 | distributed.core | INFO] Connection to tcp://127.0.0.1:40579 has been closed.\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,407 | distributed.core | INFO] Connection to tcp://127.0.0.1:40579 has been closed.\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,407 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:34457. Reason: nanny-close\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,407 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:39219. Reason: nanny-close\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,407 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,408 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:35857'. Reason: nanny-close\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,408 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,409 | distributed.core | INFO] Connection to tcp://127.0.0.1:40579 has been closed.\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,410 | distributed.core | INFO] Connection to tcp://127.0.0.1:40579 has been closed.\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,410 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:44419. Reason: nanny-close\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,412 | distributed.core | INFO] Connection to tcp://127.0.0.1:40579 has been closed.\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,413 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:60874; closing.\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,413 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:60864; closing.\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,414 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:60896; closing.\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,414 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:60842; closing.\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,414 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:60870; closing.\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,414 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:60850; closing.\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,415 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:41955', name: 0, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718190990.4152262')\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,416 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:46563. Reason: nanny-close\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,419 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:38963', name: 1, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718190990.4188588')\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,420 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:43835', name: 3, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718190990.4199238')\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,420 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:42531', name: 2, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718190990.420636')\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,421 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:34457', name: 5, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718190990.421173')\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,424 | distributed.core | INFO] Connection to tcp://127.0.0.1:40579 has been closed.\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,422 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:39219', name: 4, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718190990.421595')\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,430 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:60834; closing.\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,432 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:44419', name: 7, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718190990.4321113')\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,434 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:60884; closing.\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,436 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:46563', name: 6, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718190990.4358516')\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,436 | distributed.scheduler | INFO] Lost all workers\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,825 | distributed.scheduler | INFO] Scheduler closing due to unknown reason...\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,826 | distributed.scheduler | INFO] Scheduler closing all comms\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,830 | fondant.component.executor | INFO] Saving output manifest to /home/pietercoussement/Software/Sandbox/protein-feature-extraction/data/:/data/dataset-17/dataset-17-20240612131522/load_from_parquet/manifest.json\n",
      "load_from_parquet-1    | [2024-06-12 11:16:30,830 | fondant.component.executor | INFO] Writing cache key with manifest reference to /home/pietercoussement/Software/Sandbox/protein-feature-extraction/data/:/data/dataset-17/cache/80e8e3d381608751bd0b582a6aa398fa.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[Kload_from_parquet-1 exited with code 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "biopython_component-1  | [2024-06-12 11:16:33,319 | fondant.cli | INFO] Component `BiopythonComponent` found in module main\n",
      "biopython_component-1  | [2024-06-12 11:16:33,324 | fondant.component.executor | INFO] Caching disabled for the component\n",
      "biopython_component-1  | [2024-06-12 11:16:33,325 | root | INFO] Executing component\n",
      "biopython_component-1  | [2024-06-12 11:16:33,851 | distributed.http.proxy | INFO] To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy\n",
      "biopython_component-1  | [2024-06-12 11:16:33,902 | distributed.scheduler | INFO] State start\n",
      "biopython_component-1  | [2024-06-12 11:16:33,908 | distributed.scheduler | INFO]   Scheduler at:     tcp://127.0.0.1:37543\n",
      "biopython_component-1  | [2024-06-12 11:16:33,908 | distributed.scheduler | INFO]   dashboard at:  http://127.0.0.1:8787/status\n",
      "biopython_component-1  | [2024-06-12 11:16:33,909 | distributed.scheduler | INFO] Registering Worker plugin shuffle\n",
      "biopython_component-1  | [2024-06-12 11:16:33,948 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:44963'\n",
      "biopython_component-1  | [2024-06-12 11:16:33,961 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:44879'\n",
      "biopython_component-1  | [2024-06-12 11:16:33,971 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:33253'\n",
      "biopython_component-1  | [2024-06-12 11:16:33,980 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:35509'\n",
      "biopython_component-1  | [2024-06-12 11:16:34,009 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:42493'\n",
      "biopython_component-1  | [2024-06-12 11:16:34,021 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:42589'\n",
      "biopython_component-1  | [2024-06-12 11:16:34,033 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:46601'\n",
      "biopython_component-1  | [2024-06-12 11:16:34,046 | distributed.nanny | INFO]         Start Nanny at: 'tcp://127.0.0.1:45585'\n",
      "biopython_component-1  | [2024-06-12 11:16:35,820 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:39909\n",
      "biopython_component-1  | [2024-06-12 11:16:35,821 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:39909\n",
      "biopython_component-1  | [2024-06-12 11:16:35,821 | distributed.worker | INFO]           Worker name:                          4\n",
      "biopython_component-1  | [2024-06-12 11:16:35,821 | distributed.worker | INFO]          dashboard at:            127.0.0.1:35951\n",
      "biopython_component-1  | [2024-06-12 11:16:35,821 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:37543\n",
      "biopython_component-1  | [2024-06-12 11:16:35,821 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1  | [2024-06-12 11:16:35,821 | distributed.worker | INFO]               Threads:                          1\n",
      "biopython_component-1  | [2024-06-12 11:16:35,821 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "biopython_component-1  | [2024-06-12 11:16:35,821 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-suyb9kvg\n",
      "biopython_component-1  | [2024-06-12 11:16:35,821 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1  | [2024-06-12 11:16:35,855 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:39909', name: 4, status: init, memory: 0, processing: 0>\n",
      "biopython_component-1  | [2024-06-12 11:16:35,890 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:39909\n",
      "biopython_component-1  | [2024-06-12 11:16:35,891 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:55744\n",
      "biopython_component-1  | [2024-06-12 11:16:35,894 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "biopython_component-1  | [2024-06-12 11:16:35,895 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:37543\n",
      "biopython_component-1  | [2024-06-12 11:16:35,896 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1  | [2024-06-12 11:16:35,901 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:37543\n",
      "biopython_component-1  | [2024-06-12 11:16:36,014 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:32773\n",
      "biopython_component-1  | [2024-06-12 11:16:36,014 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:32773\n",
      "biopython_component-1  | [2024-06-12 11:16:36,015 | distributed.worker | INFO]           Worker name:                          2\n",
      "biopython_component-1  | [2024-06-12 11:16:36,015 | distributed.worker | INFO]          dashboard at:            127.0.0.1:42081\n",
      "biopython_component-1  | [2024-06-12 11:16:36,015 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:37543\n",
      "biopython_component-1  | [2024-06-12 11:16:36,015 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1  | [2024-06-12 11:16:36,015 | distributed.worker | INFO]               Threads:                          1\n",
      "biopython_component-1  | [2024-06-12 11:16:36,015 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "biopython_component-1  | [2024-06-12 11:16:36,015 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-nr1phfhk\n",
      "biopython_component-1  | [2024-06-12 11:16:36,015 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1  | [2024-06-12 11:16:36,032 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:32773', name: 2, status: init, memory: 0, processing: 0>\n",
      "biopython_component-1  | [2024-06-12 11:16:36,034 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:32773\n",
      "biopython_component-1  | [2024-06-12 11:16:36,034 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:55760\n",
      "biopython_component-1  | [2024-06-12 11:16:36,042 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "biopython_component-1  | [2024-06-12 11:16:36,048 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:37543\n",
      "biopython_component-1  | [2024-06-12 11:16:36,048 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1  | [2024-06-12 11:16:36,051 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:37543\n",
      "biopython_component-1  | [2024-06-12 11:16:36,067 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:41667\n",
      "biopython_component-1  | [2024-06-12 11:16:36,067 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:41667\n",
      "biopython_component-1  | [2024-06-12 11:16:36,067 | distributed.worker | INFO]           Worker name:                          1\n",
      "biopython_component-1  | [2024-06-12 11:16:36,067 | distributed.worker | INFO]          dashboard at:            127.0.0.1:33227\n",
      "biopython_component-1  | [2024-06-12 11:16:36,067 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:37543\n",
      "biopython_component-1  | [2024-06-12 11:16:36,068 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1  | [2024-06-12 11:16:36,068 | distributed.worker | INFO]               Threads:                          1\n",
      "biopython_component-1  | [2024-06-12 11:16:36,068 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "biopython_component-1  | [2024-06-12 11:16:36,068 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-y1flrwbh\n",
      "biopython_component-1  | [2024-06-12 11:16:36,068 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1  | [2024-06-12 11:16:36,088 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:41667', name: 1, status: init, memory: 0, processing: 0>\n",
      "biopython_component-1  | [2024-06-12 11:16:36,089 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:41667\n",
      "biopython_component-1  | [2024-06-12 11:16:36,089 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:55772\n",
      "biopython_component-1  | [2024-06-12 11:16:36,093 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "biopython_component-1  | [2024-06-12 11:16:36,095 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:37543\n",
      "biopython_component-1  | [2024-06-12 11:16:36,095 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1  | [2024-06-12 11:16:36,096 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:37543\n",
      "biopython_component-1  | [2024-06-12 11:16:36,165 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:36915\n",
      "biopython_component-1  | [2024-06-12 11:16:36,165 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:36915\n",
      "biopython_component-1  | [2024-06-12 11:16:36,165 | distributed.worker | INFO]           Worker name:                          5\n",
      "biopython_component-1  | [2024-06-12 11:16:36,165 | distributed.worker | INFO]          dashboard at:            127.0.0.1:33371\n",
      "biopython_component-1  | [2024-06-12 11:16:36,165 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:37543\n",
      "biopython_component-1  | [2024-06-12 11:16:36,165 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1  | [2024-06-12 11:16:36,165 | distributed.worker | INFO]               Threads:                          1\n",
      "biopython_component-1  | [2024-06-12 11:16:36,166 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "biopython_component-1  | [2024-06-12 11:16:36,166 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-navwk5l0\n",
      "biopython_component-1  | [2024-06-12 11:16:36,166 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1  | [2024-06-12 11:16:36,179 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:36915', name: 5, status: init, memory: 0, processing: 0>\n",
      "biopython_component-1  | [2024-06-12 11:16:36,182 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:36915\n",
      "biopython_component-1  | [2024-06-12 11:16:36,182 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:55782\n",
      "biopython_component-1  | [2024-06-12 11:16:36,184 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "biopython_component-1  | [2024-06-12 11:16:36,186 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:37543\n",
      "biopython_component-1  | [2024-06-12 11:16:36,186 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1  | [2024-06-12 11:16:36,187 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:37543\n",
      "biopython_component-1  | [2024-06-12 11:16:36,277 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:36025\n",
      "biopython_component-1  | [2024-06-12 11:16:36,278 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:36025\n",
      "biopython_component-1  | [2024-06-12 11:16:36,278 | distributed.worker | INFO]           Worker name:                          0\n",
      "biopython_component-1  | [2024-06-12 11:16:36,278 | distributed.worker | INFO]          dashboard at:            127.0.0.1:44673\n",
      "biopython_component-1  | [2024-06-12 11:16:36,278 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:37543\n",
      "biopython_component-1  | [2024-06-12 11:16:36,278 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1  | [2024-06-12 11:16:36,278 | distributed.worker | INFO]               Threads:                          1\n",
      "biopython_component-1  | [2024-06-12 11:16:36,278 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "biopython_component-1  | [2024-06-12 11:16:36,278 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-i5zhdudn\n",
      "biopython_component-1  | [2024-06-12 11:16:36,278 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1  | [2024-06-12 11:16:36,292 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:36025', name: 0, status: init, memory: 0, processing: 0>\n",
      "biopython_component-1  | [2024-06-12 11:16:36,293 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:36025\n",
      "biopython_component-1  | [2024-06-12 11:16:36,293 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:55790\n",
      "biopython_component-1  | [2024-06-12 11:16:36,294 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "biopython_component-1  | [2024-06-12 11:16:36,295 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:37543\n",
      "biopython_component-1  | [2024-06-12 11:16:36,295 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1  | [2024-06-12 11:16:36,297 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:37543\n",
      "biopython_component-1  | [2024-06-12 11:16:36,307 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:39577\n",
      "biopython_component-1  | [2024-06-12 11:16:36,307 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:39577\n",
      "biopython_component-1  | [2024-06-12 11:16:36,308 | distributed.worker | INFO]           Worker name:                          3\n",
      "biopython_component-1  | [2024-06-12 11:16:36,308 | distributed.worker | INFO]          dashboard at:            127.0.0.1:41359\n",
      "biopython_component-1  | [2024-06-12 11:16:36,308 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:37543\n",
      "biopython_component-1  | [2024-06-12 11:16:36,308 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1  | [2024-06-12 11:16:36,308 | distributed.worker | INFO]               Threads:                          1\n",
      "biopython_component-1  | [2024-06-12 11:16:36,308 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "biopython_component-1  | [2024-06-12 11:16:36,308 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-3oy40g6l\n",
      "biopython_component-1  | [2024-06-12 11:16:36,308 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1  | [2024-06-12 11:16:36,322 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:39577', name: 3, status: init, memory: 0, processing: 0>\n",
      "biopython_component-1  | [2024-06-12 11:16:36,324 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:39577\n",
      "biopython_component-1  | [2024-06-12 11:16:36,324 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:55792\n",
      "biopython_component-1  | [2024-06-12 11:16:36,325 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "biopython_component-1  | [2024-06-12 11:16:36,326 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:37543\n",
      "biopython_component-1  | [2024-06-12 11:16:36,326 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1  | [2024-06-12 11:16:36,328 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:37543\n",
      "biopython_component-1  | [2024-06-12 11:16:36,338 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:45127\n",
      "biopython_component-1  | [2024-06-12 11:16:36,338 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:45127\n",
      "biopython_component-1  | [2024-06-12 11:16:36,338 | distributed.worker | INFO]           Worker name:                          6\n",
      "biopython_component-1  | [2024-06-12 11:16:36,338 | distributed.worker | INFO]          dashboard at:            127.0.0.1:43649\n",
      "biopython_component-1  | [2024-06-12 11:16:36,338 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:37543\n",
      "biopython_component-1  | [2024-06-12 11:16:36,338 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1  | [2024-06-12 11:16:36,338 | distributed.worker | INFO]               Threads:                          1\n",
      "biopython_component-1  | [2024-06-12 11:16:36,339 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "biopython_component-1  | [2024-06-12 11:16:36,339 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-ccu_20cb\n",
      "biopython_component-1  | [2024-06-12 11:16:36,339 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1  | [2024-06-12 11:16:36,350 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:45127', name: 6, status: init, memory: 0, processing: 0>\n",
      "biopython_component-1  | [2024-06-12 11:16:36,351 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:45127\n",
      "biopython_component-1  | [2024-06-12 11:16:36,351 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:55794\n",
      "biopython_component-1  | [2024-06-12 11:16:36,352 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "biopython_component-1  | [2024-06-12 11:16:36,353 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:37543\n",
      "biopython_component-1  | [2024-06-12 11:16:36,353 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1  | [2024-06-12 11:16:36,354 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:37543\n",
      "biopython_component-1  | [2024-06-12 11:16:36,402 | distributed.worker | INFO]       Start worker at:      tcp://127.0.0.1:35577\n",
      "biopython_component-1  | [2024-06-12 11:16:36,402 | distributed.worker | INFO]          Listening to:      tcp://127.0.0.1:35577\n",
      "biopython_component-1  | [2024-06-12 11:16:36,402 | distributed.worker | INFO]           Worker name:                          7\n",
      "biopython_component-1  | [2024-06-12 11:16:36,403 | distributed.worker | INFO]          dashboard at:            127.0.0.1:46169\n",
      "biopython_component-1  | [2024-06-12 11:16:36,403 | distributed.worker | INFO] Waiting to connect to:      tcp://127.0.0.1:37543\n",
      "biopython_component-1  | [2024-06-12 11:16:36,403 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1  | [2024-06-12 11:16:36,403 | distributed.worker | INFO]               Threads:                          1\n",
      "biopython_component-1  | [2024-06-12 11:16:36,403 | distributed.worker | INFO]                Memory:                 468.51 MiB\n",
      "biopython_component-1  | [2024-06-12 11:16:36,403 | distributed.worker | INFO]       Local Directory: /tmp/dask-scratch-space/worker-2tcansqm\n",
      "biopython_component-1  | [2024-06-12 11:16:36,403 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1  | [2024-06-12 11:16:36,422 | distributed.scheduler | INFO] Register worker <WorkerState 'tcp://127.0.0.1:35577', name: 7, status: init, memory: 0, processing: 0>\n",
      "biopython_component-1  | [2024-06-12 11:16:36,423 | distributed.scheduler | INFO] Starting worker compute stream, tcp://127.0.0.1:35577\n",
      "biopython_component-1  | [2024-06-12 11:16:36,423 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:55800\n",
      "biopython_component-1  | [2024-06-12 11:16:36,424 | distributed.worker | INFO] Starting Worker plugin shuffle\n",
      "biopython_component-1  | [2024-06-12 11:16:36,425 | distributed.worker | INFO]         Registered to:      tcp://127.0.0.1:37543\n",
      "biopython_component-1  | [2024-06-12 11:16:36,425 | distributed.worker | INFO] -------------------------------------------------\n",
      "biopython_component-1  | [2024-06-12 11:16:36,426 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:37543\n",
      "biopython_component-1  | [2024-06-12 11:16:36,493 | distributed.scheduler | INFO] Receive client connection: Client-3b84f7d1-28ad-11ef-8001-0242ac190002\n",
      "biopython_component-1  | [2024-06-12 11:16:36,494 | distributed.core | INFO] Starting established connection to tcp://127.0.0.1:55812\n",
      "biopython_component-1  | [2024-06-12 11:16:36,560 | fondant.component.data_io | INFO] The number of partitions of the input dataframe is 1. The available number of workers is 8.\n",
      "biopython_component-1  | [2024-06-12 11:16:36,562 | fondant.component.data_io | INFO] Repartitioning the data to 8 partitions before processing to maximize worker usage\n",
      "biopython_component-1  | [2024-06-12 11:16:36,563 | root | INFO] Columns of dataframe: ['sequence']\n",
      "biopython_component-1  | [2024-06-12 11:16:36,624 | root | WARNING] Failed to infer dtype of index column, falling back to `string`. Specify the dtype explicitly to prevent this.\n",
      "biopython_component-1  | [2024-06-12 11:16:36,815 | fondant.component.executor | INFO] Received empty partition, skipping transformation.\n",
      "biopython_component-1  | [2024-06-12 11:16:36,906 | fondant.component.executor | INFO] Received empty partition, skipping transformation.\n",
      "biopython_component-1  | [2024-06-12 11:16:36,986 | distributed.worker | WARNING] Compute Failed\n",
      "biopython_component-1  | Key:       ('getitem-2c482f30bc6bd24cbbd0f21f7bdc95c0', 5)\n",
      "biopython_component-1  | Function:  subgraph_callable-02b71b2e791f1b8b7dad50237c938bae\n",
      "biopython_component-1  | args:      (['sequence', 'sequence_length', 'molecular_weight', 'aromaticity', 'isoelectric_point', 'instability_index', 'gravy', 'helix', 'turn', 'sheet', 'charge_at_ph3', 'charge_at_ph5', 'charge_at_ph7', 'charge_at_ph9', 'molar_extinction_coefficient_oxidized', 'molar_extinction_coefficient_reduced', 'flexibility_max', 'flexibility_min', 'flexibility_mean'], Index([], dtype='object', name='id'), 'index', Empty DataFrame\n",
      "biopython_component-1  | Columns: [sequence]\n",
      "biopython_component-1  | Index: [], 'repartition-8-49213cf5bf48eacafb23352962c536d2')\n",
      "biopython_component-1  | kwargs:    {}\n",
      "biopython_component-1  | Exception: 'KeyError(\"[\\'sequence_length\\', \\'molecular_weight\\', \\'aromaticity\\', \\'isoelectric_point\\', \\'instability_index\\', \\'gravy\\', \\'helix\\', \\'turn\\', \\'sheet\\', \\'charge_at_ph3\\', \\'charge_at_ph5\\', \\'charge_at_ph7\\', \\'charge_at_ph9\\', \\'molar_extinction_coefficient_oxidized\\', \\'molar_extinction_coefficient_reduced\\', \\'flexibility_max\\', \\'flexibility_min\\', \\'flexibility_mean\\'] not in index\")'\n",
      "biopython_component-1  | \n",
      "biopython_component-1  | [2024-06-12 11:16:37,007 | fondant.component.executor | INFO] Received empty partition, skipping transformation.\n",
      "biopython_component-1  | Traceback (most recent call last):\n",
      "biopython_component-1  |   File \"/usr/local/bin/fondant\", line 8, in <module>\n",
      "biopython_component-1  |     sys.exit(entrypoint())\n",
      "biopython_component-1  |   File \"/usr/local/lib/python3.9/site-packages/fondant/cli.py\", line 90, in entrypoint\n",
      "biopython_component-1  |     args.func(args)\n",
      "biopython_component-1  |   File \"/usr/local/lib/python3.9/site-packages/fondant/cli.py\", line 749, in execute\n",
      "biopython_component-1  | [2024-06-12 11:16:37,014 | distributed.worker | WARNING] Compute Failed\n",
      "biopython_component-1  | Key:       ('getitem-2c482f30bc6bd24cbbd0f21f7bdc95c0', 0)\n",
      "biopython_component-1  | Function:  subgraph_callable-02b71b2e791f1b8b7dad50237c938bae\n",
      "biopython_component-1  | args:      (['sequence', 'sequence_length', 'molecular_weight', 'aromaticity', 'isoelectric_point', 'instability_index', 'gravy', 'helix', 'turn', 'sheet', 'charge_at_ph3', 'charge_at_ph5', 'charge_at_ph7', 'charge_at_ph9', 'molar_extinction_coefficient_oxidized', 'molar_extinction_coefficient_reduced', 'flexibility_max', 'flexibility_min', 'flexibility_mean'], Index([], dtype='object', name='id'), 'index', Empty DataFrame\n",
      "biopython_component-1  | Columns: [sequence]\n",
      "biopython_component-1  | Index: [], 'repartition-8-49213cf5bf48eacafb23352962c536d2')\n",
      "biopython_component-1  | kwargs:    {}\n",
      "biopython_component-1  | Exception: 'KeyError(\"[\\'sequence_length\\', \\'molecular_weight\\', \\'aromaticity\\', \\'isoelectric_point\\', \\'instability_index\\', \\'gravy\\', \\'helix\\', \\'turn\\', \\'sheet\\', \\'charge_at_ph3\\', \\'charge_at_ph5\\', \\'charge_at_ph7\\', \\'charge_at_ph9\\', \\'molar_extinction_coefficient_oxidized\\', \\'molar_extinction_coefficient_reduced\\', \\'flexibility_max\\', \\'flexibility_min\\', \\'flexibility_mean\\'] not in index\")'\n",
      "biopython_component-1  | \n",
      "biopython_component-1  |     executor.execute(component)\n",
      "biopython_component-1  |   File \"/usr/local/lib/python3.9/site-packages/fondant/component/executor.py\", line 349, in execute\n",
      "biopython_component-1  |     output_manifest = self._run_execution(component_cls, input_manifest)\n",
      "biopython_component-1  |   File \"/usr/local/lib/python3.9/site-packages/fondant/component/executor.py\", line 312, in _run_execution\n",
      "biopython_component-1  |     self._write_data(dataframe=output_df, manifest=output_manifest)\n",
      "biopython_component-1  |   File \"/usr/local/lib/python3.9/site-packages/fondant/component/executor.py\", line 229, in _write_data\n",
      "biopython_component-1  |     data_writer.write_dataframe(dataframe)\n",
      "biopython_component-1  |   File \"/usr/local/lib/python3.9/site-packages/fondant/component/data_io.py\", line 178, in write_dataframe\n",
      "biopython_component-1  |     self._write_dataframe(dataframe)\n",
      "biopython_component-1  |   File \"/usr/local/lib/python3.9/site-packages/fondant/component/data_io.py\", line 252, in _write_dataframe\n",
      "biopython_component-1  |     future.result()\n",
      "biopython_component-1  |   File \"/usr/local/lib/python3.9/site-packages/distributed/client.py\", line 323, in result\n",
      "biopython_component-1  |     return self.client.sync(self._result, callback_timeout=timeout)\n",
      "biopython_component-1  |   File \"/usr/local/lib/python3.9/site-packages/fondant/component/executor.py\", line 515, in wrapped_transform\n",
      "biopython_component-1  |     return dataframe[columns]\n",
      "biopython_component-1  |   File \"/usr/local/lib/python3.9/site-packages/pandas/core/indexes/base.py\", line 6252, in _raise_if_missing\n",
      "biopython_component-1  |     raise KeyError(f\"{not_found} not in index\")\n",
      "biopython_component-1  | KeyError: \"['sequence_length', 'molecular_weight', 'aromaticity', 'isoelectric_point', 'instability_index', 'gravy', 'helix', 'turn', 'sheet', 'charge_at_ph3', 'charge_at_ph5', 'charge_at_ph7', 'charge_at_ph9', 'molar_extinction_coefficient_oxidized', 'molar_extinction_coefficient_reduced', 'flexibility_max', 'flexibility_min', 'flexibility_mean'] not in index\"\n",
      "biopython_component-1  | [2024-06-12 11:16:37,030 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:44963'. Reason: nanny-close\n",
      "biopython_component-1  | [2024-06-12 11:16:37,031 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "biopython_component-1  | [2024-06-12 11:16:37,032 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:44879'. Reason: nanny-close\n",
      "biopython_component-1  | [2024-06-12 11:16:37,033 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "biopython_component-1  | [2024-06-12 11:16:37,033 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:33253'. Reason: nanny-close\n",
      "biopython_component-1  | [2024-06-12 11:16:37,033 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "biopython_component-1  | [2024-06-12 11:16:37,033 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:36025. Reason: nanny-close\n",
      "biopython_component-1  | [2024-06-12 11:16:37,034 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:35509'. Reason: nanny-close\n",
      "biopython_component-1  | [2024-06-12 11:16:37,035 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "biopython_component-1  | [2024-06-12 11:16:37,034 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:41667. Reason: nanny-close\n",
      "biopython_component-1  | [2024-06-12 11:16:37,035 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:42493'. Reason: nanny-close\n",
      "biopython_component-1  | [2024-06-12 11:16:37,036 | distributed.core | INFO] Connection to tcp://127.0.0.1:37543 has been closed.\n",
      "biopython_component-1  | [2024-06-12 11:16:37,036 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "biopython_component-1  | [2024-06-12 11:16:37,036 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:42589'. Reason: nanny-close\n",
      "biopython_component-1  | [2024-06-12 11:16:37,036 | distributed.worker.state_machine | WARNING] Async instruction for <Task cancelled name=\"execute(('getitem-2c482f30bc6bd24cbbd0f21f7bdc95c0', 2))\" coro=<Worker.execute() done, defined at /usr/local/lib/python3.9/site-packages/distributed/worker_state_machine.py:3615>> ended with CancelledError\n",
      "biopython_component-1  | [2024-06-12 11:16:37,036 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "biopython_component-1  | [2024-06-12 11:16:37,037 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:46601'. Reason: nanny-close\n",
      "biopython_component-1  | [2024-06-12 11:16:37,037 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:39577. Reason: nanny-close\n",
      "biopython_component-1  | [2024-06-12 11:16:37,037 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "biopython_component-1  | [2024-06-12 11:16:37,037 | distributed.nanny | INFO] Closing Nanny at 'tcp://127.0.0.1:45585'. Reason: nanny-close\n",
      "biopython_component-1  | [2024-06-12 11:16:37,037 | distributed.nanny | INFO] Nanny asking worker to close. Reason: nanny-close\n",
      "biopython_component-1  | [2024-06-12 11:16:37,038 | distributed.core | INFO] Connection to tcp://127.0.0.1:37543 has been closed.\n",
      "biopython_component-1  | [2024-06-12 11:16:37,038 | distributed.worker.state_machine | WARNING] Async instruction for <Task cancelled name=\"execute('to_parquet-f16bb78f-894b-4eab-9326-22a489715d0b')\" coro=<Worker.execute() done, defined at /usr/local/lib/python3.9/site-packages/distributed/worker_state_machine.py:3615>> ended with CancelledError\n",
      "biopython_component-1  | [2024-06-12 11:16:37,039 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:55790; closing.\n",
      "biopython_component-1  | [2024-06-12 11:16:37,040 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:55772; closing.\n",
      "biopython_component-1  | [2024-06-12 11:16:37,040 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:35577. Reason: nanny-close\n",
      "biopython_component-1  | [2024-06-12 11:16:37,040 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:36025', name: 0, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718190997.0404422')\n",
      "biopython_component-1  | [2024-06-12 11:16:37,040 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:36915. Reason: nanny-close\n",
      "biopython_component-1  | [2024-06-12 11:16:37,040 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:39909. Reason: nanny-close\n",
      "biopython_component-1  | [2024-06-12 11:16:37,041 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:41667', name: 1, status: closing, memory: 1, processing: 1> (stimulus_id='handle-worker-cleanup-1718190997.0410242')\n",
      "biopython_component-1  | [2024-06-12 11:16:37,041 | distributed.core | INFO] Connection to tcp://127.0.0.1:37543 has been closed.\n",
      "biopython_component-1  | [2024-06-12 11:16:37,042 | distributed.worker.state_machine | WARNING] Async instruction for <Task cancelled name='gather_dep(tcp://127.0.0.1:32773, {...})' coro=<Worker.gather_dep() done, defined at /usr/local/lib/python3.9/site-packages/distributed/worker_state_machine.py:3615>> ended with CancelledError\n",
      "biopython_component-1  | [2024-06-12 11:16:37,042 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:45127. Reason: nanny-close\n",
      "biopython_component-1  | [2024-06-12 11:16:37,042 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:55792; closing.\n",
      "biopython_component-1  | [2024-06-12 11:16:37,043 | distributed.core | INFO] Connection to tcp://127.0.0.1:37543 has been closed.\n",
      "biopython_component-1  | [2024-06-12 11:16:37,045 | distributed.core | INFO] Connection to tcp://127.0.0.1:37543 has been closed.\n",
      "biopython_component-1  | [2024-06-12 11:16:37,046 | distributed.core | INFO] Connection to tcp://127.0.0.1:37543 has been closed.\n",
      "biopython_component-1  | [2024-06-12 11:16:37,048 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:39577', name: 3, status: closing, memory: 1, processing: 1> (stimulus_id='handle-worker-cleanup-1718190997.04794')\n",
      "biopython_component-1  | [2024-06-12 11:16:37,051 | distributed.core | INFO] Connection to tcp://127.0.0.1:37543 has been closed.\n",
      "biopython_component-1  | [2024-06-12 11:16:37,050 | distributed.batched | INFO] Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:37543 remote=tcp://127.0.0.1:55792>\n",
      "biopython_component-1  | Traceback (most recent call last):\n",
      "biopython_component-1  |   File \"/usr/local/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 297, in write\n",
      "biopython_component-1  |     raise StreamClosedError()\n",
      "biopython_component-1  | tornado.iostream.StreamClosedError: Stream is closed\n",
      "biopython_component-1  | \n",
      "biopython_component-1  | The above exception was the direct cause of the following exception:\n",
      "biopython_component-1  | \n",
      "biopython_component-1  | Traceback (most recent call last):\n",
      "biopython_component-1  |   File \"/usr/local/lib/python3.9/site-packages/distributed/batched.py\", line 115, in _background_send\n",
      "biopython_component-1  |     nbytes = yield coro\n",
      "biopython_component-1  |   File \"/usr/local/lib/python3.9/site-packages/tornado/gen.py\", line 767, in run\n",
      "biopython_component-1  |     value = future.result()\n",
      "biopython_component-1  |   File \"/usr/local/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 307, in write\n",
      "biopython_component-1  |     convert_stream_closed_error(self, e)\n",
      "biopython_component-1  |   File \"/usr/local/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "biopython_component-1  |     raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "biopython_component-1  | distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:37543 remote=tcp://127.0.0.1:55792>: Stream is closed\n",
      "biopython_component-1  | [2024-06-12 11:16:37,057 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:55744; closing.\n",
      "biopython_component-1  | [2024-06-12 11:16:37,057 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:55782; closing.\n",
      "biopython_component-1  | [2024-06-12 11:16:37,058 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:39909', name: 4, status: closing, memory: 0, processing: 1> (stimulus_id='handle-worker-cleanup-1718190997.0587542')\n",
      "biopython_component-1  | [2024-06-12 11:16:37,060 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:36915', name: 5, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718190997.0600002')\n",
      "biopython_component-1  | [2024-06-12 11:16:37,060 | distributed.worker | INFO] Stopping worker at tcp://127.0.0.1:32773. Reason: nanny-close\n",
      "biopython_component-1  | [2024-06-12 11:16:37,062 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:55800; closing.\n",
      "biopython_component-1  | [2024-06-12 11:16:37,064 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:55794; closing.\n",
      "biopython_component-1  | [2024-06-12 11:16:37,066 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:35577', name: 7, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718190997.0661154')\n",
      "biopython_component-1  | [2024-06-12 11:16:37,069 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:45127', name: 6, status: closing, memory: 0, processing: 1> (stimulus_id='handle-worker-cleanup-1718190997.069744')\n",
      "biopython_component-1  | [2024-06-12 11:16:37,078 | distributed.batched | INFO] Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:37543 remote=tcp://127.0.0.1:55800>\n",
      "biopython_component-1  | Traceback (most recent call last):\n",
      "biopython_component-1  |   File \"/usr/local/lib/python3.9/site-packages/distributed/batched.py\", line 115, in _background_send\n",
      "biopython_component-1  |     nbytes = yield coro\n",
      "biopython_component-1  |   File \"/usr/local/lib/python3.9/site-packages/tornado/gen.py\", line 767, in run\n",
      "biopython_component-1  |     value = future.result()\n",
      "biopython_component-1  |   File \"/usr/local/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 262, in write\n",
      "biopython_component-1  |     raise CommClosedError()\n",
      "biopython_component-1  | distributed.comm.core.CommClosedError\n",
      "biopython_component-1  | [2024-06-12 11:16:37,080 | distributed.batched | INFO] Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:37543 remote=tcp://127.0.0.1:55794>\n",
      "biopython_component-1  | Traceback (most recent call last):\n",
      "biopython_component-1  |   File \"/usr/local/lib/python3.9/site-packages/distributed/batched.py\", line 115, in _background_send\n",
      "biopython_component-1  |     nbytes = yield coro\n",
      "biopython_component-1  |   File \"/usr/local/lib/python3.9/site-packages/tornado/gen.py\", line 767, in run\n",
      "biopython_component-1  |     value = future.result()\n",
      "biopython_component-1  |   File \"/usr/local/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 262, in write\n",
      "biopython_component-1  |     raise CommClosedError()\n",
      "biopython_component-1  | distributed.comm.core.CommClosedError\n",
      "biopython_component-1  | [2024-06-12 11:16:37,119 | distributed.worker.state_machine | WARNING] Async instruction for <Task cancelled name=\"execute(('getitem-2c482f30bc6bd24cbbd0f21f7bdc95c0', 7))\" coro=<Worker.execute() done, defined at /usr/local/lib/python3.9/site-packages/distributed/worker_state_machine.py:3615>> ended with CancelledError\n",
      "biopython_component-1  | [2024-06-12 11:16:37,140 | distributed.core | INFO] Connection to tcp://127.0.0.1:37543 has been closed.\n",
      "biopython_component-1  | [2024-06-12 11:16:37,143 | distributed.core | INFO] Received 'close-stream' from tcp://127.0.0.1:55760; closing.\n",
      "biopython_component-1  | [2024-06-12 11:16:37,147 | distributed.scheduler | INFO] Remove worker <WorkerState 'tcp://127.0.0.1:32773', name: 2, status: closing, memory: 5, processing: 6> (stimulus_id='handle-worker-cleanup-1718190997.146854')\n",
      "biopython_component-1  | [2024-06-12 11:16:37,150 | distributed.scheduler | INFO] Lost all workers\n",
      "biopython_component-1  | [2024-06-12 11:16:37,552 | distributed.scheduler | INFO] Scheduler closing due to unknown reason...\n",
      "biopython_component-1  | [2024-06-12 11:16:37,552 | distributed.scheduler | INFO] Scheduler closing all comms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[Kbiopython_component-1 exited with code 1\n",
      "Finished workflow run.\n"
     ]
    }
   ],
   "source": [
    "from fondant.dataset.runner import DockerRunner\n",
    "import shutil\n",
    "\n",
    "# remove the most recent output folder if the manifest file is removed\n",
    "# without a manifest file in the most recent output folder, the pipeline cannot be run\n",
    "if OUTPUT_FOLDER and REMOVED_MANIFEST:\n",
    "\tshutil.rmtree(OUTPUT_FOLDER)\n",
    "\t# remove cache\n",
    "\tshutil.rmtree(os.path.join(BASE_PATH, PIPELINE_NAME, \"cache\"))\n",
    "\n",
    "# get current full path to the project\n",
    "mounted_data = os.path.join(os.path.abspath(\"data\"), \":/data\")\n",
    "\n",
    "runner = DockerRunner()\n",
    "runner.run(dataset=final_dataset, working_directory=mounted_data, extra_volumes=mounted_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "The following results have been taken from the output of the pipeline, which is stored in the `.fondant` directory. This directory contains the output of each component, together with the cache of the previous run. Currently, the pipeline doesn't implement the `write_to_file` component, so the results will be taken individually from the output of each component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No matching folders found\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "stat: path should be string, bytes, os.PathLike or integer, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo matching folders found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m     exit()\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mOUTPUT_FOLDER\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     13\u001b[0m \t\u001b[38;5;66;03m# remove the manifest file from each folder in the output folder\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \t\u001b[38;5;28;01mfor\u001b[39;00m root, dirs, files \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mwalk(OUTPUT_FOLDER):\n\u001b[1;32m     15\u001b[0m \t\t\u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m files:\n",
      "File \u001b[0;32m/usr/lib/python3.10/genericpath.py:19\u001b[0m, in \u001b[0;36mexists\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 19\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: stat: path should be string, bytes, os.PathLike or integer, not NoneType"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "# get the most recent folder in the folder named: BASE_PATH + PIPELINE_NAME + PIPELINE_NAME-<timestamp>\n",
    "matching_folders = glob.glob(f\"{BASE_PATH}/{PIPELINE_NAME}/{PIPELINE_NAME}-*\")\n",
    "\n",
    "if matching_folders:\n",
    "    OUTPUT_FOLDER = max(matching_folders, key=os.path.getctime)\n",
    "else:\n",
    "    print(\"No matching folders found\")\n",
    "    exit()\n",
    "\n",
    "if os.path.exists(OUTPUT_FOLDER):\n",
    "\t# remove the manifest file from each folder in the output folder\n",
    "\tfor root, dirs, files in os.walk(OUTPUT_FOLDER):\n",
    "\t\tfor file in files:\n",
    "\t\t\tif file == \"manifest.json\":\n",
    "\t\t\t\tos.remove(os.path.join(root, file))\n",
    "\t\t\t\tREMOVED_MANIFEST = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def merge_parquet_folders(folder_path):\n",
    "\tmerge_df = pd.DataFrame()\n",
    "\t\n",
    "\tfor folder in os.listdir(folder_path):\n",
    "\t\tparquet_partitions = os.path.join(folder_path, folder)\n",
    "\t\tdf = pd.read_parquet(parquet_partitions)\n",
    "\t\t\n",
    "\t\tif merge_df.empty:\n",
    "\t\t\tmerge_df = df\n",
    "\t\telse:\n",
    "\t\t\tmerge_df = merge_df.merge(df, on=\"sequence\")\n",
    "\t\n",
    "\treturn merge_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if REMOVED_MANIFEST and os.path.exists(OUTPUT_FOLDER):\n",
    "\tmerged_df = merge_parquet_folders(OUTPUT_FOLDER)\n",
    "\tmerged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if REMOVED_MANIFEST and os.path.exists(OUTPUT_FOLDER):\n",
    "\tif not os.path.exists(os.path.join(os.path.abspath(\"data\"), \"export\")):\n",
    "\t\tos.makedirs(os.path.join(os.path.abspath(\"data\"), \"export\"))\n",
    "\n",
    "\toutput_path = os.path.join(os.path.abspath(\"data\"), \"export\")\n",
    "\n",
    "\tmerged_df.to_parquet(os.path.join(output_path, \"results.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the output file\n",
    "\n",
    "output_df = pd.read_parquet(\"./data/export/results.parquet\")\n",
    "output_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "protein-feature-extraction-NoVdeDG9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
